 ##### Section 13: K8s Storage (Volumes)
1. Introduction to Kubernetes Storage types
		Introduction to Kubernetes Storage types- [Doc]
2. Overview on Kubernetes Volumes
		Overview on Kubernetes Volumes - [Doc]
3. Working principle of `emptyDir` volume type
		Overview on Kubernetes Volumes - [Doc]
4. `EmptyDir` volume type (Disk and Memory)
		EmptyDir Demo for `emptyDir` volume type (Disk and Memory) - [Doc]
5. Working principle of `hostPath` volume type
		HostPath Working principle of `hostPath` volume type - [Doc]
6. `HostPath` volume type (Directory and `DirectoryOrCreate`)
		`HostPath` Demo for `hostPath` volume type (Directory and `DirectoryOrCreate`) - [Doc]
7. `HostPath` volume type (File and FileOrCreate)
		`HostPath` Demo for `hostPath` volume type (File and `FileOrCreate`) - [Doc]
8. Working principle of `nfs` volume type
9. Setup NFS server for Kubernetes Volume Demo
10. NFS volume type
11. Jenkins CICD Deployment Object with active and passive mode (NFS volume type)
12. `DownwardAPI` (Information: fieldRef)
13. `DownwardAPI` (Information: resourceFieldRef)

---

# Section 13: K8s Storage (Volumes)
# 1. Introduction to Kubernetes Storage types


The next very important topic in this **K 8 s - Storage (Volumes)**
	Which is so critical for applications and for the clusters. Especially for **Applications** running on clusters. 

What's is in it?
First, need to know about 
- Volumes (followed by)
- Persistent Volumes (pv) (and the differences),
- Persistent volume claim (pvc)
- Storage class
And more storage types such as,
- Volume expansion (volume scaling)  
- Volumes snapshot
- Dynamic Volume provisioning
- And more advanced storage types. 

To start with storage. Will get familiar with volumes and proceed to more advanced types as we listed above. 

And see more of 
- When to use it,
- Where to use it,
- And use cases. 

Lets get started with volumes,

---
# 2. Overview on Kubernetes Volumes
Overview on Kubernetes Volumes - [Doc]

Volumes -
- Purpose of volumes,
- Why volumes for the workload resources
- When to use
- Where to use
- Limitations of volumes 
- Pros and cons of volumes
- Volumes for both Pods and also the Containers

==**Volumes for Pods are persistent . 
Volumes for containers are ephemeral.**==
Will dig in deep about storage and volumes in brief. 
- In general, containers are **ephemeral** which files will be lost once a containers crashes.
-  Since kubelet restarts containers which comes with clean state data will be lost. 
- When we want to **share data** across container inside the POD, we can use volumes.
- When containers restarts with clean state, POD volume will help to persist the data. Since POD uses volumes as persistent storage solution for containers. 
- Container mounts directory under `.spec.containers[*].volumeMounts` to volumes under `.spec.volumes` with common name.
- K8s supports two types of volumes,
	- Ephemeral volumes (emptyDir,  configMap, secret, downwardAPI, CSI ephemeral volumes. Etc.,)
	- Persistent volumes (PV -> Static and dynamic with StorageClass)
- Volume types supported by K8s,
	- AWS EBS CSI
	- AzureDisk CSI, [[azure_disk]]
	- GCE CSI
	- vSphere CSI
	- CephFS, rbd
	- ConfigMap, secret, downwardAPI, hostPath
	- iSCSI, local, NFS [[nfs-server-setup]]
	- PVC
	- Projected

###### In general, containers are ephemeral which files will be lost once a containers crashes.
Eg: 
- take a POD.
- No matter if it is a standalone or workload resource based POD.
- And that POD contains containers. 
- Container1, Container2
- Until a container gets recreated, the data will be on that container. 
- If a container crashes, data crashes and lost. Where `kubelet` tries to recreate the POD and along with containers. from scratch (like creating a new VM, so the configurations, data's and volumes)
That way containers - ephemeral volumes. Till a container is alive, the data will be alive.  

Once a container goes down, `kubelet` will recreate a new container which results in losing data. 

######  Since kubelet restarts containers which comes with clean state data will be lost. 
Since kubelet restarts containers which comes with clean state data will be lost. -> `kubelet` doesn't restarts the container. It recreates. 

Like in docker, docker container stop -> rm -> run. Not docker restart.  Only recreation, no restarts. Again, which results in losing data creating a new container. 
>  cannot use as persistent volumes, cannot store the data persistently. Cannot modify the configs or the containers itself when the storage is ephemeral.   

###### When we want to share data across  container inside the POD, we can use volumes.
To **share data** across container inside the POD, -> use ==volumes==. 

If we recall the definition of PODs. [[00. section3-full-k8sPods]]] 

> [!NOTE] **What is a POD**
> - ==Pod **creates a logical layer**  
>     to group 1 or more containers = to have **common network** + **shared storage.**==
> - Pod has a unique IP address assigned from `--pod-network-cidr=10.244.0.0/16`
> - Containers inside those POD talks to each other on `localhost` domain or `127.0.0.1`, since container share the same network stack.
> - Containers share data inside POD.
> - In general, we need to create containers inside POD  
>     which are dependent, not different application.

> Point to recall - Pod **creates a logical layer**  
    to group 1 or more containers = to have **common network** + **shared storage.**

The term **shared storage** means here is that -> We can share the data
- **across containers** ->  **inside the PODs.** 
Eg: like what we have done with the **nginx-multicontainer** - `deploy.yaml`

Where we saw two container's working directory, sharing same volumes out of PODs `EmptyDir()` volume.  
![[PodvsContainerStorage]]
**Purpose/Objective:** Volumes used inside the Pod to share the data across the containers. 
**Benefit:** 
- Shared volume across containers inside a POD (using `EmptyDir()`). 
- If a container gets crashed and gets recreated freshly from scratch inside the POD, the data will be persistent not get washed.  (*Data will not be lost which that came out of that dead container, where the volume didn't get deleted.*)
- Where the volume will be deleted if the POD gets deleted, not the container.
> Volumes - a shared storage volume solution to work with the data of the containers inside the POD. 

###### When containers restarts with `clean state`, POD volume will help to persist the data. Since POD uses volumes as persistent storage solution for containers. 
When containers restarts with **clean state**, *POD volume will help to persist the data.*

Since POD uses volumes as persistent storage solution for containers which we just discussed in the above point.  

The core part of the volume is that to share the volume, to provision a persistent storage. Even if it gets crashed and recreated, the volume will not be lost. 

**==For container -  it is a persistent storage, 
For POD - its ephemeral 
where if the POD gets f'ed up and so the volumes too.==**  


###### Container mounts directory under `.spec.containers[*].volumeMounts` to volumes under `.spec.volumes` with common name.

Volume mounting - Container mounts directory under 
> -  `.spec.containers[*].volumeMounts` - **Containers**
> - to volumes under `.spec.volumes` with common name.


###### K8s supports two types of volumes,
 - **Ephemeral/Temp volumes (emptyDir,  configMap, secret, downwardAPI, CSI ephemeral volumes. Etc.,)**
As we all know, once f'ed, it's f'ed. 
processes like for eg: RAM's memory, CPU's processes and such.  - which couldn't be persistent where everything gets washed once it gets restarted. 

- **Persistent volumes (PV -> Static and dynamic with StorageClass)**
Persistent is persistent. Like EG: Hard disks, SSD, NAS , Memory card, Pen drive and such.
Persistent - Exists even if there is a restart. 

>  These are the two types of Volumes in K8s. 

In these two types of volumes, there are sub types/ format to volumes, and here there are certain volumes types supported in K8s,
###### Volume types supported by K8s, 
> Note: **CSI, Container Storage Interface** - plugins
- AWS EBS CSI (AWS Elastic Block Storage )
- AzureDisk CSI, [[azure_disk]]
- GCE CSI
- vSphere CSI
- Openstack CSI
- CephFS, rbd
- ConfigMap, secret, downwardAPI, hostPath
- iSCSI, local, NFS [[nfs-server-setup]]
- PVC
- Projected

Varies based on different environments. Takes these plugins, volume types in order to store the data. 

Will dig deep into all the volumes types and see which applies where. 

Since we could not be able to store things with what we are doing, will see ,
- How to work with this 
- How to share data between workloads
- How to persist the data and such. 


---
# 3. EmptyDir(): Working principle of `emptyDir` volume type
Overview on Kubernetes Volumes - [Doc]
```
VolumeType: 
- EmptyDir
```

###### Let see about one of the volume type - **`EmptyDir()`**
And the working principle of `emptyDir` volume type.

Critical points and 
**Working Principles of `EmptyDir()`:**
![[emptyDir()]]

> Nothing different. Exactly same as what we were doing to the `nginx-multicontainer` workload using `emptyDir()` VolumeType for volume which is an **Ephemeral VolumeType**

- We got an application running on a K8s Cluster having one **controlplane** and two **workernodes**
- Application deployed using Deployment workload resource object. 
- Deployment, `replica=3`, 2 Pods (Pod1, Pod3) on workernode1, one (Pod2) on workernode2. (which gets appropriately distributed based on multiple metrics)
-  here, whenever the Pod uses `EmptyDir()`, which creates a volume directory under `/var/lib/kubelet/pods/Pod'sUID/volumes/kubernetes.io~empty-dir/volume_name/data`
- Creates `emptyDir()` under this directory path. 
- Exists until the POD is available. 
- Applies to all the Pods exists here. 
- Here, the context is the The WORKDIR of the container -> mounted -> to `emptyDir()` of the POD.
>  - so For the container, it is persistent
>  - for the PODs, its ephemeral. 

**This is not a wise choice if you want the data to be persistent (good for batch, not recommended for Production purpose) since the `emptyDir()` is totally ephemeral.** 

If we happened to migrate these workloads from one node to to other node, its a brand new directory gets created from scratch which couldn't be reutilized.

Not a viable storage solution for deploying mission critical applications on PODs. 

Will dig deep about this and see why this does and doesn't work in **Practice: `EmptyDir()`**

---
# 4 . `EmptyDir()`: Demo for `emptyDir()` volume type (Disk and Memory)
EmptyDir Demo for emptyDir volume type (Disk and Memory) - [Doc]

Practical's on Implementing the `EmptyDir()` VolumeType. 

Will see this by creating a simple standalone POD having 2 Containers with the `emptyDir()` VolumeType (or any Workload Resource type as per convenience).

Manifest for the Standalone  `pod.yaml` having 2 containers with **emptyDir ()** VolumeType. 
```yaml
apiVersion: v1 #apiVersion of the Pod
kind: Pod #Kind of the Object
metadata: #Metadata of the Pod (name, labels as key-value pairs)
  name: standalone-pod
  labels:
    app: nginx
    env: dev
    release: v1.0

spec: #Spec of the Pod
  containers: #Containers under the Pod
  - name: write-app
    image: alpine
    command: ["/bin/sh"]
    args: ["-c", "while true; do date >> /var/log/index.html; sleep 10; done"] #Command that writes the data to the emptyDir() PodVolume
    volumeMounts: #VolumeMounts of the Container to the emptyDir() PodVolume.
    - name: standalone-pod-volume
      mountPath: /var/log #workDir of the Container mounted to the emptyDir() PodVolume

  - name: serve-app
    image: nginx:latest
    ports:
    - containerPort: 80 #Port of the Container
    volumeMounts:
      - name: standalone-pod-volume
        mountPath: /usr/share/nginx/html
        #workDir of the Container mounted to the emptyDir() PodVolume, serving the data from the emptyDir() PodVolume

  volumes:
  - name: standalone-pod-volume #Name of the emptyDir() PodVolume
    emptyDir: {} #emptyDir() PodVolume
```

```
kubectl create po /path/to/pod.yaml
kubectl po -o wide
```

SO, THE POD IS RUNNING THO!

Here, Regarding Volume where we ran, `emptyDir`, 
Check , where it is running. 

AND DIAGNOSE THE POD:
```
ssh username@NodeVMIp
```

Where the POD gets to run in a particular Node. 
```
ls -l /var/lib/kubelets/pods
```
Where you can see all the PODs running on that node named with an `uid`. 

HOW DO WE KNOW WHAT `uid` USES WHICH POD? Go back to `controlplane` and Diagnose the **PodIDs**.

```
kubectl get po
```
Get the PodName, and Inspect it,
``` 
kubectl get po podName -o yaml
kubectl get po podName -o yaml | grep uid
```
After getting the UID from `controlplane`, get back to node and 
```
ls -l /var/lib/kubelet/pods/podUID/volumes/kubernetes.io~empty-dir/PodVolumeName/<data>
```
Here, all the respective data will be getting stored with the shared manner. 

```
cat <data>
curl PodIP
```
Which is the data what goes there, gets shared too. 
 >  ==**Here, one container is writing the data in that volume, and the other container is serving that data from that volume.**==

**Where is `EmptyDir)()` here?** 
-> back to Node, this shared volume works as same or moreover similar to the Volume mount, Bind mount, tmpfs.

Delete pod and verify,
In `controlplanenode`
```
kubectl delete po PodName
```
In `computeplanenode`
```
ls -l /var/lib/kubelet/pods/UID/
```
Will not exist since we removed the POD. `No such file or Directory`.

Here, if we recreate or whatever modification that we perform or to recreate the same POD here,  IT WILL NOT BE AS SAME AS IT WAS. 

And there will be bunch of mismatches in multiple levels. 
- Ip will not be the same
- PodUID 
- PodIP
- Pod placement in the clusters - Node selection
- **Especially, Data will be so lost, since it is not persistent.** 
- And much more.

**WHERE IS THE DATA GETS STORED - IS IT THE RAM or DISK OF THE HOST MACHINE!?**
Create the POD once again. 

```
kubectl create -f path/to/standalonepod.yaml
kubectl get po -o wide
```
POD gets created and running in a clean state. 

 Get the UID and back to the workernode where the POD is running,
 ```
 kubectl get po -o yaml | grep uid 
 ```
And to the workernode,
```
ls -l /var/lib/kubelet/pods/<uid>/volumes/kubernetes.io/shared-data/<*data>
```
Where the `<*data>` gets stored?
The POD is created on the host machine tho.  Where in the host machine the data lies at? Speaking of the storage medium of the host machine - all we have is RAM which is volatile and Storage Disk which is non-volatile.

Here, If we don't pass any parameters, the data will be stored in the disk by default. 

```
df -h
mount | grep -i emptyDir
mount | grep -i podUID
```
Which that the volume got mounted to ==**`tmpfs`**==,

The entire Filesystem is stored on the disk tho. Find out by passing 
```
df -h
```
Also on this node, the POD is running. Where the Kubernetes created a volume under that root filesystem which is mount to the host disk.  

To double check, go back to controlplane, and login to each container,
```
kubectl exec -it -u 0 PodName -c ContainerName1 -- sh
```

After getting into the shell,
```
df -h
```
where you can find that the volume has been mounted to the host's disk. 

 This how the data is getting stored in the shared manner between containers. 
```
kubectl exec -it -u 0 PodName -c ContainerName1 -- sh
```
And check the same by passing 
```
dh -f
mount
```
Such folder got mounted to such disk partition. 

This way, we conclude that the storage medium here is the Disk not RAM on the host. 

Cleanup resources,
```
kubectl delete po podName
```

IF, WE WANT TO RUN THINGS RAM, nothing much but simply
```
volumes:
  - name: PodVolumeName
  emptyDir:
    medium: Memory
  ```

And to create the same,
```
kubectl create -f /path/to/standaonepod.yaml
```

Verify and check in which node the POD lies at, 
```
kubectl get po -o wide
```

`ssh` into that node and verify volume
```
ls -l /var/lib/kubelet/pods/<uid>/volumes/kubernetes.io~emptydir/podVolumeName/<data>
```

NOW, HOW TO VERIFY THAT THE MEDIUM OF STORAGE TO THIS POD - is RAM of the host?,

```
kubectl exec -it -u 0 PodName -c ContainerName1 -- sh
```
```
kubectl exec -it -u 0 PodName -c ContainerName2 -- sh
```

Here, check the mounts disks, 
```
df -h | grep 
```

Mounted to `tmpfs`, not to the disk.

**In memory storage medium, we can also set limits that how much of space to be used in it**,
```
volumes:
  - name: PodVolumeName
    emptyDir:
      medium: Memory
      sizeLimit: 300Mi #300mb
```

Apply and verify the same by, spinning up a POD and login to a container, 
```
df -h
```
In the mount of `tmpfs`, the size limit of it will be shown. 


WHERE AND WHEN TO USE `disk` and `memory ` under `emptyDir` ?
- RAM - Any critical or sensitive data to be stored for a sec to be volatile - on the container to the RAM.
- Memory - Store volume in the disk to working and provisioning data.

Cleanup,
```
kubectl delete po PodName
```

>  **emptyDir** is nothing but like in docker's anonymous volumes.  Stays in K8s not binding to anything. If it gets deleted, deleted. 

This is all about `emptyDir`.


---
# 5. `hostPath`:  Working principle of hostPath volume type
HostPath Working principle of hostPath volume type - [Doc]

###### HostPath: Working principle of hostPath volume type

Now,  will cover the next **Volume Type: hostPath** and learn what are all the advantages and such with this one - **HostPath**
![[hostpath]]

Will take the same example as we did for `emptyDir` where we have a cluster with 2 nodes namely `computeplanenode1` having `pod1 and pod3`  and ` computeplanenode2` having `pod2` where it is running a `deployment` object with `replica=3` where each POD having 2 containers in it.

Here, in the volumes perspective,
We have used `emptyDir` which keeps things in the `/var/lib/kubelet/pods/<UID>/kubernetes.io~emptyDir/PodVolumeName/<data`

How does this `hostPath` shares data in-between?
>  just like in docker how we do **bind mount** between container and the local fs. SAME HERE!

```
mountpath/of/the/local/fs -> /WORKDIR
```

AS PER THE PODs defined in the diagram, 
- Pod 1, Pod 3's WORKDIR `/var/tmp` has been mounted to a folder in our local system,
- Pod 2's WORKDIR `/var/tmp` has been mounted to a folder in our local system separately.
And this separation is the **limitation** is what we have in this `VolumeType`.

1) No common shared storage space available for the PODs to share the data on scale. 

**So, what makes the `hostPath` different from `emptyDir` is that if we delete or recreate the volume or POD, the data doesn't go along with that.**
And 

2) If we get to recreate a POD, to spawn the data in the clean state POD, simply can just mount the previous directory to the Pod's WORKDIR.
But can't guarantee that the POD will be spun on the same node again, to be mounted to the previous directory to the POD to respawn back the data into it.

**If the POD get to be in the same node, then only we can able to make this VolumeType work.** 

Backup, restore and such only if the POD gets to be on the same node. 
**==To remediate that, we should stuff such as NodeAffinity, Node selector, Node Name where we can able to get to control the scheduler to make it work as per our convenience, these things are viable. Will cover those soon.==** 

If the entire node gets down. Poofff! No data will left with this `emptyDir` and `hostpath`.

`HostPath` is comparatively better than `emptyDir`. Since you can able to recover. But not so much where we want volumes to be retrievable even to the level where the node itself crashes. 

**Will compare `HostPath` with another VolumeType called `NFS`**, that tell the cons of `HostPath` thou.

Now, will dig deep into `HostPath` and will see the  
- What is it?
- Types of HostPath
- Files based, directory based
- Socket based, block storage based and more. 


---
# 6. `hostPath`: Demo for `hostPath`  volume type (Directory and DirectoryOrCreate)

**HostPath: Demo for hostPath volume type (Directory and DirectoryOrCreate)** 

Demo of `HostPath` VolumeType. Will take the same PodTemplate what we used before for `emptyDir`.

One writing data, one serving data and such. Nothing much to change but the `volume` parameter.

```yaml
apiVersion: v1 #apiVersion of the Pod
kind: Pod #Kind of the Object
metadata: #Metadata of the Pod (name, labels as key-value pairs)
  name: pod-hostpath
  labels:
    app: nginx
    env: dev
    release: v1.0
  
spec: #Spec of the Pod
  containers: #Containers under the Pod
  - name: write-app
    image: alpine:latest
    command: ["/bin/sh"]
    args: ["-c", "while true; do date >> /var/log/index.html; sleep 10; done"] #Command that writes the data to the emptyDir() PodVolume
    volumeMounts: #VolumeMounts of the Container to    the emptyDir() PodVolume.
    - name: standalone-pod-volume
      mountPath: /var/log #workDir of the Container mounted to the emptyDir() PodVolume

  - name: serve-app
    image: nginx:latest
    ports:
    - containerPort: 80 #Port of the Container
    volumeMounts:
      - name: pod-hostpath-volume
        mountPath: /usr/share/nginx/html
        #workDir of the Container mounted to the emptyDir() PodVolume, serving the data from the emptyDir() PodVolume

  volumes:
  - name: pod-hostpath-volume #Name of the emptyDir() PodVolume
    hostPath:
      path: /var/tmp #hostpath -> PodVolumeType
```

Here,
```yaml
volumes:
  - name: hostpath-volume
    hostpath:
    path: /var/tmp/nginx-data
```


There are multiple types of `hostpath`, will get to each one.

##### 1) **DirectoryOrCreate**
Either mount to a directory or if no directory, create one and get mounted. 

```
volumes:
  - name: hostpath-volume
    hostpath:
    path: /var/tmp/nginx-data
    type: DirectoryOrCreate
```

```
kubectl create -f /path/to/hostpath.yaml
kubectl get po -o wide
```

HOW TO VERIFY IF THIS WORKED?
```
curl PodIP
```

HERE, THE DATA WAS GETTING SERVED via `emptyDir` but now, either `DirectoryOrCreate` and getting out of that!.

VERIFY IT IN THE NODE ITSELF. 
```
ssh username@NodeVMIp
ls -l /var/tmp/
```
There you have it. Probably the `nginx-data` would've not been there but if not i would've created and mounted it. (with the permission of 755). 

```
ls -l /var/tmp/nginx.data
cat file 
```

Here, if it is `emptyDir`, the volume would be lost if gets deleted or recreated. With `hostpath`, the content will not be lost tho. So, the folder will not get deleted, unless you manually do it. 

Will check if it true or false.

```
kubectl get po 
kubectl delete po PodName
```
```
ssh username@NodeVMIp
ls -l /var/tmp/nginx-data
cat file 
```
Data will still be there. Till it was served and alive. WHICH THE DATA CAN BE RESPAWNED AND WORK IF GETS PRESENTED ON THE SAME NODE. 

```
kubectl create -f /path/to/hostpath.yaml
kubectl get po -o wide
```
```
curl PodIP
```
+
```
ssh username@NodeVMIp
ls -l /var/tmp/nginx-data
cat file 
```
Unscathed!!!. Which it was used to be. Data will be generated and gets spinned up since where it left. 
> Even though the POD got deleted, the leftover data will still be available.

Sample manifest:
```yaml
apiVersion: v1 #apiVersion of the Pod
kind: Pod #Kind of the Object
metadata: #Metadata of the Pod (name, labels as key-value pairs)
name: pod-hostpathDirOrCreate
labels:
app: nginx
env: dev
release: v1.0

spec: #Spec of the Pod
containers: #Containers under the Pod
- name: write-app
image: alpine:latest
command: ["/bin/sh"]
args: ["-c", "while true; do date >> /var/log/index.html; sleep 10; done"] #Command that writes the data to the emptyDir() PodVolume
volumeMounts: #VolumeMounts of the Container to the emptyDir() PodVolume.
- name: pod-hostpathDirOrCreate-volume
mountPath: /var/log #workDir of the Container mounted to the emptyDir() PodVolume

- name: serve-app
image: nginx:latest
ports:
- containerPort: 80 #Port of the Container
volumeMounts:
- name: pod-hostpathDirOrCreate-volume
mountPath: /usr/share/nginx/html
#workDir of the Container mounted to the emptyDir() PodVolume, serving the data from the emptyDir() PodVolume
  
volumes:
- name: pod-hostpathDirOrCreate-volume #Name of the emptyDir() PodVolume
hostPath:
path: /var/tmp/nginx-data #hostpath -> PodVolumeType
type: DirectoryOrCreate #PodVolumeType
#also can simply use `Directory` as value. if directory exists, will be used, if not, fails.
```



---
# 7 . `hostPath`: Demo for `hostPath` volume type (File and `FileOrCreate`)
`HostPath` Demo for `hostPath` volume type (File and `FileOrCreate`)

Next one of `hostpath` - **`FileOrCreate`**. Here on contrast to **`DirectoryOrCreate`** where it mounts directory or create one and gets mounted, here it is point to an entry-point file instead of a directory.

**NOTE: Can use multiple types of Volume Types to one single resource as we can see here.**

Here with this one, applying this manifest out of our template for our example,
```yaml
apiVersion: v1 #apiVersion of the Pod
kind: Pod #Kind of the Object
metadata: #Metadata of the Pod (name, labels as key-value pairs)
name: pod-hostpathfileorcreate
labels:
app: nginx
env: dev
release: v1.0
  
spec: #Spec of the Pod
containers: #Containers under the Pod
- name: write-app
image: alpine:latest
command: ["/bin/sh"]
args: ["-c", "while true; do date >> /var/log/index.html; sleep 10; done"] #Command that writes the data to the hostpath PodVolume
volumeMounts: #VolumeMounts of the Container to the hostpath PodVolume.
- name: shareddata-hostpathfileorcreate #Name of the hostpath PodVolume
mountPath: /var/log #hostpath -> PodVolumeType
- name: infodata-hostpathfileorcreate #Name of the hostpath PodVolume:
mountPath: /var/tmp/info.php #hostpath -> PodVolumeType

- name: serve-app
image: nginx:latest
ports:
- containerPort: 80 #Port of the Container
volumeMounts:
- name: shareddata-hostpathfileorcreate
mountPath: /usr/share/nginx/html
#workDir of the Container mounted to the hostpath PodVolume, serving the data from the hostpath PodVolume

volumes:
- name: infodata-hostpathfileorcreate #Name of the hostpath PodVolume
hostPath:
path: /var/tmp/info.php #hostpath -> PodVolumeType
type: FileOrCreate #here for the FileOrCreate PodVolumeType, we are using it on only the write-app Container along with shared data volumetype

- name: shareddata-hostpathfileorcreate #Name of the emptyDir() PodVolume
hostPath:
path: /var/tmp/nginx-data #hostpath -> PodVolumeType
type: DirectoryOrCreate
```

Apply and verify
```
kubectl create -f /path/to/hostpathfileorcreate.yaml
kubectl get po -o wide
```

Check the mount,
```sh
kubectl exec it PodName -c ContainerName -- sh
ls -l /var/tmp/ #shows the file
```
With the permission of `644`. 

Verify the same in the node too,
```
ssh username@NodeVMIp
ls -l /var/tmp/
```

There you go!

As you can see, if create the file since it didn't exists, or if did gets mounted both on Node and in the container too. 

Perform cleanup,
```
kubectl delete po PodName
```

This works if you declare value as just `File` in VolumeType. 
Since we deleted that file, it exists, and gets mounted, if not, will throw error going (**FailedMount**).

The mount happens once it gets successfull then it performs the rest by pulling the image and executes the rest and spins up the container. 

That is pretty much it for **`FileOrCreate` `hostpath` VolumeType**


---
# 8. `NFS`: Working principle of `nfs`volume types

**`NFS`: Working principle of `nfs` volume types**. *important VolumeType.* 
Will cover what `nfs` VolumeType and have an `nfs` server separately somewhere and how to mount with the same using this `nfs`, which is **network file system**.

Will see setting up `nfs` which will run as another environment, here VM, Will expose directories to mount and making it work by sharing data across environments. 
 

![[nfs]]

As per this diagram having deployment object, the POD's volume will be mounted to a server **sharing common data** in between across node, 

 These ain't possible with what we have seen in the previous `VolumeType`. 

==With the diagrams, each container happened to have a  **==share data across node==** wherever your pods are running. - **nfs** - network server. Will be deploying this one on as a separate server. In order to use `nfs`, have to install package to make this one work. ==


---
# 9. `NFS`: Setup `NFS` server for Kubernetes Volume Demo

Will Setup `NFS` server for demonstration,

In order to move forward, we mentioned that we need to get this `nfs` server as a separate host. Simply add another host to configure `nfs` which will be used by K8s to share data in between nodes in ease. 

After spinning up a VM, installing Ubuntu which can be seen here. [[00. section2-full-k8sClusterSetup#5. Kubernetes multi-node setup using kubeadm tool(cri-containerd)]]

And ssh into that VM
```sh
ssh username@NodeVMIp
```

And install **`NFS`**,
```sh
sudo apt install nfs-kernel-server
sudo apt install -y nfs-common
```
```
sudo systemctl restart kubelet
```

And verify status, 
```
sudo systemctl status nfs-kernel-server
```
And its running,. Now, we can able to access data in a shared manner.   

Now, which directory we gotta expose inorder to execute `nfs`, where the clients can use that particular folder to access the data and share it in a shared manner.

Here, the directory where the config lies is,
```
/etc/exports
```
Here in this config, have to declare by mentioning which directory to expose to the clients and for `nfs` for the others to use on this network. 

For that, will create a separate folder to expose,
```
sudo -i
```
```
mkdir /var/nfs/
```

Now we have to mention it to the config for the `nfs` to expose it to the outside for all to be shared. And we also need to change permission of the folder too, to check permission
```
ls -ld 
```
Saying that,
```
drwxr-xr-x 2 root root 4096 May 16 13:03 /var/nfs/
```
Both belongs to root. 

Have to make this folder anonymous, to make it anonymous,
```
chown -R nobody:nogroup /var/nfs
```

To verify that,
```
ls -ld
```
Saying that,
```
drwxr-xr-x 2 nobody nogroup 4096 May 16 13:03 /var/nfs/
```
Belongs to nobody and nogroup. In order to expose this directory to `nfs`, how to tell the `nfs` to expose that folder, to that `nfs` s config. Where is that config, `/etc/exports`, here EDIT THE **config**,
```
vi /etc/exports
```
And add entries that,
```
...
/var/nfs	192.168.0.0/24(rw,sync,no_subtree_check,no_root_squash)
```
-  which folder to expose **1. `/var/nfs`**
- And to which network to access/expose to - to all the client machines. 
- Enter the **2. CIDR Block**, to give access for such clients. OR
- Enter specific machines by declaring all the **machine's IP** instead of a CIDR Block.  
- And the permission to that client, **3. (rw, sync, no_subtree_check, no_root_squash)**
And apply changes.

```sh
exportfs -av
```
`exporting 192.168.0.0/24:/var/nfs`. 

Exported the folder with the client/network. Applied! Now we have successfully exposed a directory out to the client. JOB DONE WITH THE `nfs` server. 

**NOW, THE CLIENTS ON THAT CIDR BLOCK CAN ACCESS `/var/nfs`  FOLDER USING `nfs` SERVER.** 

How clients which are lying in the **CIDR** block can access and utilized the exposed `/var/nfs` directory?

WE ARE GOOD WITH THE `NFS` SERVER FOR NOW!.


---
# 10. NFS: Demo for volume type

**GETTING DEEP INTO `NFS` MOUNTING STUFF WITH `NFS` SHARING DATA ACROSS NODES.** 
After setting up the `nfs` server, lets write a manifest implementing a `deployment` object to utilize the `nfs` exposed directory from that storage server to share data across replicas of our PODs out of the deployment object.

Same `deploy.yaml` to `nfs-deploy.yaml` manifest template, with the `nfs` VolumeType to it, 
`nfs-deploy.yaml`,
```yaml
apiVersion: apps/v1
kind: Deployment
metadata:
	name: nfx-nginx-deploy
	labels:
		app: nginx-app
		env: prod
		release: v1.0

spec:
	replicas: 2
	selector:
	matchLabels:
		app: nginx-app
		env: prod
		release: v1.0

template:
	metadata:
	name: nfx-nginx-deploy
	labels:
		app: nginx-app
		env: prod
		release: v1.0

spec:
	containers:
		- name: write-app
		image: alpine
		command: ["/bin/sh"]
		args: ["-c", "while true; do date >> /var/log/index.html; sleep 10; done"]
		resources:
		limits:
			cpu: 100m
			memory: 100Mi
		volumeMounts:
			- name: nfs-volume
			mountPath: /var/log
  
		- name: serve-app
		image: nginx:1.27.4
		ports:
			- containerPort: 80
		resources:
		limits:
			cpu: 100m
			memory: 100Mi
		volumeMounts:
			- name: nfs-volume
			mountPath: /usr/share/nginx/html

volumes:
	- name: nfs-volume
	nfs:
		# server: nfs-server.default.svc.cluster.local
		server: 192.168.0.126
		path: /var/nfs
# readOnly: true #commented out since we have a write container writing to the volume
```

Just the volume and,
```sh
volumes: #type
	- name: deploy-shared-volume #name of the volume 
	nfs: #VolumeType
	server: 192.168.0.125 #ip of the nfs server 
	path: /var/nfs #the directory in that nfs server. 
```

Create and verify,
```
kubectl create -f /path/to/nfs-deploy.yaml
```

WE HAVE INTENTIONALLY MADE AN ERROR HERE TO SIMPLY OBSERVE IT AND TO SEE HOW TO RESOLVE IT,

```
kubectl get deploy,rs,po -o wide
```

```
kubectl get deploy,rs,po -o yaml
```
To verify mounts. 

Will be telling that the container is still creating, so THERE IS A PROBLEM. HOW TO DEBUG IT.
```
NAME      nginx-deploy-97b9d97bd-gcdhd 
READY     0/2 
STATUS    ContainerCreating
```

To check pod under the hood,
```
kubectl describe po podName
```

```
Warning:  
FailedMount  75s (x10 over 5m25s)  kubelet            MountVolume.SetUp failed for volume "deploy-shared-volume" : mount failed: exit status 32

Mounting command: mount
Mounting arguments: -t nfs 192.168.0.125:/var/nfs /var/lib/kubelet/pods/e57628f7-0fab-4f72-81c3-78a0057ca3e3/volumes/kubernetes.io~nfs/deploy-shared-volume

Output: 
mount: /var/lib/kubelet/pods/e57628f7-0fab-4f72-81c3-78a0057ca3e3/volumes/kubernetes.io~nfs/deploy-shared-volume: bad option; for several filesystems (e.g. nfs, cifs) you might need a /sbin/mount.<type> helper program.
       dmesg(1) may have more information after failed mount system call.
```

Here in the events, you can see that 
- it performs the volume mounting with the `nfs` VolumeType. 
- Saying **FailedMount** 
- Executing `mount` command,
- Passing `mount` arguments to perform volume mount over the network to the storage server using `nfs` right to the directory that we exposed. 
- Output that this is mount is not executed well due to failed mount. 
==**The reason for the failed mount is that it is telling that there are more FS packages missing inorder to support the NFS to perform mounting, (Which can be seen in the Output - for several filesystems (e.g. nfs, cifs) you might need a /sbin/mount. helper program. )**==

To further troubleshooting, let see in which node these are the PODs that are getting deployed to. So, lets install the rest of the dependencies into all those nodes and to the storage server. 

INSTALL `nfs` and the rest of its dependencies,
```
sudo apt install nfs-common nfs-kernel-server
```

And now, this got mounted over the network. `NFS` has been performed successfully.

IF THE CLUSTER GETS TO BE DEAD, THE DATA WILL NOT GET TO DUST, SINCE WE ARE STORING ALL IN A REMOTE LOCATION. 

Here for now, i am facing a problem even after installing dependencies, after describing the pod, in events we can see that,
```
Connection refused for 192.168.0.125:/var/nfs on /var/lib/kubelet/pods/e4dc1598-99f5-49bd-9282-502404772651/volumes/kubernetes.io~nfs/deploy-shared-volume

Output: mount.nfs: access denied by server while mounting 192.168.0.125:/var/nfs
```

**Troubleshoot! By passing `describe`, `events` and `logs`.** 

TO verify in deep by listing the PODs and get to know where it is running around and ,
```
kubectl get po -o wide
```

And getting into the node and verify whether the POD has been mounted to which volume, by passing `df -h` command,
```
df -h | grep nfs 
```

Output as:
```
192.168.0.126:/var/nfs              15G  4.5G  9.5G  33% /var/lib/kubelet/pods/ee93d8d9-ef06-4ae6-89f5-8aa1c56da078/volumes/kubernetes.io~nfs/nfs-volume
```
Telling the directory mounted to which, and there it goes. 192.168.0.126 via `nfs`.

And also pass `mount` command to verify mount,
```
mount | grep nfs 
```

And output as,
```
nfsd on /proc/fs/nfsd type nfsd (rw,relatime)
192.168.0.126:/var/nfs on /var/lib/kubelet/pods/ee93d8d9-ef06-4ae6-89f5-8aa1c56da078/volumes/kubernetes.io~nfs/nfs-volume type nfs4 (rw,relatime,vers=4.2,rsize=262144,wsize=262144,namlen=255,hard,proto=tcp,timeo=600,retrans=2,sec=sys,clientaddr=192.168.0.124,local_lock=none,addr=192.168.0.126)
```

Telling,
- Nfs (nfsd on /proc/fs/nfsd type nfsd (rw, relatime) -> mounted to host (192.168.0.126:/var/nfs) -> 
- PodVolume Directory (/var/lib/kubelet/pods/PodVolumeUID/volumes/kubernetes. Io~nfs/nfs-volume) -> 
- type (type nfs4 ) -> 
- rules ((rw, relatime, vers=4.2, rsize=262144, wsize=262144, namlen=255, hard, proto=tcp, timeo=600, retrans=2, sec=sys, clientaddr=192.168.0.124, local_lock=none, addr=192.168.0.126))

Where this `controlplane` directory data got attached to `nfs`  storage server. 

For these types of usecases/scenarios, use `nfs`.

And ping each pod to get the response,
```
curl PodIP1
curl PodIP2
curl PodIP3
```

Use `nfs`, where you can still be able to use the common data in a shared manner. 

Cleanup:
```
kubectl delete deploy deployObjName
```

And check in the node `mount` status, which gets *unmounted*,
```
mount | grep nfs 
```

And ssh into nfs and clear data,
```
ssh username@NodeVMIp(nfs)
```

Delete content, 
```
ls -la /var/nfs/
rm /var/nfs/`files`
```

EVEN THE CLUSTER CRASHES DOWN, THE DATA WILL BE PERSISTENT IN THE `NFS` STORAGE SERVER, where we store data in a remote location and done!



---
# 11 . `NFS`: Jenkins `CI/CD` Deployment Object with active and passive mode (`NFS` volume type)

**Lets see how to deploy `Jenkins` CI/CD Application in the available cluster.** 

And in this case, we **cannot use active and passive mode** where **Kubernetes** itself will self-heal and recreate the same.  

**In that case, will use `Deployment` object, with `replica=1` where the volume storage gets mounted in `nfs` storage server** using Jenkins.

Because, EVEN THE CLUSTER CRASHES DOWN, THE DATA WILL BE PERSISTENT IN THE `NFS` STORAGE SERVER, where we store data in a remote location even when the POD gets recreated. By controller, where it reschedule the PODs to re-spin again on any other node with the volumes mounted together with no data loss. A fresh set of containers with the same data just a flicker of downtime. 

So Here, will see,
- The manifest - `deployment`
- And how to use `nfs`

previously, we have saw the practices to handle volumes to persist even the pod or the node crashes.

We are done with the basics. Now will see the same in brief by deploying a Jenkins Service with `Deployment` Object with `NFS` using Jenkins. 

```yaml
apiVersion: apps/v1
kind: Deployment
metadata:
	name: jenkins-nfs-deploy
	labels:
		app: cicd-server
		env: prod
		release: v1.0

spec:
# replicas: 3
	selector:
	matchLabels:
		app: cicd-server
		env: prod
		release: v1.0
	template:
		metadata:
			name: jenkins-nfs-deploy
			labels:
				app: cicd-server
				env: prod
				release: v1.0

	spec:
		containers:
		  - name: cicd-server
			image: jenkins/jenkins:lts
			ports:
			- name: cicd-port
			  containerPort: 8080
			  protocol: TCP
			volumeMounts:
			- name: jenkins-nfs
			  mountPath: /var/jenkins_home #var/lib/jenkins
	
	volumes:
	- name: jenkins-nfs
	nfs:
		server: 192.168.0.126
		path: /var/nfs
```

And ensure the path directory exists in the `nfs` server too. Or else
```
ssh username@nfsServerIp
mkdir /var/nfs/volumeName
```

After that, create resource,
```
kubectl create -f /path/to/jenkinsNfsDEploy.yaml
```

Check status,
```
kubectl get deploy,rs,po -o wide
```

`CrashLoopBackOff`. If any issue, troubleshoot be checking the status and logs, to do that,
```
kubectl describe po podName
kubectl events po podName
```

To check logs,
```
kubectl logs podName
```
==**DOES NOT HAVE REQUIRED PERMISSIONS.**==
Throws this, **==WHY?==**
```
INSTALL WARNING: User:  missing rw permissions on JENKINS_HOME: /var/jenkins_home

touch: cannot touch '/var/jenkins_home/copy_reference_file.log': Permission denied

Can not write to /var/jenkins_home/copy_reference_file.log. Wrong volume permissions?

stream closed EOF for default/jenkins-nfs-deploy-856db8b78b-gttdh (cicd-server)
```

**In the logs, it is telling that we doesn't have enough permissions.** so, the scenario is that, 

Since, One of the container in the deployment object is a writing data and that too is getting stored in `nfs`. THAT WAS THE SETUP RIGHT?, Here,

But who is trying to write that data in that mount nfs container mount? IT IS JENKINS USER. And that doesn't have enough permissions.

>  tldr: `nfs` have permissions only to the root (no permission to no user, no group). But the write container is who are writing the data under the mounted folder(which have permissions only to the root)? JENKINS which doesn't have required permissions to write in a restricted directory which doesnt have the permissions.

Practically two solutions, 
1) change the directory's permission access to users or
2) give the permission to the user. (
What to do, change permissions - groupid1000)
Will see how we do that,

*Back to `nfs`, cannot give `JENKINS` the permissions cause, it expects the user to be present on the host `nfs`., instead, use `userID`

```
ssh username@nfsServerIpVMIp
```

```
sudo chown -R 1000:1000 /var/nfs/jenkins-nfs(mountDir)
```

And voila. Runs smooth like butter. Verify the same,
```
kubectl get deploy, rs, po -o wide
```

```
kubectl exec -it jenkinsPodName --bash 
```

```
id
```
Output: `uid=0(root) gid=0(root) groups=0(root)`

Expose the same to the application which u deploy. 

Verify `JENKINS` home directory.
```
ca /etc/passwd | grep jenkins
```
Shows the `user permission:homeDir: shell`

ALL GOOD!. WORKS JUST FINE! 

==**now IN ORDER TO ACCESS JENKINS, WILL BE EXPOSING THE `Deployment` Object USING `NODEPORT`**==

For reference:
**WORKFLOW OVERVIEW**
![[Nodeport-workflow.excalidraw]]

WORKFLOW IN BRIEF:
![[Nodeport-workflow-brief.excalidraw]]


```
apiVersion: v1
kind: Service
metadata:
	name: jenkins-nodeport-svc
	labels:
	  app: nodeportsvc
  
spec:
	selector:
	  app: cicd-server
	  env: prod
	  release: v1.0

ports:
	- name: cicd-port
	  port: 80
	  targetPort: 8080
	  protocol: TCP
type: NodePort
```

WITH THIS, WE HAVE SUCCESSFULLY DEPLOYED `JENKINS`.
And use the JENKINS in ease. 

Here, WE DO SO MUCH WORK, ALSO PRODUCING LOT OF DATA, SUCH AS,
- Logs,
- Artifacts
- .csv's
- Configs
- .dotfiles
- Dumps
- Jobs
- Deployment cleanups
- Outputs
- And much more data. 

*What happens if a POD gets deleted, Will K8s recreates the POD and these existing workload gets vanished and will all these be new and gets vanished?* 
**Simple. Data is stored well and good in `nfs` server and even after the POD gets deleted and recreated, nothing inside will be washed out.**

To verify, put some workload in the same and delete,
```
kubectl delete po podName
```
And see if anything persist inside still.

THIS IS HOW IT WORKS. 
**Though it is a new pod, the files inside are still intact from the `nfs` server getting utilized here.** With this, we have seen how to deploy Jenkins Application on K8s Cluster as a `deployment` object exposed in `nodeport` with the `nfs` volumeType - only one replica. 

If we are trying to run multiple replica on HA setup and such, `nfs` won't allow if one container writing the data and another container assessing the same file, it wont allow.  THAT IS WHY, Jenkins can be deploy as only one replica. 

IF WE ARE LOOKING TO RUN THE SAME IN MULTIPLES O `HA` SETUP - Have to use External LOAD BALANCERS (takes extra work to write scripts, more work and all) or deploy `Jenkinsx`.

SO THIS IS THE SETUP TO DEPLOY `JENKINS` ON K8s. 

###### Cleanup,
```
kubectl delete -f /path/to/manifest
```

```
ssh username@nfsServerIpVMIp 
cd /var/nfs/jenkins-master
```



---
# 12.  `downwardAPI`  overview: (refs)

The last one - **`downwardAPI` VolumeType**  
If you are looking forward to implement `downwardAPI`  as either as a **VOLUME** or as **ENVIRONMENT VARIABLES**, there are two parameters to it,
- **fieldRef**
- **resourceFieldRef**
**These two parameters we can pass to the applications that are running inside a container.** 

As per [documentation](https://kubernetes.io/docs/concepts/workloads/pods/downward-api/)
```cardlink
url: https://kubernetes.io/docs/concepts/workloads/pods/downward-api/
title: "Downward API"
description: "There are two ways to expose Pod and container fields to a running container: environment variables, and as files that are populated by a special volume type. Together, these two ways of exposing Pod and container fields are called the downward API."
host: kubernetes.io
favicon: https://kubernetes.io/icons/favicon-64.png
image: https://kubernetes.io/images/kubernetes-open-graph.png
```
[Downward API \| Kubernetes](https://kubernetes.io/docs/concepts/workloads/pods/downward-api/)

###### **Will be using this `Deployment` object's manifest in order to tinker with `DownwardAPI` VolumeType.** - 

**FOR `FieldRef`**
[For Reference](https://github.com/rithishsamm/myTutorialHell/blob/main/Orchestration/k8engineers.com/kubernetes_latest_manifest/Kubernetes/07-Storage/downwardapi_env.yaml[Fetching Data#m8ls](https://github.com/rithishsamm/myTutorialHell/blob/main/Orchestration/k8engineers.com/kubernetes_latest_manifest/Kubernetes/07-Storage/downwardapi_env.yaml))
```cardlink
url: https://github.com/rithishsamm/myTutorialHell/blob/main/Orchestration/k8engineers.com/kubernetes_latest_manifest/Kubernetes/07-Storage/downwardapi_env.yaml
title: "myTutorialHell/Orchestration/k8engineers.com/kubernetes_latest_manifest/Kubernetes/07-Storage/downwardapi_env.yaml at main · rithishsamm/myTutorialHell"
description: "personalDocumentation. Contribute to rithishsamm/myTutorialHell development by creating an account on GitHub."
host: github.com
favicon: https://github.githubassets.com/favicons/favicon.svg
image: https://opengraph.githubassets.com/6a8706450e2229126c40e2b3c57d8f7633b42c9d769d991e36d0c55623b7dcfc/rithishsamm/myTutorialHell
```


```yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: nginx-deploy
  labels:
    app: nginx
    environment: production
    
spec:
  replicas: 3
  selector:
    matchLabels:
      environment: production
      
  template:
    metadata:
      name: nginx-demo
      labels:
        app: nginx
        environment: production
        
    spec:
      containers:
      - name: write-container
        image: alpine
        command: ["/bin/sh"]
        # args: ["-c", "while true;do echo `date`: $POD_IP | tee -a /var/log/index.html >> /proc/1/fd/1; sleep 10;done"]
        args: ["-c", "while true;do echo `date`: $POD_IP: $POD_NAME | tee -a /var/log/index.html >> /proc/1/fd/1; sleep 10;done"]

        env:
          - name: POD_IP
            valueFrom:
              fieldRef:
                fieldPath: status.podIP

          - name: POD_NAME
            valueFrom:
              fieldRef:
                fieldPath: metadata.name
          # and more environment variables can be added here...

        volumeMounts:
          - name: shared-data
            mountPath: /var/log
      volumes:
        - name: shared-data
          emptyDir: {}
```

Will use this manifest in order to tinker with `DownwardAPI` VolumeType. 


---
# 13 . **fieldRef** *`downwardAPI`*: Demo and overview:

```cardlink
url: https://kubernetes.io/docs/concepts/workloads/pods/downward-api/#downwardapi-fieldRef
title: "fieldRef Downward API"
description: "There are two ways to expose Pod and container fields to a running container: environment variables, and as files that are populated by a special volume type. Together, these two ways of exposing Pod and container fields are called the downward API."
host: kubernetes.io
favicon: https://kubernetes.io/icons/favicon-64.png
image: https://kubernetes.io/images/kubernetes-open-graph.png
```
[FieldRef Downward API \| Kubernetes](https://kubernetes.io/docs/concepts/workloads/pods/downward-api/#downwardapi-fieldRef)

##### Under **`fieldRef`** (parameters for general POD level Components)
There are some Pod-level fields, you can provide them to a container either as an **environment variable** or using a **`downwardAPI` volume**. - provides POD specific information. 

##### General (env var + volume):
*can provide these parameters to a container either as an **environment variable** or using a **`downwardAPI` volume***
- **`metadata.name`** the pod's name
- **`metadata.namespace`** - the pod's [namespace](https://kubernetes.io/docs/concepts/overview/working-with-objects/namespaces)
- **`metadata.uid`** - the pod's unique ID
- **`metadata.annotations['<KEY>']`** -  the value of the pod's [annotation](https://kubernetes.io/docs/concepts/overview/working-with-objects/annotations) named `<KEY>` (for example, `metadata.annotations['myannotation']`)
- **`metadata.labels['<KEY>']`** - the text value of the pod's [label](https://kubernetes.io/docs/concepts/overview/working-with-objects/labels) named `<KEY>` (for example, `metadata.labels['mylabel']`) 

##### Environment variables only:
*available as **environment variables** but not as volume:*
- **`spec.serviceAccountName`** - the name of the pod's [service account](https://kubernetes.io/docs/tasks/configure-pod-container/configure-service-account/)
- **`spec.nodeName`** - the name of the [node](https://kubernetes.io/docs/concepts/architecture/nodes/) where the Pod is executing
- **`status.hostIP`** - the primary IP address of the node to which the Pod is assigned
- **`status.hostIPs`** - the IP addresses is a dual-stack version of `status.hostIP`, the first is always the same as `status.hostIP`.
- **`status.podIP`** - the pod's primary IP address (usually, its `IPv4` address)
- **`status.podIPs`** - the IP addresses is a dual-stack version of `status.podIP`, the first is always the same as `status.podIP`

### FieldRef brief:
Here, as we are using [DownwardAPI manifest](https://github.com/rithishsamm/myTutorialHell/blob/main/Orchestration/k8engineers.com/kubernetes_latest_manifest/Kubernetes/07-Storage/downwardapi_env.yaml[Fetching Data#m8ls](https://github.com/rithishsamm/myTutorialHell/blob/main/Orchestration/k8engineers.com/kubernetes_latest_manifest/Kubernetes/07-Storage/downwardapi_env.yaml)) ![[00.Section13-K8sStorage(Volumes)#**Will be using this `Deployment` object's manifest in order to tinker with `DownwardAPI` VolumeType.** -]]
###### FieldRef with env vars:
Here in this manifest, 
- This a one container POD on a `Deployment` object. 
- here in this container, runs a `while` loop for every ten seconds, 
- The loops echoes the **$PodIP** every 10 Seconds in the webpage located in `/var/log/index.html` to print in it and also in /proc/l/fd/1 to execute the same in the standard output too.  
THE CORE IS, 
>  HOW COME WE KNOW ABOUT THE **PodIP** AND TELL IT TO THE CONTAINER TO GET IT TO KNOW ABOUT THE `POD` 's IP. 
==For that, we use **DownwardAPI**. ==, In `DownwardAPI`, will pull the **PodIP** address using **ENVIRONMENT VARIABLES**. 
How to fetch the PodIP? - `FieldRef`

Will see how it works in action:

Create and verify `deployment` object:
```
kubectl create -f /path/to/downwardAPI.yaml
```
```
kubectl get deploy,rs,po -o wide 
```

Output?
```
kubectl logs podName1
kubectl logs podName2
kubectl logs podName3
```
 Prints its own **PodIP**, since it is a dynamic value which we couldn't be able to determined to execute what to run in  

Just like these `PodIP` we fetch multiple values as we seen in the above given **env vars**, This is what we do using `DownwardAPI`. 

**==SO, THIS IS HOW WE USE FieldRef `DownwardAPI` in env vars. 

NOW WILL SEE HOW TO USE THE SAME, resourceFieldRef `DownwardAPI` ==** - store information in env vars


##### Volumes only:
*available as **downwardAPI volume** but not as  environment variables :*
-  **`metadata.labels`** - all of the pod's labels, formatted as `label-key="escaped-label-value"` with one label per line
- **`metadata.annotations`** - all of the pod's annotations, formatted as `annotation-key="escaped-annotation-value"` with one annotation per line

###### FieldRef with volumes:
```yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: nginx-deploy
  labels:
    app: nginx
    environment: production
    
spec:
  replicas: 3
  selector:
    matchLabels:
      environment: production
      
  template:
    metadata:
      name: nginx-demo
      labels:
        app: nginx
        environment: production
        
    spec:
      containers:
      - name: write-container
        image: alpine
        command: ["/bin/sh"]
        args: ["-c", "while true; do echo \"Time: $(date +%r) POD_NAME: $(cat /var/log/index.html)\" > /proc/1/fd/1; sleep 10; done"]
        env:
          - name: POD_IP
            valueFrom:
              fieldRef:
                fieldPath: status.podIP

          - name: POD_NAME
            valueFrom:
              fieldRef:
                fieldPath: metadata.name
          # and more environment variables can be added here...

        volumeMounts:
          - name: shared-data
            mountPath: /var/log
      volumes:
        - name: shared-data
          downwardAPI:
            items:
              - path: "index.html"
                fieldRef:
                  fieldPath: metadata.name
              # - path: "status.podIP"
              #   fieldRef:
              #     fieldPath: status.podIP
```

Here, similar to the ENVIRONMENT VARIABLES, 

Create and verify `deployment` object:
```
kubectl create -f /path/to/downwardAPI.yaml
```
```
kubectl get deploy,rs,po -o wide 
```

Output?
```
kubectl logs podName1
kubectl logs podName2
kubectl logs podName3
```

Printing,
- **Time Parameter,**
- The string we passed,
- **The PodName**,
**==THIS IS HOW YOU DO, IF YOU WANT TO FETCH VARIABLES AND STORE SUCH FILES/DATA IN THE VOLUME - ==DownwardAPI with FieldRef Volumes====** - store information in VOLUMES.


---
# 14 . **resourceFieldRef** *`downwardAPI`*: Demo and overview

```cardlink
url: https://kubernetes.io/docs/concepts/workloads/pods/downward-api/#downwardapi-resourceFieldRef
title: "resourceFieldRef Downward API"
description: "There are two ways to expose Pod and container fields to a running container: environment variables, and as files that are populated by a special volume type. Together, these two ways of exposing Pod and container fields are called the downward API."
host: kubernetes.io
favicon: https://kubernetes.io/icons/favicon-64.png
image: https://kubernetes.io/images/kubernetes-open-graph.png
```
[resourceFieldRef Downward API \| Kubernetes](https://kubernetes.io/docs/concepts/workloads/pods/downward-api/#downwardapi-resourceFieldRef)

##### Under **`resourceFieldRef`** (for Container level Components - for the workload's hard resources:
- **`resource: limits.cpu`** - A container's CPU limits
- **`resource: requests.cpu`** - A container's CPU request
- **`resource: limits.memory`** - A container's memory limits
- **`resource: requests.memory`** - A container's memory request
- **`resource: limits.hugepages-*`** - A container's hugepages limit
- **`resource: requests.hugepages-*`** - A container's hugepages request
- **`resource: limits.ephemeral-storage`** - A container's ephemeral-storage limit
- **`resource: requests.ephemeral-storage`** - A container's ephemeral-storage request

> *If CPU and memory limits are not specified for a container, and you use the downward API to try to expose that information, then the kubelet defaults to exposing the maximum allocatable value for CPU and memory based on the [node allocatable](https://kubernetes.io/docs/tasks/administer-cluster/reserve-compute-resources/#node-allocatable) calculation.*

Have seen - `FieldRef` and the types to pass parameters. Will see how to do the same in `resourceFieldRef`. This has to do the container specific information.

###### resourceFieldRef with env vars:
Will pick the same manifest here too,
![[00.Section13-K8sStorage(Volumes)#**Will be using this `Deployment` object's manifest in order to tinker with `DownwardAPI` VolumeType.** -]]

In order to pass these `resourceFieldRef` parameters, there should be container resources under the pod's container,
Resources such as,
- Cpu
- Memory
- Ephimeral storage
- And such

Instead of adding parameters externally, will pass it here,
Here will use, only CPU and memory, 
**Just parameters under under Container section for the resourceFieldRef to be used,**
```yaml
#<boilerplate-apiV,kind,spec,template,containers>
#under the containers section,pass resources and environment variables field. 

resources:
  limits:
    memory: "250Mi" #250MB
    cpu: "250m" #20% of a single core
  requests:
    memory: "100Mi" #100MB
    cpu: "100m" #10% of a single core
env:
  - name: CPU_LIMIT
    valueFrom:
      resourceFieldRef:
        containerName: write-container
        resource: limits.cpu
  - name: MEMORY_LIMIT
    valueFrom:
      resourceFieldRef:
        containerName: write-container
        resource: limits.memory
  - name: POD_IP
    valueFrom:
      fieldRef:
        fieldPath: status.podIP
  - name: POD_NAME
    valueFrom:
      fieldRef:
        fieldPath: metadata.name
  # and more environment variables can be added here...
```

Create and verify
```
kubectl create -f /path/to/downwardapiResourceFieldVol.yaml
```
```
kubectl get deploy,rs,po -o wide
```

To check the environments available to these containers,
```
kubectl exec -ti po podName -- env
```

Pointless to print resources but to check the resources used here. 

==**THIS IS HOW YOU USE `ResourceFieldRef` to pull the values of the resources and put it in the ENV VARIABLES .**  ==

Will see how to perform  the same with Volumes and using the same manifest but to modify some values in the same. (NOTHING MUCH BUT KEEP Resources and REMOVE THE ENV VARS)

Manifest:
- Remove env vars 
-  pass appropriate arguments
- Change the same in the volumes sections. 
```
#<boilerplate-apiV,kind,spec,template,containers>
#under the containers section,pass resources and environment variables field. 

args: ["-c", "while true; do echo \"Time: $(date +%r) CPU_LIMIT: $(cat /var/log/cpu_limit.txt) CPU_REQUEST: $(cat /var/log/cpu_request.txt) MEMORY_LIMIT: $(cat /var/log/memory_limit.txt) MEMORY_REQUEST: $(cat /var/log/memory_request.txt)\" > /proc/1/fd/1; sleep 10; done"]

...

volumes:
  - name: shared-data
    downwardAPI:
      items:
        - path: cpu_limit.txt
          resourceFieldRef:
            containerName: write-container
            resource: limits.cpu
        - path: cpu_request.txt
          resourceFieldRef:
            containerName: write-container
            resource: requests.cpu
        - path: memory_limit.txt
          resourceFieldRef:
            containerName: write-container
            resource: limits.memory
        - path: memory_request.txt
          resourceFieldRef:
            containerName: write-container
            resource: requests.memory
```

```
kubectl create -f /path/to/downwardAPI-resFieldVol.yaml
```
```
kubectl logs po podName
```


Output:
Prints the VARIABLE's values for every ten seconds. 
```
Time: 11:38:29 AM CPU_LIMIT: 1 CPU_REQUEST: 1 MEMORY_LIMIT: 209715200 MEMORY_REQUEST: 104857600
```

THIS IS HOW TO PULL THE INFO USING `DownwardAPI` using `Volumes` under `ResourceFieldRef` .

These are all the four ways to pull info out of `DownwardAPI` . 

---

