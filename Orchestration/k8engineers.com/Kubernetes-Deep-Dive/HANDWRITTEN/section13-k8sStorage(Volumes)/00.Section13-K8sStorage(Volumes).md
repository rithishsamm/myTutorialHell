##### Section 13: K8s Storage (Volumes)
1. Introduction to Kubernetes Storage types
		Introduction to Kubernetes Storage types- [Doc]
2. Overview on Kubernetes Volumes
		Overview on Kubernetes Volumes - [Doc]
3. Working principle of emptyDir volume type
		Overview on Kubernetes Volumes - [Doc]
4. EmptyDir volume type (Disk and Memory)
		EmptyDir Demo for emptyDir volume type (Disk and Memory) - [Doc]
5. Working principle of hostPath volume type
		HostPath Working principle of hostPath volume type - [Doc]
6. HostPath volume type (Directory and DirectoryOrCreate)
		HostPath Demo for hostPath volume type (Directory and DirectoryOrCreate) - [Doc]
7. HostPath volume type (File and FileOrCreate)
		HostPath Demo for hostPath volume type (File and FileOrCreate) - [Doc]
8. Working principle of nfs volume type
9. Setup NFS server for Kubernetes Volume Demo
10. NFS volume type
11. Jenkins CICD Deployment Object with active and passive mode (NFS volume type)
12. DownwardAPI (Information: fieldRef)
13. DownwardAPI (Information: resourceFieldRef)

---

# Section 13: K8s Storage (Volumes)
# 1. Introduction to Kubernetes Storage types

The next very important topic in this K8s - Storage (Volumes)
	Which is so critical for applications and for the clusters. Especially for **Applications** running on clusters. 

What's is in it?
First, need to know about 
- Volumes (followed by)
- Persistent Volumes (pv) (and the differences),
- Static and dynamic persistent volumes
- Persistent volume claim (pvc)
- Static and dynamic persistent volume claims
- Storage class
And more storage types such as,
- Volume expansion (volume scaling)  
- Volumes snapshot
- Dynamic Volume provisioning
- And more advanced storage types. 

To start with storage. Will get familiar with volumes and proceed to more advanced types as we listed above. 

And see more of 
- When to use it,
- Where to use it,
- And use cases. 

Lets get started with volumes,

---
# 2. Overview on Kubernetes Volumes
Overview on Kubernetes Volumes - [Doc]

Volumes -
- Purpose of volumes,
- Why volumes for the workload resources
- When to use
- Where to use
- Limitations of volumes 
- Pros and cons of volumes
- Volumes for both Pods and also the Containers

==**Volumes for Pods are persistent . 
Volumes for containers are ephemeral.**==
Will dig in deep about storage and volumes in brief. 
- In general, containers are **ephemeral** which files will be lost once a containers crashes.
-  Since kubelet restarts containers which comes with clean state data will be lost. 
- When we want to **share data** across container inside the POD, we can use volumes.
- When containers restarts with clean state, POD volume will help to persist the data. Since POD uses volumes as persistent storage solution for containers. 
- Container mounts directory under `.spec.containers[*].volumeMounts` to volumes under `.spec.volumes` with common name.
- K8s supports two types of volumes,
	- Ephemeral volumes (emptyDir,  configMap, secret, downwardAPI, CSI ephemeral volumes. Etc.,)
	- Persistent volumes (PV -> Static and dynamic with StorageClass)
- Volume types supported by K8s,
	- AWS EBS CSI
	- AzureDisk CSI, [[azure_disk]]
	- GCE CSI
	- vSphere CSI
	- CephFS, rbd
	- ConfigMap, secret, downwardAPI, hostPath
	- iSCSI, local, NFS [[nfs-server-setup]]
	- PVC
	- Projected

###### In general, containers are ephemeral which files will be lost once a containers crashes.
Eg: 
- take a POD.
- No matter if it is a standalone or workload resource based POD.
- And that POD contains containers. 
- Container1, Container2
- Until a container gets recreated, the data will be on that container. 
- If a container crashes, data crashes and lost. Where `kubelet` tries to recreate the POD and along with containers. from scratch (like creating a new VM, so the configurations, data's and volumes)
That way containers - ephemeral volumes. Till a container is alive, the data will be alive.  

Once a container goes down, `kubelet` will recreate a new container which results in losing data. 

######  Since kubelet restarts containers which comes with clean state data will be lost. 
Since kubelet restarts containers which comes with clean state data will be lost. -> `kubelet` doesn't restarts the container. It recreates. 

Like in docker, docker container stop -> rm -> run. Not docker restart.  Only recreation, no restarts. Again, which results in losing data creating a new container. 
>  cannot use as persistent volumes, cannot store the data persistently. Cannot modify the configs or the containers itself when the storage is ephemeral.   

###### When we want to share data across  container inside the POD, we can use volumes.
To **share data** across container inside the POD, -> use ==volumes==. 

If we recall the definition of PODs. [[00. section3-full-k8sPods]]] 

> [!NOTE] **What is a POD**
> - ==Pod **creates a logical layer**  
>     to group 1 or more containers = to have **common network** + **shared storage.**==
> - Pod has a unique IP address assigned from `--pod-network-cidr=10.244.0.0/16`
> - Containers inside those POD talks to each other on `localhost` domain or `127.0.0.1`, since container share the same network stack.
> - Containers share data inside POD.
> - In general, we need to create containers inside POD  
>     which are dependent, not different application.

> Point to recall - Pod **creates a logical layer**  
    to group 1 or more containers = to have **common network** + **shared storage.**

The term **shared storage** means here is that -> We can share the data
- **across containers** ->  **inside the PODs.** 
Eg: like what we have done with the **nginx-multicontainer** - `deploy.yaml`

Where we saw two container's working directory, sharing same volumes out of PODs `EmptyDir()` volume.  
![[PodvsContainerStorage]]
**Purpose/Objective:** Volumes used inside the Pod to share the data across the containers. 
**Benefit:** 
- Shared volume across containers inside a POD (using `EmptyDir()`). 
- If a container gets crashed and gets recreated freshly from scratch inside the POD, the data will be persistent not get washed.  (*Data will not be lost which that came out of that dead container, where the volume didn't get deleted.*)
- Where the volume will be deleted if the POD gets deleted, not the container.
> Volumes - a shared storage volume solution to work with the data of the containers inside the POD. 

###### When containers restarts with `clean state`, POD volume will help to persist the data. Since POD uses volumes as persistent storage solution for containers. 
When containers restarts with **clean state**, *POD volume will help to persist the data.*

Since POD uses volumes as persistent storage solution for containers which we just discussed in the above point.  

The core part of the volume is that to share the volume, to provision a persistent storage. Even if it gets crashed and recreated, the volume will not be lost. 

**==For container -  it is a persistent storage, 
For POD - its ephemeral 
where if the POD gets f'ed up and so the volumes too.==**  


###### Container mounts directory under `.spec.containers[*].volumeMounts` to volumes under `.spec.volumes` with common name.

Volume mounting - Container mounts directory under 
> -  `.spec.containers[*].volumeMounts` - **Containers**
> - to volumes under `.spec.volumes` with common name.


###### K8s supports two types of volumes,
 - **Ephemeral/Temp volumes (emptyDir,  configMap, secret, downwardAPI, CSI ephemeral volumes. Etc.,)**
As we all know, once f'ed, it's f'ed. 
processes like for eg: RAM's memory, CPU's processes and such.  - which couldn't be persistent where everything gets washed once it gets restarted. 

- **Persistent volumes (PV -> Static and dynamic with StorageClass)**
Persistent is persistent. Like EG: Hard disks, SSD, NAS , Memory card, Pen drive and such.
Persistent - Exists even if there is a restart. 

>  These are the two types of Volumes in K8s. 

In these two types of volumes, there are sub types/ format to volumes, and here there are certain volumes types supported in K8s,
###### Volume types supported by K8s, 
> Note: **CSI, Container Storage Interface** - plugins
- AWS EBS CSI (AWS Elastic Block Storage )
- AzureDisk CSI, [[azure_disk]]
- GCE CSI
- vSphere CSI
- Openstack CSI
- CephFS, rbd
- ConfigMap, secret, downwardAPI, hostPath
- iSCSI, local, NFS [[nfs-server-setup]]
- PVC
- Projected

Varies based on different environments. Takes these plugins, volume types in order to store the data. 

Will dig deep into all the volumes types and see which applies where. 

Since we could not be able to store things with what we are doing, will see ,
- How to work with this 
- How to share data between workloads
- How to persist the data and such. 


---
# 3. EmptyDir(): Working principle of `emptyDir` volume type
Overview on Kubernetes Volumes - [Doc]
```
VolumeType: 
- EmptyDir
```

###### Let see about one of the volume type - **`EmptyDir()`**
And the working principle of `emptyDir` volume type.

Critical points and 
**Working Principles of `EmptyDir()`:**
![[emptyDir()]]

> Nothing different. Exactly same as what we were doing to the `nginx-multicontainer` workload using `emptyDir()` VolumeType for volume which is an **Ephemeral VolumeType**

- We got an application running on a K8s Cluster having one **controlplane** and two **workernodes**
- Application deployed using Deployment workload resource object. 
- Deployment, `replica=3`, 2 Pods (Pod1, Pod3) on workernode1, one (Pod2) on workernode2. (which gets appropriately distributed based on multiple metrics)
-  here, whenever the Pod uses `EmptyDir()`, which creates a volume directory under `/var/lib/kubelet/pods/Pod'sUID/volumes/kubernetes.io~empty-dir/volume_name/data`
- Creates `emptyDir()` under this directory path. 
- Exists until the POD is available. 
- Applies to all the Pods exists here. 
- Here, the context is the The WORKDIR of the container -> mounted -> to `emptyDir()` of the POD.
>  - so For the container, it is persistent
>  - for the PODs, its ephemeral. 

**This is not a wise choice if you want the data to be persistent (good for batch, not recommended for Production purpose) since the `emptyDir()` is totally ephemeral.** 

If we happened to migrate these workloads from one node to to other node, its a brand new directory gets created from scratch which couldn't be reutilized.

Not a viable storage solution for deploying mission critical applications on PODs. 

Will dig deep about this and see why this does and doesn't work in **Practice: `EmptyDir()`**

---
# 4 . `EmptyDir()`: Demo for `emptyDir()` volume type (Disk and Memory)
EmptyDir Demo for emptyDir volume type (Disk and Memory) - [Doc]

Practical's on Implementing the `EmptyDir()` VolumeType. 

Will see this by creating a simple standalone POD having 2 Containers with the `emptyDir()` VolumeType (or any Workload Resource type as per convenience).

Manifest for the Standalone  `pod.yaml` having 2 containers with **emptyDir ()** VolumeType. 
```yaml
apiVersion: v1 #apiVersion of the Pod
kind: Pod #Kind of the Object
metadata: #Metadata of the Pod (name, labels as key-value pairs)
  name: standalone-pod
  labels:
    app: nginx
    env: dev
    release: v1.0

spec: #Spec of the Pod
  containers: #Containers under the Pod
  - name: write-app
    image: alpine
    command: ["/bin/sh"]
    args: ["-c", "while true; do date >> /var/log/index.html; sleep 10; done"] #Command that writes the data to the emptyDir() PodVolume
    volumeMounts: #VolumeMounts of the Container to the emptyDir() PodVolume.
    - name: standalone-pod-volume
      mountPath: /var/log #workDir of the Container mounted to the emptyDir() PodVolume

  - name: serve-app
    image: nginx:latest
    ports:
    - containerPort: 80 #Port of the Container
    volumeMounts:
      - name: standalone-pod-volume
        mountPath: /usr/share/nginx/html
        #workDir of the Container mounted to the emptyDir() PodVolume, serving the data from the emptyDir() PodVolume

  volumes:
  - name: standalone-pod-volume #Name of the emptyDir() PodVolume
    emptyDir: {} #emptyDir() PodVolume
```

```
kubectl create po /path/to/pod.yaml
kubectl po -o wide
```

SO, THE POD IS RUNNING THO!

Here, Regarding Volume where we ran, `emptyDir`, 
Check , where it is running. 

AND DIAGNOSE THE POD:
```
ssh username@NodeVMIp
```

Where the POD gets to run in a particular Node. 
```
ls -l /var/lib/kubelets/pods
```
Where you can see all the PODs running on that node named with an `uid`. 

HOW DO WE KNOW WHAT `uid` USES WHICH POD? Go back to `controlplane` and Diagnose the **PodIDs**.

```
kubectl get po
```
Get the PodName, and Inspect it,
``` 
kubectl get po podName -o yaml
kubectl get po podName -o yaml | grep uid
```
After getting the UID from `controlplane`, get back to node and 
```
ls -l /var/lib/kubelet/pods/podUID/volumes/kubernetes.io~empty-dir/PodVolumeName/<data>
```
Here, all the respective data will be getting stored with the shared manner. 

```
cat <data>
curl PodIP
```
Which is the data what goes there, gets shared too. 
 >  ==**Here, one container is writing the data in that volume, and the other container is serving that data from that volume.**==

**Where is `EmptyDir)()` here?** 
-> back to Node, this shared volume works as same or moreover similar to the Volume mount, Bind mount, tmpfs.

Delete pod and verify,
In `controlplanenode`
```
kubectl delete po PodName
```
In `computeplanenode`
```
ls -l /var/lib/kubelet/pods/UID/
```
Will not exist since we removed the POD. `No such file or Directory`.

Here, if we recreate or whatever modification that we perform or to recreate the same POD here,  IT WILL NOT BE AS SAME AS IT WAS. 

And there will be bunch of mismatches in multiple levels. 
- Ip will not be the same
- PodUID 
- PodIP
- Pod placement in the clusters - Node selection
- **Especially, Data will be so lost, since it is not persistent.** 
- And much more.

**WHERE IS THE DATA GETS STORED - IS IT THE RAM or DISK OF THE HOST MACHINE!?**
Create the POD once again. 

```
kubectl create -f path/to/standalonepod.yaml
kubectl get po -o wide
```
POD gets created and running in a clean state. 

 Get the UID and back to the workernode where the POD is running,
 ```
 kubectl get po -o yaml | grep uid 
 ```
And to the workernode,
```
ls -l /var/lib/kubelet/pods/<uid>/volumes/kubernetes.io/shared-data/<*data>
```
Where the `<*data>` gets stored?
The POD is created on the host machine tho.  Where in the host machine the data lies at? Speaking of the storage medium of the host machine - all we have is RAM which is volatile and Storage Disk which is non-volatile.

Here, If we don't pass any parameters, the data will be stored in the disk by default. 

```
df -h
mount | grep -i emptyDir
mount | grep -i podUID
```
Which that the volume got mounted to ==**`tmpfs`**==,

The entire Filesystem is stored on the disk tho. Find out by passing `df -h`.

To double check, go back to controlplane, and login to each container,
```
kubectl exec -it -u 0 PodName -c ContainerName -- sh
```

After getting into the shell,
```

```




---
# 5. `hostPath`:  Working principle of hostPath volume type
HostPath Working principle of hostPath volume type - [Doc]

 

---
# 6. `hostPath`: Demo for `hostPath`  volume type (Directory and DirectoryOrCreate)
HostPath Demo for hostPath volume type (Directory and DirectoryOrCreate) - [Doc]



---
# 7 . `hostPath`: Demo for `hostPath` volume type (File and FileOrCreate)
HostPath Demo for hostPath volume type (File and FileOrCreate) - [Doc]



---
# 8. NFS: Working principle of nfs volume types



---
# 9. NFS: Setup NFS server for Kubernetes Volume Demo




---
# 10. NFS: Demo for volume type




---
# 11 . NFS: Jenkins CICD Deployment Object with active and passive mode (NFS volume type)



---
# 12 . `downwardAPI`: Demo on `downwardAPI` (Information: fieldRef)



---
# 13 . `downwardAPI`: Demo on `downwardAPI` (Information: resourceFieldRef)




---

