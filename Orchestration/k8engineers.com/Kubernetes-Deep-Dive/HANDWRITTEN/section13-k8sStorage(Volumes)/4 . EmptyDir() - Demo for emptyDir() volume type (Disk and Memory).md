- EmptyDir Demo for emptyDir volume type (Disk and Memory) - Doc


EmptyDir Demo for emptyDir volume type (Disk and Memory) - [Doc]

Practical's on Implementing the `EmptyDir()` VolumeType. 

Will see this by creating a simple standalone POD having 2 Containers with the `emptyDir()` VolumeType (or any Workload Resource type as per convenience).

Manifest for the Standalone  `pod.yaml` having 2 containers with **emptyDir ()** VolumeType. 
```yaml
apiVersion: v1 #apiVersion of the Pod
kind: Pod #Kind of the Object
metadata: #Metadata of the Pod (name, labels as key-value pairs)
  name: standalone-pod
  labels:
    app: nginx
    env: dev
    release: v1.0

spec: #Spec of the Pod
  containers: #Containers under the Pod
  - name: write-app
    image: alpine
    command: ["/bin/sh"]
    args: ["-c", "while true; do date >> /var/log/index.html; sleep 10; done"] #Command that writes the data to the emptyDir() PodVolume
    volumeMounts: #VolumeMounts of the Container to the emptyDir() PodVolume.
    - name: standalone-pod-volume
      mountPath: /var/log #workDir of the Container mounted to the emptyDir() PodVolume

  - name: serve-app
    image: nginx:latest
    ports:
    - containerPort: 80 #Port of the Container
    volumeMounts:
      - name: standalone-pod-volume
        mountPath: /usr/share/nginx/html
        #workDir of the Container mounted to the emptyDir() PodVolume, serving the data from the emptyDir() PodVolume

  volumes:
  - name: standalone-pod-volume #Name of the emptyDir() PodVolume
    emptyDir: {} #emptyDir() PodVolume
```

```
kubectl create po /path/to/pod.yaml
kubectl po -o wide
```

SO, THE POD IS RUNNING THO!

Here, Regarding Volume where we ran, `emptyDir`, 
Check , where it is running. 

AND DIAGNOSE THE POD:
```
ssh username@NodeVMIp
```

Where the POD gets to run in a particular Node. 
```
ls -l /var/lib/kubelets/pods
```
Where you can see all the PODs running on that node named with an `uid`. 

HOW DO WE KNOW WHAT `uid` USES WHICH POD? Go back to `controlplane` and Diagnose the **PodIDs**.

```
kubectl get po
```
Get the PodName, and Inspect it,
``` 
kubectl get po podName -o yaml
kubectl get po podName -o yaml | grep uid
```
After getting the UID from `controlplane`, get back to node and 
```
ls -l /var/lib/kubelet/pods/podUID/volumes/kubernetes.io~empty-dir/PodVolumeName/<data>
```
Here, all the respective data will be getting stored with the shared manner. 

```
cat <data>
curl PodIP
```
Which is the data what goes there, gets shared too. 
 >  ==**Here, one container is writing the data in that volume, and the other container is serving that data from that volume.**==

**Where is `EmptyDir)()` here?** 
-> back to Node, this shared volume works as same or moreover similar to the Volume mount, Bind mount, tmpfs.

Delete pod and verify,
In `controlplanenode`
```
kubectl delete po PodName
```
In `computeplanenode`
```
ls -l /var/lib/kubelet/pods/UID/
```
Will not exist since we removed the POD. `No such file or Directory`.

Here, if we recreate or whatever modification that we perform or to recreate the same POD here,  IT WILL NOT BE AS SAME AS IT WAS. 

And there will be bunch of mismatches in multiple levels. 
- Ip will not be the same
- PodUID 
- PodIP
- Pod placement in the clusters - Node selection
- **Especially, Data will be so lost, since it is not persistent.** 
- And much more.

**WHERE IS THE DATA GETS STORED - IS IT THE RAM or DISK OF THE HOST MACHINE!?**
Create the POD once again. 

```
kubectl create -f path/to/standalonepod.yaml
kubectl get po -o wide
```
POD gets created and running in a clean state. 

 Get the UID and back to the workernode where the POD is running,
 ```
 kubectl get po -o yaml | grep uid 
 ```
And to the workernode,
```
ls -l /var/lib/kubelet/pods/<uid>/volumes/kubernetes.io/shared-data/<*data>
```
Where the `<*data>` gets stored?
The POD is created on the host machine tho.  Where in the host machine the data lies at? Speaking of the storage medium of the host machine - all we have is RAM which is volatile and Storage Disk which is non-volatile.

Here, If we don't pass any parameters, the data will be stored in the disk by default. 

```
df -h
mount | grep -i emptyDir
mount | grep -i podUID
```
Which that the volume got mounted to ==**`tmpfs`**==,

The entire Filesystem is stored on the disk tho. Find out by passing 
```
df -h
```
Also on this node, the POD is running. Where the Kubernetes created a volume under that root filesystem which is mount to the host disk.  

To double check, go back to controlplane, and login to each container,
```
kubectl exec -it -u 0 PodName -c ContainerName1 -- sh
```

After getting into the shell,
```
df -h
```
Where you can find that the volume has been mounted to the host's disk. 

 This how the data is getting stored in the shared manner between containers. 
```
kubectl exec -it -u 0 PodName -c ContainerName1 -- sh
```
And check the same by passing 
```
dh -f
mount
```
Such folder got mounted to such disk partition. 

This way, we conclude that the storage medium here is the Disk not RAM on the host. 

Cleanup resources,
```
kubectl delete po podName
```

IF, WE WANT TO RUN THINGS RAM, nothing much but simply
```
volumes:
  - name: PodVolumeName
  emptyDir:
    medium: Memory
  ```

And to create the same,
```
kubectl create -f /path/to/standaonepod.yaml
```

Verify and check in which node the POD lies at, 
```
kubectl get po -o wide
```

`ssh` into that node and verify volume
```
ls -l /var/lib/kubelet/pods/<uid>/volumes/kubernetes.io~emptydir/podVolumeName/<data>
```

NOW, HOW TO VERIFY THAT THE MEDIUM OF STORAGE TO THIS POD - is RAM of the host?,

```
kubectl exec -it -u 0 PodName -c ContainerName1 -- sh
```
```
kubectl exec -it -u 0 PodName -c ContainerName2 -- sh
```

Here, check the mounts disks, 
```
df -h | grep 
```

Mounted to `tmpfs`, not to the disk.

**In memory storage medium, we can also set limits that how much of space to be used in it**,
```
volumes:
  - name: PodVolumeName
    emptyDir:
      medium: Memory
      sizeLimit: 300Mi #300mb
```

Apply and verify the same by, spinning up a POD and login to a container, 
```
df -h
```
In the mount of `tmpfs`, the size limit of it will be shown. 


WHERE AND WHEN TO USE `disk` and `memory ` under `emptyDir` ?
- RAM - Any critical or sensitive data to be stored for a sec to be volatile - on the container to the RAM.
- Memory - Store volume in the disk to working and provisioning data.

Cleanup,
```
kubectl delete po PodName
```

>  **emptyDir** is nothing but like in docker's anonymous volumes.  Stays in K 8 s not binding to anything. If it gets deleted, deleted. 

This is all about `emptyDir`.


---
