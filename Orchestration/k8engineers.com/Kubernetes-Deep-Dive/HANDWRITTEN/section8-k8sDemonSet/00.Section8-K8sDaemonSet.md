##### Section 8: K8S Daemon Set
1.  Introduction to Daemon Set(ds)
		k8s workload resource Introduction to DaemonSet (ds) - [Doc]
2.  DaemonSet workflow(Declarative approach)
		DaemonSet workflow (Declarative approach) - [Doc]
3.  DaemonSet rollout and rollback(strategy type: Rolling Update)
		DaemonSet rollout and rollback (strategy type Rolling Update) - [Doc]
4.  DaemonSet rollout and rollback(strategy type: On Delete)
		DaemonSet rollout and rollback (strategy type On Delete) - [Doc]
5.  DaemonSet controller revision resource
		DaemonSet controller revision resource - [Doc]
---

# 1.  Introduction to DaemonSet(ds)
k8s workload resource Introduction to DaemonSet (ds) - [Doc]

> K8s Workloads - ==**DaemonSet `ds`**==

Will see our next as part the of the list of **Kubernetes Workload Resources:** **`DaemonSet`**`ds`

Will cover in brief about,
- what is `DaemonSet`
- the purpose, usecases and applications
- and all the interrogatives

> The purpose of a DaemonSet `ds` is simple:
**==-If you want to run an app in the PODs on each and every available node in your K8s cluster, go for DaemonSet `ds`==**

##### K8s Workload Resources: **DaemonSet** `ds`
- `DaemonSet` object makes sures that one **POD will running on each node** (control plane and worker node).
- It doesn't support `replica` concept, for which `scaleIn` and `scaleOut` feature is not available
- **Workflow**: `DaemonSet` -> POD
- `apiVersion` for `DaemonSet` Object is `apps/v1`
- Since by default control plane node doesn't launch workloads, we need to define tolerations under template
- DaemonSet object supports two selector parameters (`matchLabels` and `matchExpressions`)
- In order to manage PODs controller by `ds`, `.spec.selector` should match with `.spec.template.metadata.labels`
- We can still control the `ds` to launch POD on specific node with help of `nodeSelector` and `nodeAffinity`
- **DaemonSet** supports `rollout` and `rollback` of PODs
- `DaemonSet` has two **update strategies**:
1) **`RollingUpdate`** (default) -> max unavailable, max surge
2) OnDelete
- **DaemonSet** revision history limit can be set under `.spec.revisionHistoryLimit` (default:10)
```
kubectl rollout history ds/daemonsetName
```
- For DaemonSet, each revision is stored in a resource names ControllerRevision
```
kubectl get controllerrevision -l ds-key=ds-value
```


##### `DaemonSet` object makes sures that one **POD will running on each node** (control plane and worker node).
If you want to run an app in the PODs on each and every available node in your K8s cluster, `ds` DaemonSet is the choice. Regardless of how many nodes, how many clusters and such. 

can't use the rest of the K8s workload resources that we gone through so far. all the `standalone pods,rc,rs,deploy` and such. Because, **we will not know how many NODES and PODs in all those nodes we will be having in our clusters.**  

If you want to run all your applications on each and every available node in your cluster. Control plane or number of worker nodes  - makes sure that atleast one POD will be running on each and every available node. For that, `ds` is the way. 


##### It doesn't support `replica` concept, for which `scaleIn` and `scaleOut` feature is not available
No `replication` concept here. Since we saw that it makes sure that it runs at least one POD on the nodes, Purpose of it doesn't fit here.  

The concept of `Replication` that we have seen in the following K8s workload resources `rc, rs, deploy`. NOTHING HERE IN THE `ds` DaemonSet. 

No features of replication will be available for us here,
- replica
- scaleIn and scaleOut
- HA practices 
and such

**TO PUT THINGS SIMPLY:**
You want some specific PODs (systemic service oriented PODs like logs, jobs and services) to be running on each and every available nodes at are in the present or in future. You will not know how many nodes and PODs that you are going to need in near future. you'll never know. 
I should be making sure that i will be running these PODs on all the available nodes.
> FOR THESE: DaemonSet `ds`

##### **Workflow**: `DaemonSet` -> POD
that simple! right the `ds` manifest -> creates PODs in all the available nodes. (no strings attached such as replication or other workload resources)

##### `apiVersion` for `DaemonSet` Object is `apps/v1`
as same as `deploy` and such. 
```
apiVersion: apps/v1
kind: DaemonSet
```

##### Since by default control plane node doesn't launch workloads, we need to define tolerations under template
As we saw that `ds` **DaemonSet** will be making sure that the PODs that must be running on all the nodes and such, That nodes includes the **Control Plane** too.

Since by the default, we will be not running PODs in the Control Plane node, he we should. To make `ds` its way to spin up the PODs in the **control plane** too, need to enable and define **`Taints`** and **`tolerations`** under the same.  

##### DaemonSet object supports two selector parameters (`matchLabels` and `matchExpressions`)
For the **selectors** and such for the identification of PODs, `matchLabels` and `matchExpressions` works just fine! 
same as we saw with the `rs` and `deploy`. All the **selectors** but not the *replication. 

 
##### In order to manage PODs controller by `ds`, `.spec.selector` should match with `.spec.template.metadata.labels`
Selectors in the sense, all that we saw previously with the PODs identification, labels, labels being identical with the PodTemplate and such. 

```
.spec.selector = .spec.template.metadata.labels
```
this way `ds` identify the POD that handles. 

Where `kube-controller` creates the PODs, we have to/will be needed to identify those PODs. How? with the help of the labels concept and these components of `matchLabels` and `matchExpressions` .

Labels are simply just the metadata. But that helpful  with the workload resources as well. 


##### We can still control the `ds` to launch POD on specific node with help of `nodeSelector` and `nodeAffinity`
 We have a concept named `nodeSelector` and `nodeAffinity`, 

We've been seeing that the `ds` just spins up the PODs on all the available nodes and stuff, even in the control plane. On an overview, it seems like that the `ds` that we have no control over it but to create and remove one. BUT
> We can control the `ds` from going out of hand with some exceptions. 
==We control the `ds` to launch POD on specific node with help of `nodeSelector` and `nodeAffinity`==


##### **DaemonSet** supports `rollout` and `rollback` of PODs




##### `DaemonSet` has two **update strategies**:
1) **`RollingUpdate`** (default) -> max unavailable, max surge
2) OnDelete



##### **DaemonSet** revision history limit can be set under `.spec.revisionHistoryLimit` (default:10)
```
kubectl rollout history ds/daemonsetName
```



##### For DaemonSet, each revision is stored in a resource names ControllerRevision
```
kubectl get controllerrevision -l ds-key=ds-value
```





---
