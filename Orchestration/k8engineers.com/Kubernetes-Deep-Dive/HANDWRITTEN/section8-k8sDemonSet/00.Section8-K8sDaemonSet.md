##### Section 8: K8S Daemon Set
1.  Introduction to Daemon Set(ds)
		k8s workload resource Introduction to DaemonSet (ds) - [Doc]
2.  DaemonSet workflow(Declarative approach)
		DaemonSet workflow (Declarative approach) - [Doc]
3.  DaemonSet rollout and rollback(strategy type: Rolling Update)
		DaemonSet rollout and rollback (strategy type Rolling Update) - [Doc]
4.  DaemonSet rollout and rollback(strategy type: On Delete)
		DaemonSet rollout and rollback (strategy type On Delete) - [Doc]
5.  DaemonSet controller revision resource
		DaemonSet controller revision resource - [Doc]
---

# 1.  Introduction to DaemonSet(ds)
k8s workload resource Introduction to DaemonSet (ds) - [Doc]

> K8s Workloads - ==**DaemonSet `ds`**==

Will see our next as part the of the list of **Kubernetes Workload Resources:** **`DaemonSet`**`ds`

Will cover in brief about,
- what is `DaemonSet`
- the purpose, usecases and applications
- and all the interrogatives

> The purpose of a DaemonSet `ds` is simple:
**==-If you want to run an app in the PODs on each and every available node in your K8s cluster, go for DaemonSet `ds`==**

##### K8s Workload Resources: **DaemonSet** `ds`
- `DaemonSet` object makes sures that one **POD will running on each node** (control plane and worker node).
- It doesn't support `replica` concept, for which `scaleIn` and `scaleOut` feature is not available
- **Workflow**: `DaemonSet` -> POD
- `apiVersion` for `DaemonSet` Object is `apps/v1`
- Since by default control plane node doesn't launch workloads, we need to define tolerations under template
- DaemonSet object supports two selector parameters (`matchLabels` and `matchExpressions`)
- In order to manage PODs controller by `ds`, `.spec.selector` should match with `.spec.template.metadata.labels`
- We can still control the `ds` to launch POD on specific node with help of `nodeSelector` and `nodeAffinity`
- **DaemonSet** supports `rollout` and `rollback` of PODs
- `DaemonSet` has two **update strategies**:
1) **`RollingUpdate`** (default) -> max unavailable, max surge
2) OnDelete
- **DaemonSet** revision history limit can be set under `.spec.revisionHistoryLimit` (default:10)
```
kubectl rollout history ds/daemonsetName
```
- For DaemonSet, each revision is stored in a resource names ControllerRevision
```
kubectl get controllerrevision -l ds-key=ds-value
```


##### `DaemonSet` object makes sures that one **POD will running on each node** (control plane and worker node).
If you want to run an app in the PODs on each and every available node in your K8s cluster, `ds` DaemonSet is the choice. Regardless of how many nodes, how many clusters and such. 

can't use the rest of the K8s workload resources that we gone through so far. all the `standalone pods,rc,rs,deploy` and such. Because, **we will not know how many NODES and PODs in all those nodes we will be having in our clusters.**  

If you want to run all your applications on each and every available node in your cluster. Control plane or number of worker nodes  - makes sure that atleast one POD will be running on each and every available node. For that, `ds` is the way. 


##### It doesn't support `replica` concept, for which `scaleIn` and `scaleOut` feature is not available
No `replication` concept here. Since we saw that it makes sure that it runs at least one POD on the nodes, Purpose of it doesn't fit here.  

The concept of `Replication` that we have seen in the following K8s workload resources `rc, rs, deploy`. NOTHING HERE IN THE `ds` DaemonSet. 

No features of replication will be available for us here,
- replica
- scaleIn and scaleOut
- HA practices 
and such

**TO PUT THINGS SIMPLY:**
You want some specific PODs (systemic service oriented PODs like logs, jobs and services) to be running on each and every available nodes at are in the present or in future. You will not know how many nodes and PODs that you are going to need in near future. you'll never know. 
I should be making sure that i will be running these PODs on all the available nodes.
> FOR THESE: DaemonSet `ds`

##### **Workflow**: `DaemonSet` -> POD
that simple! right the `ds` manifest -> creates PODs in all the available nodes. (no strings attached such as replication or other workload resources)

##### `apiVersion` for `DaemonSet` Object is `apps/v1`
as same as `deploy` and such. 
```
apiVersion: apps/v1
kind: DaemonSet
```

##### Since by default control plane node doesn't launch workloads, we need to define tolerations under template
As we saw that `ds` **DaemonSet** will be making sure that the PODs that must be running on all the nodes and such, That nodes includes the **Control Plane** too.

Since by the default, we will be not running PODs in the Control Plane node, he we should. To make `ds` its way to spin up the PODs in the **control plane** too, need to enable and define **`Taints`** and **`tolerations`** under the same.  

##### DaemonSet object supports two selector parameters (`matchLabels` and `matchExpressions`)
For the **selectors** and such for the identification of PODs, `matchLabels` and `matchExpressions` works just fine! 
same as we saw with the `rs` and `deploy`. All the **selectors** but not the *replication. 

 
##### In order to manage PODs controller by `ds`, `.spec.selector` should match with `.spec.template.metadata.labels`
Selectors in the sense, all that we saw previously with the PODs identification, labels, labels being identical with the PodTemplate and such. 

```
.spec.selector = .spec.template.metadata.labels
```
this way `ds` identify the POD that handles. 

Where `kube-controller` creates the PODs, we have to/will be needed to identify those PODs. How? with the help of the labels concept and these components of `matchLabels` and `matchExpressions` .

Labels are simply just the metadata. But that helpful  with the workload resources as well. 


##### We can still control the `ds` to launch POD on specific node with help of `nodeSelector` and `nodeAffinity`
 We have a concept named 
 - **`nodeSelector` and**
 - **`nodeAffinity`,** 

We've been seeing that the `ds` just spins up the PODs on all the available nodes and stuff, even in the control plane. On an overview, it seems like that the `ds` that we have no control over it but to create and remove one. BUT
> We can control the `ds` from going out of hand with some exceptions. 
==We control the scheduling of the POD created by `ds` to launch POD on specific node
**with help of `nodeSelector` and `nodeAffinity`**  decides in which node you can launch and control the PODs== 

##### **DaemonSet** supports `rollout` and `rollback` of PODs
Supports rollout and rollback -> maintaining `ds` PODs version with the same. In the same, we have two types of StrategyUpdate:
- RollingUpdate - rolls one and kills one for update one at a time
- OnDelete - !new - Only if the POD is deleted, the new on gets rolled out.

##### `DaemonSet` has two **update strategies**:
In the same, we have two types of StrategyUpdate:
- **`RollingUpdate`** (default) ->  rolls one and kills one for update one at a time (same maxes)
- OnDelete - !new - Only if the POD is deleted, the new on gets rolled out.

##### **DaemonSet** revision history limit can be set under `.spec.revisionHistoryLimit` (default:10)
Since it supports Rollout and rollback upgrading and downgrading the application, the revision history can be seen for `ds` DaemonSet too.
```
kubectl rollout history ds/daemonsetName
```

##### For DaemonSet, each revision is stored in a resource names ControllerRevision
Shows the revision numbers of `ds` as similar as for the `deploy` objects. Where all those revisions will be stored in the Resource Name - Controller Revision.
```
kubectl get controllerrevision -l ds-key=ds-value
```
To fetch that, passing this command will be handy where it is called the same from `controllerrevision` -list key and its value of the **DaemonSet**

Will see all the same in practice! 

---

# 2.  DaemonSet workflow(Declarative approach)
DaemonSet workflow (Declarative approach) 

Will create a manifest for this `ds` to get started:

Will pick a sample app instead of images to understand the context here, all the functionalities, options available in the `ds`  in brief.  

or we can simply drop in the same template of deploy - (removing the `replica` and changing the appropriate terms and such.)
> THE ONLY MAJOR DIFFERENCE THAT WE WILL BE HAVING HERE IN OUR MANIFEST IS THE ==`taints` and `tolerations`== - this itself it's a different topic that comes under **scheduling**

By default, the Control plane don't accept to deploy PODs in that node. To make that work, have to remove the same. Taints and Tolerations.  

1) To do that, we have removed it imperatively. by doint
```
kubectl get no -o wide
kubectl describe no nodeName(ctrlplane) -o wide
kubectl events no nodeName(ctrlplane) -o wide
```
```
kubectl edit no nodeName(ctrlplane)
```
remove taints and tolerations and done! good to go for `ds`.

if not done! Do it 

2) declaratively, we do
```yaml
spec:
      tolerations:
      # these tolerations are to have the daemonset runnable on control plane nodes
      # remove them if your control plane nodes should not run pods
      - key: node-role.kubernetes.io/control-plane
        operator: Exists
        effect: NoSchedule
      - key: node-role.kubernetes.io/master
        operator: Exists
        effect: NoSchedule
```
DEPENDS UPON THE NODE. 

If already `taints` released in the control plane node, no need to declare in the manifests. IT WORKS JUST FINE!


