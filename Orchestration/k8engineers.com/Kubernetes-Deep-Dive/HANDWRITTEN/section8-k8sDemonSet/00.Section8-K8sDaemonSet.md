##### Section 8: K8S Daemon Set
1.  Introduction to Daemon Set(ds)
		k8s workload resource Introduction to DaemonSet (ds) - [Doc]
2.  DaemonSet workflow(Declarative approach)
		DaemonSet workflow (Declarative approach) - [Doc]
3.  DaemonSet rollout and rollback(strategy type: Rolling Update)
		DaemonSet rollout and rollback (strategy type Rolling Update) - [Doc]
4.  DaemonSet rollout and rollback(strategy type: On Delete)
		DaemonSet rollout and rollback (strategy type On Delete) - [Doc]
5.  DaemonSet controller revision resource
		DaemonSet controller revision resource - [Doc]
---

# 1.  Introduction to DaemonSet(ds)
k8s workload resource Introduction to DaemonSet (ds) - [Doc]

> K8s Workloads - ==**DaemonSet `ds`**==

Will see our next as part the of the list of **Kubernetes Workload Resources:** **`DaemonSet`**`ds`

Will cover in brief about,
- what is `DaemonSet`
- the purpose, usecases and applications
- and all the interrogatives

> The purpose of a DaemonSet `ds` is simple:
**==-If you want to run an app in the PODs on each and every available node in your K8s cluster, go for DaemonSet `ds`==**

##### K8s Workload Resources: **DaemonSet** `ds`
- `DaemonSet` object makes sures that one **POD will running on each node** (control plane and worker node).
- It doesn't support `replica` concept, for which `scaleIn` and `scaleOut` feature is not available
- **Workflow**: `DaemonSet` -> POD
- `apiVersion` for `DaemonSet` Object is `apps/v1`
- Since by default control plane node doesn't launch workloads, we need to define tolerations under template
- DaemonSet object supports two selector parameters (`matchLabels` and `matchExpressions`)
- In order to manage PODs controller by `ds`, `.spec.selector` should match with `.spec.template.metadata.labels`
- We can still control the `ds` to launch POD on specific node with help of `nodeSelector` and `nodeAffinity`
- **DaemonSet** supports `rollout` and `rollback` of PODs
- `DaemonSet` has two **update strategies**:
1) **`RollingUpdate`** (default) -> max unavailable, max surge
2) OnDelete
- **DaemonSet** revision history limit can be set under `.spec.revisionHistoryLimit` (default:10)
```
kubectl rollout history ds/daemonsetName
```
- For DaemonSet, each revision is stored in a resource names ControllerRevision
```
kubectl get controllerrevision -l ds-key=ds-value
```


##### `DaemonSet` object makes sures that one **POD will running on each node** (control plane and worker node).
If you want to run an app in the PODs on each and every available node in your K8s cluster, `ds` DaemonSet is the choice. Regardless of how many nodes, how many clusters and such. 

can't use the rest of the K8s workload resources that we gone through so far. all the `standalone pods,rc,rs,deploy` and such. Because, **we will not know how many NODES and PODs in all those nodes we will be having in our clusters.**  

If you want to run all your applications on each and every available node in your cluster. Control plane or number of worker nodes  - makes sure that atleast one POD will be running on each and every available node. For that, `ds` is the way. 


##### It doesn't support `replica` concept, for which `scaleIn` and `scaleOut` feature is not available
No `replication` concept here. Since we saw that it makes sure that it runs at least one POD on the nodes, Purpose of it doesn't fit here.  

The concept of `Replication` that we have seen in the following K8s workload resources `rc, rs, deploy`. NOTHING HERE IN THE `ds` DaemonSet. 

No features of replication will be available for us here,
- replica
- scaleIn and scaleOut
- HA practices 
and such

**TO PUT THINGS SIMPLY:**
You want some specific PODs (systemic service oriented PODs like logs, jobs and services) to be running on each and every available nodes at are in the present or in future. You will not know how many nodes and PODs that you are going to need in near future. you'll never know. 
I should be making sure that i will be running these PODs on all the available nodes.
> FOR THESE: DaemonSet `ds`

##### **Workflow**: `DaemonSet` -> POD
that simple! right the `ds` manifest -> creates PODs in all the available nodes. (no strings attached such as replication or other workload resources)

##### `apiVersion` for `DaemonSet` Object is `apps/v1`
as same as `deploy` and such. 
```
apiVersion: apps/v1
kind: DaemonSet
```

##### Since by default control plane node doesn't launch workloads, we need to define tolerations under template
As we saw that `ds` **DaemonSet** will be making sure that the PODs that must be running on all the nodes and such, That nodes includes the **Control Plane** too.

Since by the default, we will be not running PODs in the Control Plane node, he we should. To make `ds` its way to spin up the PODs in the **control plane** too, need to enable and define **`Taints`** and **`tolerations`** under the same.  

##### DaemonSet object supports two selector parameters (`matchLabels` and `matchExpressions`)
For the **selectors** and such for the identification of PODs, `matchLabels` and `matchExpressions` works just fine! 
same as we saw with the `rs` and `deploy`. All the **selectors** but not the *replication. 

 
##### In order to manage PODs controller by `ds`, `.spec.selector` should match with `.spec.template.metadata.labels`
Selectors in the sense, all that we saw previously with the PODs identification, labels, labels being identical with the PodTemplate and such. 

```
.spec.selector = .spec.template.metadata.labels
```
this way `ds` identify the POD that handles. 

Where `kube-controller` creates the PODs, we have to/will be needed to identify those PODs. How? with the help of the labels concept and these components of `matchLabels` and `matchExpressions` .

Labels are simply just the metadata. But that helpful  with the workload resources as well. 


##### We can still control the `ds` to launch POD on specific node with help of `nodeSelector` and `nodeAffinity`
 We have a concept named 
 - **`nodeSelector` and**
 - **`nodeAffinity`,** 

We've been seeing that the `ds` just spins up the PODs on all the available nodes and stuff, even in the control plane. On an overview, it seems like that the `ds` that we have no control over it but to create and remove one. BUT
> We can control the `ds` from going out of hand with some exceptions. 
==We control the scheduling of the POD created by `ds` to launch POD on specific node
**with help of `nodeSelector` and `nodeAffinity`**  decides in which node you can launch and control the PODs== 

##### **DaemonSet** supports `rollout` and `rollback` of PODs
Supports rollout and rollback -> maintaining `ds` PODs version with the same. In the same, we have two types of StrategyUpdate:
- RollingUpdate - rolls one and kills one for update one at a time
- OnDelete - !new - Only if the POD is deleted, the new on gets rolled out.

##### `DaemonSet` has two **update strategies**:
In the same, we have two types of StrategyUpdate:
- **`RollingUpdate`** (default) ->  rolls one and kills one for update one at a time (same maxes)
- OnDelete - !new - Only if the POD is deleted, the new on gets rolled out.

##### **DaemonSet** revision history limit can be set under `.spec.revisionHistoryLimit` (default:10)
Since it supports Rollout and rollback upgrading and downgrading the application, the revision history can be seen for `ds` DaemonSet too.
```
kubectl rollout history ds/daemonsetName
```

##### For DaemonSet, each revision is stored in a resource names ControllerRevision
Shows the revision numbers of `ds` as similar as for the `deploy` objects. Where all those revisions will be stored in the Resource Name - Controller Revision.
```
kubectl get controllerrevision -l ds-key=ds-value
```
To fetch that, passing this command will be handy where it is called the same from `controllerrevision` -list key and its value of the **DaemonSet**

Will see all the same in practice! 

---
# 2.  DaemonSet workflow(Declarative approach)
DaemonSet workflow (Declarative approach) 

Will create a manifest for this `ds` to get started:

Will pick a sample app instead of images to understand the context here, all the functionalities, options available in the `ds`  in brief.  

or we can simply drop in the same template of deploy - (removing the `replica` and changing the appropriate terms and such.)
> THE ONLY MAJOR DIFFERENCE THAT WE WILL BE HAVING HERE IN OUR MANIFEST IS THE ==`taints` and `tolerations`== - this itself it's a different topic that comes under **scheduling**

By default, the Control plane don't accept to deploy PODs in that node. To make that work, have to remove the same. Taints and Tolerations.  

1) To do that, we have removed it imperatively. by doint
```
kubectl get no -o wide
kubectl describe no nodeName(ctrlplane) -o wide
kubectl events no nodeName(ctrlplane) -o wide
```
```
kubectl edit no nodeName(ctrlplane)
```
remove taints and tolerations and done! good to go for `ds`.
```yaml
  taints:
  - effect: NoSchedule
    key: node-role.kubernetes.io/control-plane
  - effect: NoSchedule
    key: node.kubernetes.io/not-ready
```
if not done! Do it 

1) declaratively, we do in the manifest of the same:
```yaml
spec:
      tolerations:
      # these tolerations are to have the daemonset runnable on control plane nodes
      # remove them if your control plane nodes should not run pods
      - key: node-role.kubernetes.io/control-plane
        operator: Exists
        effect: NoSchedule
      - key: node-role.kubernetes.io/master
        operator: Exists
        effect: NoSchedule
```
DEPENDS UPON THE NODE. 

If already `taints` released in the control plane node, no need to declare in the manifests. IT WORKS JUST FINE!

`ds.yaml`
```yaml
apiVersion: apps/v1
kind: DaemonSet
metadata:
  name: nginx-ds
  labels:
    app: nginx-ds
    env: prod
    release: v1.0

spec:
  selector:
    matchLabels:
      app: nginx-ds
      env: prod
      release: v1.0

  template:
    metadata:
      labels:
        app: nginx-ds
        env: prod
        release: v1.0

    spec:
      containers:
      - name: write-app
        image: alpine
        command: ["/bin/sh"]
        args: ["-c", "while true; do date >> /var/log/index.html; sleep 10; done"]
        resources:
          limits:
            cpu: 100m
            memory: 100Mi
        volumeMounts:
        - name: deploy-shared-volume
          mountPath: /var/log

      - name: serve-app
        image: nginx:1.27.4
        ports:
        - containerPort: 80
        resources:
          limits:
            cpu: 100m
            memory: 100Mi
        volumeMounts:
          - name: deploy-shared-volume
            mountPath: /usr/share/nginx/html
      volumes:  
      - name: deploy-shared-volume
        emptyDir: {}
```
++ removed taints of Control Plane,
> all the nodes will be having a POD within. 

All the essential work has been done to deploy a `ds`.

So one POD for each and every available node. 

```
kubectl create -f deploy,rs,po -o wide
kubectl get deploy,po -o wide
```
watch and analyze yourself without the concept of `replication`, who many PODs have been deployed on each available node. 

BEHAVIOUR: **==One POD per node==**
> Workflow: DaemonSet -> POD(`ds` itself creates the PODs).

> Use cases:  Running services for scripts, monitoring, logging, observability, more grunt work and such.
I NEED ALL THESE SERVICES ON ALL ON MY NODES FOR THE SAME FEATURES AND REASONS. FOR SUCH USECASES, `ds` is the way.

What if i add another node for instance, does it add the POD on that node on the fly - **==YES, by default==**! *Unless and until, if there are restrictions applied to it (with the `nodeSelector` and `nodeAffinity` and such)*

++ if any of the POD gets deleted by any chance, gets **recreated** automatically. try deleting one and watch it b yourself. 
```
kubectl get po -o wide
kubectl delete po podName
```


NOW WILL COVER MORE OF `rollout` and `rollback` as same for the `ds` object. 

----
# 3.  DaemonSet rollout and rollback(strategy type: RollingUpdate)

DaemonSet rollout and rollback(strategy type: Rolling Update)

Will cover all the **RollingStrategies** available for the `ds` object.

If we inspect the `ds`,
```sh
kubectl get ds dsName -o yaml
kubectl get ds dsName -o yaml | grep update
```

It shows all the details in the `yaml` format, there will be *UpdateStrategy* `StrategyType`=`RollingUpdate` which gets applied by default despite mentioning and declaring the same in the manifest. 

###### Rollout
 For the versions of `ds`, we do have the `revisions` for the same as same as the other `rollback`, `rollout` objects too. To check the list of revisions:
```
kubectl rollout history ds/dsName
```
TO review a specific revision, do this to print the PodTemplate,
```
kubectl rollout history ds dsName --revision=n 
```

Will perform an upgrade and have a revision for it,
```
kubectl get ds,po -o wide
kubectl set image ds dsName containerName=image:versionTag
```
and check and verify the same by
```
kubectl rollout status ds dsName
```
```
curl -I IP
```

> THE CHARACTERISTICS OF THIS UPDATE, here is a bit different. It creates the NEW one no matter what, after completion of the NEW POD and deletes the old . **==One by one, one node at a time==** 
-Regardless of balancing **max unavailable** and **surge**. 

acts differently for `deploy` and for `ds`.

ALL THE PODS ARE UPDATED IN ALL THE DIFFERENT NODES. 

###### Rollback
To get back/restore a revision means, 
To check the list of revisions:
```
kubectl rollout history ds/dsName
```
TO review a specific revision, do this to print the PodTemplate,
```
kubectl rollout history ds dsName --revision=n 
```
and check and verify the same by
```
kubectl rollout status ds dsName
```
```
curl -I IP
```

**To rollback one version back:**
```
kubectl rollout undo ds/dsName
```
**To rollout to a specific version:**
```
kubectl rollout undo ds dsName --to-revision=n
```



---
# 4.  DaemonSet rollout and rollback(strategy type: On Delete)

will cover the same Rollout, Rollback **`StrategyUpdate=OnDelete`**. Quite different from what we have seen earlier. (even than recreate)

OnDelete applies the changes first but **updates/downgrades only if we delete the PODs by ourselves**. 

To edit the `StrategyType`=`Ondelete`
Declaratively, simply apply this into the manifest:
```yaml
spec:
  strategyType: onDelete
```

Imperatively,
```sh
kubectl edit ds dsName 
```
change `StrategyType: OnDelete`

and make the changes:
```
kubectl set image ds dsName containerName=versionTag
```

ALL DONE. If you check the pods,
```
kubectl get po -o wide
```
Nothing has been changed. as per the `StrategyType=OnDelete`, deletes if we delete these PODs by ourselves. 

Will delete the POD and see the changes:
```
kubectl get po -o wide
kubectl delete po podName1 podNameN
```
when POD gets deleted, **new gets recreated BUT WITH THE UPDATED VERSION**.
```
kubectl get po -o wide
curl -I IP
```

++We can limit the **revision history** limits. (imperatively and declaratively(under the spec of ds))

---
# 5.  DaemonSet controller revision resource
Will try to understand how all the **revisions** of `ds` gets  stored in all the available nodes in the K8s Cluster. 

To see the revisions,
```
kubectl rollout history ds/dsName
```

> Generally, these revisions of the `ds` used to get stored in an object called ==**Controller Revision**==. 
A separate resource object who will stores the `ds` DaemonSet revision information. 

Before diving deep into that,
Will do pull and identify relative PODs using **Labels**. 

To print all the available PODs with the labels:
```
kubectl get ds,po -o wide --show-labels 
```
if you do that,


| NAME               | READY | STATUS  | RESTARTS | AGE | IP            | NODE          | NOMINATED NODE | READINESS GATES | LABELS                                                                                          |
| ------------------ | ----- | ------- | -------- | --- | ------------- | ------------- | -------------- | --------------- | ----------------------------------------------------------------------------------------------- |
| pod/nginx-ds-7cw54 | 2/2   | Running | 0        | 43m | 10.244.11.152 | workernode124 | <none>         | <none>          | app=nginx-ds,controller-revision-hash=c98b57755,env=prod,pod-template-generation=8,release=v1.0 |
| pod/nginx-ds-mjscm | 2/2   | Running | 0        | 43m | 10.244.55.173 | workernode124 | <none>         | <none>          | app=nginx-ds,controller-revision-hash=c98b57755,env=prod,pod-template-generation=8,release=v1.0 |
| pod/nginx-ds-z2wq7 | 2/2   | Running | 0        | 43m | 10.244.11.152 | workernode124 | <none>         | <none>          | app=nginx-ds,controller-revision-hash=c98b57755,env=prod,pod-template-generation=8,release=v1.0 |

In the labels section, shows a parameter named,
```
LABELS = pod-template-generation=n
```
This `pod-template-generation=n` indicated the number of revision it is, or it is currently using is the case.

If we perform an **upgrade or downgrade**, the revision will be added and rolled out. verify the same by performing one and check the same.
```
kubectl rollout history ds/dsName
kubectl get po -o wide --show-labels
curl -I IP
```

with this, the
```
LABELS = pod-template-generation=n+1
```

BACK TO THE POINT,
HOW THE INFORMATION IS GATHERED FROM A RESOURCE CALLED ==**Controller Revision**== FOR `ds`. 
will see that now, 
```
kubectl get po -o wide --show-labels
```

```
kubectl rollout history ds/dsName
kubectl get controllerrevision
```

Filter specific PODs using **Labels** calling the same by the particular **revision** number:
```
kubectl rollout history ds/dsName
kubectl get controllerrevision -l labelName
```
Here, there is a column named **REVISION** having the name **revision** numbers + with it's very on **hashcode**
> So this object - `controllerrevision` who controls the revision of the same. 

FROM THIS, WE GET THAT ALL THE REVISION NUMBER OF `ds` GETTING THE OBJECT'S **ROLLOUT** AND **ROLLBACK**  hAVING THE SEPERATE RESOURCE CALLED **==`controllerrevision`==**.

WHICH CAN BE SEEN ON THE PODs or using
```
kubectl rollout history ds/dsName
kubectl get controllerrevision
```

WHAT IF WE DELETES ONE?
if we delete one of these particular `controllerrevision`, will see that by practice:
```
kubectl get controllerrevision
kubectl get controllerrevision -l labelName
```
```
kubectl delete controllerrevision revisionName
 ```
gets deleted. WHAT HAPPENS TO THE POD?

As per StrategyType=OnDelete, having a version that is up and running but we delete the `controllerrevision` of the same. 
```
kubectl get ds,po -o wide --show-labels
```
and THAT PARTICULAR POD will show the hash of the POD in its label in the param `controller-revision-hash=theHashThatWeDeleted`.

NOTHING HAPPENS TO THE RUNNING POD, containing the same label from the `controllerrevision`. THE CATCH IS THAT, THERE WILL BE NO `controllerrevision` FOR THE POD. BUT WORKS!

NOTHING HAPPENS TO THE POD. 

WHAT ABOUT THE `rollout` HISTORY?
```
kubectl rollout history ds dsName
```
that revision gets deleted. since all these `revision` numbers gets controlled by the `controllerrevision` itself.

POD still exists and works just fine!

---
