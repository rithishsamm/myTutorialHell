
---
Jobs workflow (restart Policy) - [Doc]

Practically Implementing a `Job`object 

>and not `nginx` since we don't want a `Job` to be running consistently. Just a `shell` execution is enough to get the **Job** done.

Will take a simple `image` to run just the scripts alone. 

check the api-resources version using:
```sh
kubectl api-resources
kubectl api-resources | grep job
```
> apiVersion: batch/v1, kind: Job

`job.yaml`
```yaml
apiVersion: batch/v1
kind: Job
metadata:
  name: job1
spec:
  template:
    spec:
      containers:
      - name: job1-container
        image: alpine:latest
        command: ["/bin/sh"]
        args: ["-c", "echo Hello from the job1"]
      restartPolicy: OnFailure
```

**so, what this `job` object does is that,** base syntax of this `job`:
- base boilerplate: `apiVersion: batch/v1, kind: Job` and `metadata`. usual stuff
- `spec`: -> `PodTemplate` ->
- `spec` (of containers) -> name of the job container, just a simple `image` to run the shell, cmd to access the shell of that `image`, passing `args` with the accessed `shell`: Just printing one simple message
- no `volumes` and `mounts` has been set here, because it is irrelevant and we are saving nothing here to have a volume for this **job**
- mandatory for the **Job** is to have `restartPolicy` and it should never have `Always` but can have `never` or `OnFailure`.
###### 1) `restartPolicy: Never`
will apply and verify the manifest:
```sh
kubectl create -f /path/to/job.yaml
```

now, will dig deep in what's up here,
```sh
kubectl get jobs -o wide
```
output:
```
NAME            job1 #nameOfTheJob
STATUS          Complete #status
COMPLETIONS     1/1 #jobsCompleted
DURATION        5s  #execTime
AGE             5m29s #age
CONTAINERS      job1-container #container
IMAGES          alpine:latest #image
SELECTOR        batch.kubernetes.io/controller-uid=5d37e837-0d36-4ace-a122-d0a454e9f8c3 #selectorUniqueId
```

now, will dig deep in what's up here,
```sh
kubectl get jobs,po -o wide
```
output:
```
NAME           job1-8ppdt #autoPodName
READY          0/1#Job,notAnApp(oneTime)
STATUS         Completed #doneAndDusted
RESTARTS       0 #noRestarts(successful)
AGE            10m #age
IP             10.244.55.175 #podIP
NODE           workernod e125 #node
NOMINATED NODE <none>
READINESS GATES<none>
```

to inspect further of a `job`and check the events.
```sh
kubectl describe po podName
```

Where is the `execution`? (the printed message): check logs.
```sh
kubectl logs po podName
```

if we, declared `restartPolicy=never` and the exec get failed instead of succeed, 
```
error
```
and keeps on recreating the contained inside the POD and executing the jobs and keeps failing to create one. 

Like this:
![[Pasted image 20250215180857.png]]


**tinkering with cleanup:**
```sh
kubectl delete po podName
```
no pods will get recreated, obviously. It is just a job. IT IS OBSELETE.
```sh
kubectl logs po podName
```
none gets printed since we deleted the POD and no exec can be seen when the POD is absent. But the `job` still shows. Since its done and dusted. 

if, we directly delete a `job`, POD gets deleted within
```sh
kubectl delete job jobName
kubectl get jobs,po -o wide
```

###### 2) `restartPolicy: OnFailure`
for this `restartPolicy: OnFailure`, will perform something different to under this `OnFailure `

`job-onFailure.yaml`
```yaml
apiVersion: batch/v1
kind: Job
metadata:
  name: job1
spec:
  template:
    spec:
      containers:
      - name: job1-container
        image: alpine:latest
        command: ["/bin/sh"]
        args: ["-c", "ls -l samm"] #which doesn't exist and fails  
      restartPolicy: OnFailure
```
If the exec, succeeds, it just runs.
This one will gets into error status, since the cmd `exec` fails. 

apply the same and see what happens,
```sh
kubectl create -f /path/to/job-onFailure.yaml
```

verify in an sec and see
```sh
kubectl get jobs,po -o wide
kubectl get jobs,po -o wide
```
IT RESTARTS ONCE (not recreated) and Throws `CrashLoopBackOff` error with multiple restarts  and quits. 

The reason for recreated of the PODs is for the iteration until it succeeds.

JOB was created the `Pod` got failed. 

Inspect it,
```sh
kubectl describe po podName
```

EVERYTHING GOES SAME FOR `cronjobs` too.

delete,
```sh
kubectl delete job jobName
```

**WHAT IF, `restartPolicy: Always`**?
IT WILL NOT CREATE A `JOB` OBJECT AT ALL. Schema validation doesn't get pass through. telling
```
The Job "job2" is invalid: spec.template.spec.restartPolicy: Required value: valid values: "OnFailure", "Never"
```

**WHAT IF, we fail to declare `restartPolicy`**?
SAME. **restartPolicy** is a must. Schema validation doesn't get pass through. telling
```
The Job "job2" is invalid: spec.template.spec.restartPolicy: Required value: valid values: "OnFailure", "Never"
```

:)

---
