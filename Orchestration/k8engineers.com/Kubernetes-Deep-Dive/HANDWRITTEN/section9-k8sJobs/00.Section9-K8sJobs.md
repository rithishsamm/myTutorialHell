##### Section 9: K8s Jobs
1. Introduction to Jobs
		K8s workload resource Introduction to Jobs - [Doc]
2. Jobs workflow(restart Policy)
		Jobs workflow (restart Policy) - [Doc]
3. Jobs termination and cleanup
		Jobs termination and cleanup - [Doc]

---
# 1. Introduction to Jobs
K8s workload resource Introduction to Jobs - [Doc]

Running batch operations, scripts, crontab, jobs and such. How to in all these K8s clusters. varies much from what we have been implementing,

In such operational actions, will get started with Jobs:
- What is Jobs?
- Use cases, purpose and applications, 
- all the functionalities, conditions and such. 
> K8s Workloads - ==**Jobs `jobs`**==
##### K8s Workload Resources: **Jobs** `jobs`
- `Jobs` Jobs are workload resource which will ensure one or more pods created and terminate successfully.
- In general, we use `Jobs` to run some batch operations like Backups, Restore etc.,
- **Workflow**: `Jobs` -> POD
- `apiVersion` for `Jobs` Object is `batch/v1`
- `Jobs` can also run multiple pods in parallel
- Once task specified in POD container was successful, **Job** goes to completed status
- **Job** `selector` is optional
- PODs controlled by **Job** object `restartPolicy` should be set to only `Never` or `OnFailure`, not `Always`
- Types of parallel executions Jobs:
1) Non-parallel **jobs**
2) Parallel **jobs** with a fixed completion count
3) Parallel **jobs** with a work queue
- Best Practice is to enable `ttlSecondsAfterFinished: 100` for **Job** to clean up automatically after the TTL value (`.spec.ttlSecondsAfterFinished`) which system will not put pressure on the `kube-apiserver` [stable-v1.23]
- We can suspend the Job as well and start again by `.spec.suspend` value to **true** or **false**
> 1) Note: If a **Job** is suspended, any running POD that don't have status completed will be terminated
> 2) Command:
```
kubectl patch job/jobName --type=strategic --patch '{"spec":{"suspend":true}}'
```

##### `Jobs` Jobs are workload resource which will ensure one or more pods created and terminate successfully.
> Ensures one or more PODs either gets created or terminate successfully up and running.

How this Job performing this action? - is via following instructions to perform a specific task. which ensures one or more PODs either gets created or terminate successfully up and running.

WHAT TYPES OR INSTRUCTIONS AND TASKS?
##### In general, we use `Jobs` to run some batch operations like Backups, Restore etc.,
tasks such as taking backups, restores states and all the operational grunt work for our PODs, clusters, or for out applications. 
> Instead of using `initContainers` which have its own limitations, will be using `Jobs` for extensive use cases. 

MOST WE USE ALL THESE FOR **LOGS, BACKUP AND RESTORE**. once a job gets done, POD gets hibernated printing the status as `completed` if job is successful or else, `failed` or `terminated` and we should be troubleshoot them. 
you can verify that using
```
kubectl get po -o wide
```

##### **Workflow**: `Jobs` -> POD
Jobs creates PODs. running that specific task.

##### `apiVersion` for `Jobs` Object is `batch/v1`
for jobs, cronjobs, `apiVersion` for `Jobs` Object is `batch/v1`
```
apiVersion: batch/v1
kind: Jobs
```

##### `Jobs` can also run multiple pods in parallel
as the title says, `Jobs` can also run multiple pods in parallel to run, gets done and dusted. 

##### Once task specified in POD container was successful, **Job** goes to completed status
as same as we discussed earlier is that,  once a job gets done, POD gets hibernated printing the status as `completed` if job is successful or else, `failed` or `terminated` and we should be troubleshoot them. 

##### **Job** `selector` is optional
while writing the manifest for running a `Job` object, we can run `selector`, `labeling` but them are all totally optional. nothing happens and works just fine if we fail to mention too. 

##### PODs controlled by **Job** object `restartPolicy` should be set to only `Never` or `OnFailure`, not `Always`
```
restartPolicy: never/onFailure
```
but not `always`, since `jobs` object are supposed not to be actively by nature. if aint, that is not a `Job` running an instructions, runs if runs, fails if fails. 

##### Types of parallel executions Jobs:
1) Non-parallel **jobs**
2) Parallel **jobs** with a fixed completion count
3) Parallel **jobs** with a work queue
what are the types parallel executions we can do under the `Jobs` object,
1) Non-parallel **jobs** - *default*
2) Parallel **jobs** with a fixed completion count - need PARAMS
3) Parallel **jobs** with a work queue - need PARAMS
will see all these in practice. 

##### Best Practice is to enable `ttlSecondsAfterFinished: 100` for **Job** to clean up automatically after the TTL value (`.spec.ttlSecondsAfterFinished`) which system will not put pressure on the `kube-apiserver` [stable-v1.23]
ALWAYS A BEST PRACTICE TO PERFORM CLEANUPS of the `Jobs`. adds load in the `kube-apiServer` if we fail do cleanup. 
to do these - **enable `ttlSecondsAfterFinished: 100` for Job to clean up automatically after the TTL value (`.spec.ttlSecondsAfterFinished`) which system will not put pressure on the `kube-apiserver`**

default of  `ttlSecondsAfterFinished`: is `100` 

##### We can suspend the Job as well and start again by `.spec.suspend` value to **true** or **false**
We can suspend the Job as well and start again by `.spec.suspend` value to **true** or **false**.

If a **Job** is suspended, any running POD that don't have status completed will be terminated.
eg: `Job` -> `Pod`, is under creation process. meanwhile, you suspend your Job and restart if it fails to create, run or completed status.  to perform that, imperative approach running the command: 
```
kubectl batch job/jobName --type=strategic --patch '{"spec":{"suspend":true/false}}'
```

WILL SEE ALL THESE IN PRACTICE.

---
# 2. Jobs workflow(restart Policy)
Jobs workflow (restart Policy) - [Doc]

Practically Implementing a `Job`object 

>and not `nginx` since we don't want a `Job` to be running consistently. Just a `shell` execution is enough to get the **Job** done.

Will take a simple `image` to run just the scripts alone. 

check the api-resources version using:
```
kubectl api-resources
kubectl api-resources | grep job
```
> apiVersion: batch/v1, kind: Job

`job.yaml`
```
apiVersion: batch/v1
kind: Job
metadata:
  name: job1
spec:
  template:
    spec:
      containers:
      - name: job1-container
        image: alpine:latest
        command: ["/bin/sh"]
        args: ["-c", "echo Hello from the job1"]
      restartPolicy: OnFailure
```

**so, what this `job` object does is that,** base syntax of this `job`:
- base boilerplate: `apiVersion: batch/v1, kind: Job` and `metadata`. usual stuff
- `spec`: -> `PodTemplate` ->
- `spec` (of containers) -> name of the job container, just a simple `image` to run the shell, cmd to access the shell of that `image`, passing `args` with the accessed `shell`: Just printing one simple message
- no `volumes` and `mounts` has been set here, because it is irrelevant and we are saving nothing here to have a volume for this **job**
- mandatory for the **Job** is to have `restartPolicy` and it should never have `Always` but can have `never` or `OnFailure`.
###### 1) `restartPolicy: Never`
will apply and verify the manifest:
```
kubectl create -f /path/to/job.yaml
```

now, will dig deep in what's up here,
```
kubectl get jobs -o wide
```
output:
```
NAME            job1 #nameOfTheJob
STATUS          Complete #status
COMPLETIONS     1/1 #jobsCompleted
DURATION        5s  #execTime
AGE             5m29s #age
CONTAINERS      job1-container #container
IMAGES          alpine:latest #image
SELECTOR        batch.kubernetes.io/controller-uid=5d37e837-0d36-4ace-a122-d0a454e9f8c3 #selectorUniqueId
```

now, will dig deep in what's up here,
```
kubectl get jobs,po -o wide
```
output:
```
NAME           job1-8ppdt #autoPodName
READY          0/1#Job,notAnApp(oneTime)
STATUS         Completed #doneAndDusted
RESTARTS       0 #noRestarts(successful)
AGE            10m #age
IP             10.244.55.175 #podIP
NODE           workernod e125 #node
NOMINATED NODE <none>
READINESS GATES<none>
```

to inspect further of a `job`and check the events.
```
kubectl describe po podName
```

Where is the `execution`? (the printed message): check logs.
```
kubectl logs po podName
```

if we, declared `restartPolicy=never` and the exec get failed instead of succeed, 
```
error
```
and keeps on recreating the contained inside the POD and executing the jobs and keeps failing to create one. 

Like this:
![[Pasted image 20250215180857.png]]


**tinkering with cleanup:**
```
kubectl delete po podName
```
no pods will get recreated, obviously. It is just a job. IT IS OBSELETE.
```
kubectl logs po podName
```
none gets printed since we deleted the POD and no exec can be seen when the POD is absent. But the `job` still shows. Since its done and dusted. 

if, we directly delete a `job`, POD gets deleted within
```
kubectl delete job jobName
kubectl get jobs,po -o wide
```

###### 2) `restartPolicy: OnFailure`
for this `restartPolicy: OnFailure`, will perform something different to under this `OnFailure `

`job-onFailure.yaml`
```yaml
apiVersion: batch/v1
kind: Job
metadata:
  name: job1
spec:
  template:
    spec:
      containers:
      - name: job1-container
        image: alpine:latest
        command: ["/bin/sh"]
        args: ["-c", "ls -l samm"] #which doesn't exist and fails  
      restartPolicy: OnFailure
```
If the exec, succeeds, it just runs.
This one will gets into error status, since the cmd `exec` fails. 

apply the same and see what happens,
```
kubectl create -f /path/to/job-onFailure.yaml
```

verify in an sec and see
```
kubectl get jobs,po -o wide
kubectl get jobs,po -o wide
```
IT RESTARTS ONCE (not recreated) and Throws `CrashLoopBackOff` error with multiple restarts  and quits. 

The reason for recreated of the PODs is for the iteration until it succeeds.

JOB was created the `Pod` got failed. 

Inspect it,
```
kubectl describe po podName
```

EVERYTHING GOES SAME FOR `cronjobs` too.

delete,
```
kubectl delete job jobName
```

**WHAT IF, `restartPolicy: Always`**?
IT WILL NOT CREATE A `JOB` OBJECT AT ALL. Schema validation doesn't get pass through. telling
```
The Job "job2" is invalid: spec.template.spec.restartPolicy: Required value: valid values: "OnFailure", "Never"
```

**WHAT IF, we fail to declare `restartPolicy`**?
SAME. **restartPolicy** is a must. Schema validation doesn't get pass through. telling
```
The Job "job2" is invalid: spec.template.spec.restartPolicy: Required value: valid values: "OnFailure", "Never"
```

:)

---

# 3. Jobs termination and cleanup
Jobs termination and cleanup - [Doc]







---
