##### Section 9: K8s Jobs
1. Introduction to Jobs
		K8s workload resource Introduction to Jobs - [Doc]
2. Jobs workflow(restart Policy)
		Jobs workflow (restart Policy) - [Doc]
3. Jobs termination and cleanup
		Jobs termination and cleanup - [Doc]

---
# 1. Introduction to Jobs
K8s workload resource Introduction to Jobs - [Doc]

Running batch operations, scripts, crontab, jobs and such. How to in all these K8s clusters. varies much from what we have been implementing,

In such operational actions, will get started with Jobs:
- What is Jobs?
- Use cases, purpose and applications, 
- all the functionalities, conditions and such. 
> K8s Workloads - ==**Jobs `jobs`**==
##### K8s Workload Resources: **Jobs** `jobs`
- `Jobs` Jobs are workload resource which will ensure one or more pods created and terminate successfully.
- In general, we use `Jobs` to run some batch operations like Backups, Restore etc.,
- **Workflow**: `Jobs` -> POD
- `apiVersion` for `Jobs` Object is `batch/v1`
- `Jobs` can also run multiple pods in parallel
- Once task specified in POD container was successful, **Job** goes to completed status
- **Job** `selector` is optional
- PODs controlled by **Job** object `restartPolicy` should be set to only `Never` or `OnFailure`, not `Always`
- Types of parallel executions Jobs:
1) Non-parallel **jobs**
2) Parallel **jobs** with a fixed completion count
3) Parallel **jobs** with a work queue
- Best Practice is to enable `ttlSecondsAfterFinished: 100` for **Job** to clean up automatically after the TTL value (`.spec.ttlSecondsAfterFinished`) which system will not put pressure on the `kube-apiserver` [stable-v1.23]
- We can suspend the Job as well and start again by `.spec.suspend` value to **true** or **false**
> 1) Note: If a **Job** is suspended, any running POD that don't have status completed will be terminated
> 2) Command:
```sh
kubectl patch job/jobName --type=strategic --patch '{"spec":{"suspend":true}}'
```

##### `Jobs` Jobs are workload resource which will ensure one or more pods created and terminate successfully.
> Ensures one or more PODs either gets created or terminate successfully up and running.

How this Job performing this action? - is via following instructions to perform a specific task. which ensures one or more PODs either gets created or terminate successfully up and running.

WHAT TYPES OR INSTRUCTIONS AND TASKS?
##### In general, we use `Jobs` to run some batch operations like Backups, Restore etc.,
tasks such as taking backups, restores states and all the operational grunt work for our PODs, clusters, or for out applications. 
> Instead of using `initContainers` which have its own limitations, will be using `Jobs` for extensive use cases. 

MOST WE USE ALL THESE FOR **LOGS, BACKUP AND RESTORE**. once a job gets done, POD gets hibernated printing the status as `completed` if job is successful or else, `failed` or `terminated` and we should be troubleshoot them. 
you can verify that using
```sh
kubectl get po -o wide
```

##### **Workflow**: `Jobs` -> POD
Jobs creates PODs. running that specific task.

##### `apiVersion` for `Jobs` Object is `batch/v1`
for jobs, cronjobs, `apiVersion` for `Jobs` Object is `batch/v1`
```yaml
apiVersion: batch/v1
kind: Jobs
```

##### `Jobs` can also run multiple pods in parallel
as the title says, `Jobs` can also run multiple pods in parallel to run, gets done and dusted. 

##### Once task specified in POD container was successful, **Job** goes to completed status
as same as we discussed earlier is that,  once a job gets done, POD gets hibernated printing the status as `completed` if job is successful or else, `failed` or `terminated` and we should be troubleshoot them. 

##### **Job** `selector` is optional
while writing the manifest for running a `Job` object, we can run `selector`, `labeling` but them are all totally optional. nothing happens and works just fine if we fail to mention too. 

##### PODs controlled by **Job** object `restartPolicy` should be set to only `Never` or `OnFailure`, not `Always`
```yaml
restartPolicy: never/onFailure
```
but not `always`, since `jobs` object are supposed not to be actively by nature. if ain't, that is not a `Job` running an instructions, runs if runs, fails if fails. 

##### Types of parallel executions Jobs:
1) Non-parallel **jobs**
2) Parallel **jobs** with a fixed completion count
3) Parallel **jobs** with a work queue
what are the types parallel executions we can do under the `Jobs` object,
1) Non-parallel **jobs** - *default*
2) Parallel **jobs** with a fixed completion count - need PARAMS
3) Parallel **jobs** with a work queue - need PARAMS
will see all these in practice. 

##### Best Practice is to enable `ttlSecondsAfterFinished: 100` for **Job** to clean up automatically after the TTL value (`.spec.ttlSecondsAfterFinished`) which system will not put pressure on the `kube-apiserver` [stable-v1.23]
ALWAYS A BEST PRACTICE TO PERFORM CLEANUPS of the `Jobs`. adds load in the `kube-apiServer` if we fail do cleanup. 
to do these - **enable `ttlSecondsAfterFinished: 100` for Job to clean up automatically after the TTL value (`.spec.ttlSecondsAfterFinished`) which system will not put pressure on the `kube-apiserver`**

default of  `ttlSecondsAfterFinished`: is `100` 

##### We can suspend the Job as well and start again by `.spec.suspend` value to **true** or **false**
We can suspend the Job as well and start again by `.spec.suspend` value to **true** or **false**.

If a **Job** is suspended, any running POD that don't have status completed will be terminated.
eg: `Job` -> `Pod`, is under creation process. meanwhile, you suspend your Job and restart if it fails to create, run or completed status.  to perform that, imperative approach running the command: 
```sh
kubectl batch job/jobName --type=strategic --patch '{"spec":{"suspend":true/false}}'
```

WILL SEE ALL THESE IN PRACTICE.

---
# 2. Jobs workflow(restart Policy)
Jobs workflow (restart Policy) - [Doc]

Practically Implementing a `Job`object 

>and not `nginx` since we don't want a `Job` to be running consistently. Just a `shell` execution is enough to get the **Job** done.

Will take a simple `image` to run just the scripts alone. 

check the api-resources version using:
```sh
kubectl api-resources
kubectl api-resources | grep job
```
> apiVersion: batch/v1, kind: Job

`job.yaml`
```yaml
apiVersion: batch/v1
kind: Job
metadata:
  name: job1
spec:
  template:
    spec:
      containers:
      - name: job1-container
        image: alpine:latest
        command: ["/bin/sh"]
        args: ["-c", "echo Hello from the job1"]
      restartPolicy: OnFailure
```

**so, what this `job` object does is that,** base syntax of this `job`:
- base boilerplate: `apiVersion: batch/v1, kind: Job` and `metadata`. usual stuff
- `spec`: -> `PodTemplate` ->
- `spec` (of containers) -> name of the job container, just a simple `image` to run the shell, cmd to access the shell of that `image`, passing `args` with the accessed `shell`: Just printing one simple message
- no `volumes` and `mounts` has been set here, because it is irrelevant and we are saving nothing here to have a volume for this **job**
- mandatory for the **Job** is to have `restartPolicy` and it should never have `Always` but can have `never` or `OnFailure`.
###### 1) `restartPolicy: Never`
will apply and verify the manifest:
```sh
kubectl create -f /path/to/job.yaml
```

now, will dig deep in what's up here,
```sh
kubectl get jobs -o wide
```
output:
```
NAME            job1 #nameOfTheJob
STATUS          Complete #status
COMPLETIONS     1/1 #jobsCompleted
DURATION        5s  #execTime
AGE             5m29s #age
CONTAINERS      job1-container #container
IMAGES          alpine:latest #image
SELECTOR        batch.kubernetes.io/controller-uid=5d37e837-0d36-4ace-a122-d0a454e9f8c3 #selectorUniqueId
```

now, will dig deep in what's up here,
```sh
kubectl get jobs,po -o wide
```
output:
```
NAME           job1-8ppdt #autoPodName
READY          0/1#Job,notAnApp(oneTime)
STATUS         Completed #doneAndDusted
RESTARTS       0 #noRestarts(successful)
AGE            10m #age
IP             10.244.55.175 #podIP
NODE           workernod e125 #node
NOMINATED NODE <none>
READINESS GATES<none>
```

to inspect further of a `job`and check the events.
```sh
kubectl describe po podName
```

Where is the `execution`? (the printed message): check logs.
```sh
kubectl logs po podName
```

if we, declared `restartPolicy=never` and the exec get failed instead of succeed, 
```
error
```
and keeps on recreating the contained inside the POD and executing the jobs and keeps failing to create one. 

Like this:
![[Pasted image 20250215180857.png]]


**tinkering with cleanup:**
```sh
kubectl delete po podName
```
no pods will get recreated, obviously. It is just a job. IT IS OBSELETE.
```sh
kubectl logs po podName
```
none gets printed since we deleted the POD and no exec can be seen when the POD is absent. But the `job` still shows. Since its done and dusted. 

if, we directly delete a `job`, POD gets deleted within
```sh
kubectl delete job jobName
kubectl get jobs,po -o wide
```

###### 2) `restartPolicy: OnFailure`
for this `restartPolicy: OnFailure`, will perform something different to under this `OnFailure `

`job-onFailure.yaml`
```yaml
apiVersion: batch/v1
kind: Job
metadata:
  name: job1
spec:
  template:
    spec:
      containers:
      - name: job1-container
        image: alpine:latest
        command: ["/bin/sh"]
        args: ["-c", "ls -l samm"] #which doesn't exist and fails  
      restartPolicy: OnFailure
```
If the exec, succeeds, it just runs.
This one will gets into error status, since the cmd `exec` fails. 

apply the same and see what happens,
```sh
kubectl create -f /path/to/job-onFailure.yaml
```

verify in an sec and see
```sh
kubectl get jobs,po -o wide
kubectl get jobs,po -o wide
```
IT RESTARTS ONCE (not recreated) and Throws `CrashLoopBackOff` error with multiple restarts  and quits. 

The reason for recreated of the PODs is for the iteration until it succeeds.

JOB was created the `Pod` got failed. 

Inspect it,
```sh
kubectl describe po podName
```

EVERYTHING GOES SAME FOR `cronjobs` too.

delete,
```sh
kubectl delete job jobName
```

**WHAT IF, `restartPolicy: Always`**?
IT WILL NOT CREATE A `JOB` OBJECT AT ALL. Schema validation doesn't get pass through. telling
```
The Job "job2" is invalid: spec.template.spec.restartPolicy: Required value: valid values: "OnFailure", "Never"
```

**WHAT IF, we fail to declare `restartPolicy`**?
SAME. **restartPolicy** is a must. Schema validation doesn't get pass through. telling
```
The Job "job2" is invalid: spec.template.spec.restartPolicy: Required value: valid values: "OnFailure", "Never"
```

:)

---

# 3. Jobs termination and cleanup
Jobs termination and cleanup - [Doc]

Now will see more about
- Jobs creation process
- what happens under the hood
- Job Execution
- Job Termination
- Job cleanup and more.

Will take a right manifest which a `job` gets successful. 

`hellojob1.yaml`
```Yaml
apiVersion: batch/v1
kind: Job
metadata:
  name: job1
spec:
  template:
    spec:
      containers:
      - name: job1-container
        image: alpine:latest
        command: ["/bin/sh"]
        args: ["-c", "echo Hello from the job1"]
      restartPolicy: Never
```

apply the job,
```sh
kubectl create -f /path/to/hellojob1.yaml
kubectl get jobs,po -o wide
kubectl logs podName
```

1) the jobs get completed 1/1(unless and until there's an error)
2) the pods get 0/1 cause, logically it is a one time exec and just dies after completion. - default behavior of `job` object

**To cleanup these:**
```sh
kubectl delete jobs jobName/orTheManifest
```
you can keep all these resources, if you want to troubleshoot or perform and investigation or sum.

to see like,
- what was up with this job
- what are all the step that has been executed
- what are the after effects and such.
for the administrator's conveniences.

Since the `restartPolicy: never`, if the exec is an error. The Job will be recreated again and again trying to exec the same successfully. 

###### backOffLimit
AGAIN, A NEW POD WILL BE CREATED TO FIX THE SAME. GETS FAILED AND `+1` AND `1` AND `1`. so,
**WHATS THE MAXIMUM?**

will see that by executing a failure command,
```yaml
apiVersion: batch/v1
kind: Job
metadata:
  name: job1
spec:
  template:
    spec:
      containers:
      - name: job1-container
        image: alpine:latest
        command: ["/bin/sh"]
        args: ["-c", "ls -l samm"] #which doesn't exist and fails  
      restartPolicy: Never
```
*process: containerCreating -> Error*
after sometime, it stops up to a certain number. **possibly `7`**. The highest maximum. -> known as **BackOffLimit**. and
A job's **backOffLimit** default value is 7. 

`Job will reach the specified backoff limit.`


**WHATS THE MAXIMUM?**
to inspect that,
```sh
kubectl describe job jobName
kubectl get jobs jobName -o yaml
```
and there will be the backOffLimits
and you get more of the same. 

**How to set backOffLimits**? in the manifest itself.
```yaml
spec:
  backoffLimit: 3
  template:
    spec:
      containers:
      ...
```
and that simple. and work it out by creating the same. 

```sh
kubectl get jobs jobName -o yaml
kubectl create jobs jobName -o yaml
kubectl describe jobs jobName -o yaml
```
failed: 4 `n+1) `

###### Suspend
 when we create a `job`, as per the workflow, it immediately proceeds to create a POD with a container in it. 

If you don't want that. I don't want to trigger the POD to run the execution right away. 
To do that, we have to suspend the `job`. 
 
```yaml
spec:
  backoffLimit: 3
  suspend: true
  template:
    spec:
      containers:
      ...
```

create `Job` object,
```sh
kubectl create -f /path/to/jobs.yaml
```

verify,
```sh
kubectl get jobs,po -o wide
kubectl describe job jobName
```
>**`STATUS: Suspended`**, NOT POD has been created yet. 
`Events:
Job suspended by `job-controller`

It just suspended the `job`, since we passed `suspend: true`

OKAY, BUT WHY?
for deployments and such, if we create and run a job before deploying the app. 
IT SIMPLY SPINS THE TASK AND GET SUSPENDED.


to un-suspend: `suspend: false`
```sh
kubectl edit job jobName
```
and modify `suspend` parameter `=false. 

Immediately, triggers the POD right after the edit. 


###### cleanups:
```sh
kubectl delete jobs jobName/orTheManifest
```
this we all know. 

cleanup after every POD execs and it just exists after completion as a trash. IT SIMPLY EXIST IN THE CLUSTER AFTER COMPLETION DOING NOTHING. 

HOW BOUT `deletion` after the `Job` is done. It is possible by with the help of `ttl`- **Time To leave**,  passing the  param to perform cleanUp after seconds, minutes and hours. `ttlSecondsAfterFinished: 10`:
```yaml
spec:
  backoffLimit: 3
  suspend: true
  ttlSecondsAfterFinished: 10
  template:
    spec:
      containers:
      ...
```

FINAL MANIFEST:
```yaml
apiVersion: batch/v1
kind: Job
metadata:
  name: job1
spec:
  backoffLimit: 3
  suspend: false
  ttlSecondsAfterFinished: 10 #time to leave
  template:
    spec:
      containers:
      - name: job1-container
        image: alpine:latest
        command: ["/bin/sh"]
        args: ["-c", "echo Hello from the job1"]
      restartPolicy: Never
```

OR to modify and apply just the changes,
```yaml
spec:
  backoffLimit: 3
  suspend: true
  ttlSecondsAfterFinished: 10
  template:
    spec:
      containers:
      ...
```
and
```sh
kubectl replace --force -f /path/to/jobs.yaml
kubectl get jobs,po -o wide 
```

> ===**GETS DELETED AFTER merely 10Seconds of Execution**. AUTOCLEANUP===

![[moss.jpeg]]

---
