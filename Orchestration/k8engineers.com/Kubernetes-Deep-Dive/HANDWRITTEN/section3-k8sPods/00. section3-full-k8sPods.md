---

---
***
### Section 3: K8s Pods
#### 1.  Kubernetes Objects overview
###### Kubernetes Overview & Pod- [Doc](obsidian://open?vault=tutorialHell&file=Orchestration%2Fk8engineers.com%2FKubernetes-Deep-Dive%2Foffl-raw-docs%2Fkubernetes%20deepdive%2Fsec3-k8s-pods%2F1.Kubernetes%20Overview%20%26%20Pod%20Introduction.docx)
#### 2. POD overview
#### 3.Integrating VS code with k8s cluster
######  Integrating VS code with K8s Cluster - [Doc](obsidian://open?vault=tutorialHell&file=Orchestration%2Fk8engineers.com%2FKubernetes-Deep-Dive%2Foffl-raw-docs%2Fkubernetes%20deepdive%2Fsec3-k8s-pods%2F2.Integrating%20VS%20code%20with%20K8s%20Cluster.pdf)
#### 4.Overview on k8s objects creation using imperative and declarative approach
#### 5.POD creation using Declarative approach
#### 6.POD creation using Imperative approach
###### Pods creation using imperative approach - [Doc](obsidian://open?vault=tutorialHell&file=Orchestration%2Fk8engineers.com%2FKubernetes-Deep-Dive%2Foffl-raw-docs%2Fkubernetes%20deepdive%2Fsec3-k8s-pods%2F3.Pods%20creation%20using%20imperative%20approach.docx)
####  7.POD creation workflow
###### Pod Creation Workflow -[Doc](obsidian://open?vault=tutorialHell&file=Orchestration%2Fk8engineers.com%2FKubernetes-Deep-Dive%2Foffl-raw-docs%2Fkubernetes%20deepdive%2Fsec3-k8s-pods%2F4.Pod%20Creation%20Workflow.docx)
#### 8. POD resource allocation CPU and Memory
####  9.POD multi-container with shared volume
###### Multi-container pod with Shared Volume - [Doc](obsidian://open?vault=tutorialHell&file=Orchestration%2Fk8engineers.com%2FKubernetes-Deep-Dive%2Foffl-raw-docs%2Fkubernetes%20deepdive%2Fsec3-k8s-pods%2F5.Multicontainer%20pod%20with%20Shared%20Volume.docx)
####  10.Handling containers in POD using crictl(restart container POD)
###### Handling containers in Pod using crictl - [Doc](obsidian://open?vault=tutorialHell&file=Orchestration%2Fk8engineers.com%2FKubernetes-Deep-Dive%2Foffl-raw-docs%2Fkubernetes%20deepdive%2Fsec3-k8s-pods%2F6.Handling%20containers%20in%20Pod%20using%20crictl_.docx)
#### 11.Access POD application outside cluster(hostPort)
###### Access pod application outside cluster(hostport) - [Doc](obsidian://open?vault=tutorialHell&file=Orchestration%2Fk8engineers.com%2FKubernetes-Deep-Dive%2Foffl-raw-docs%2Fkubernetes%20deepdive%2Fsec3-k8s-pods%2F7.Access%20pod%20application%20outside%20cluster(hostport).docx)
####  12.POD initContainers Introduction
###### Pod initContainers - [Doc](obsidian://open?vault=tutorialHell&file=Orchestration%2Fk8engineers.com%2FKubernetes-Deep-Dive%2Foffl-raw-docs%2Fkubernetes%20deepdive%2Fsec3-k8s-pods%2F8.Pod%20initContainers.docx)
####  13.POD initContainers
####  14.POD Lifecycle: restart policy
###### Pod Lifecycle Restart Policy - [Doc](obsidian://open?vault=tutorialHell&file=Orchestration%2Fk8engineers.com%2FKubernetes-Deep-Dive%2Foffl-raw-docs%2Fkubernetes%20deepdive%2Fsec3-k8s-pods%2F9.Pod%20Lifecycle%20Restart%20Policy.docx)
####  15.Static POD (controlled by Kubelet)
###### Static Pod (Controlled by kubelet) - [Doc](obsidian://open?vault=tutorialHell&file=Orchestration%2Fk8engineers.com%2FKubernetes-Deep-Dive%2Foffl-raw-docs%2Fkubernetes%20deepdive%2Fsec3-k8s-pods%2F10.Static%20Pod%20(Controlled%20by%20kubelet).docx)
####  16.Challenges of standalone POD applications
###### Challenges of Standalone Pod - [Doc](obsidian://open?vault=tutorialHell&file=Orchestration%2Fk8engineers.com%2FKubernetes-Deep-Dive%2Foffl-raw-docs%2Fkubernetes%20deepdive%2Fsec3-k8s-pods%2F11.Challenges%20of%20Standalone%20Pod.docx)
---

#### 1.  Kubernetes Objects overview
###### Kubernetes Overview & Pod- [Doc](obsidian://open?vault=tutorialHell&file=Orchestration%2Fk8engineers.com%2FKubernetes-Deep-Dive%2Foffl-raw-docs%2Fkubernetes%20deepdive%2Fsec3-k8s-pods%2F1.Kubernetes%20Overview%20%26%20Pod%20Introduction.docx)

Kubernetes Objects overview:
**Terminologies**:
> **Objects**: also previously has been named or called as service, components. Primarily, it is called as Kubernetes objects. 
>  IF I WANT TO CREATE ANYTHING ON KUBERNETES CLUSTER, CALLED AS OBJECTS. 

Will see what are all the objects that are available in Kubernetes Cluster. Objects can also be named as **workloads**.  

> ultimately we're using Kubernetes Cluster to deploy applications. While deploying, we use objects to make things happen.

E.g.:  
#### Approaches to deploy application on K8s Cluster,
using objects or workloads such as pods, config maps, secrets, network policy's, and other relevant resources -> a.k.a objects. 

Eg: 
Workloads  -->  Pod, Replication Controller/ReplicaSet, Deployment Set, DaemonSet, Stateful sets and Jobs.
These are named as Workloads because, you are deploying application on Kubernetes Cluster are of simply these objects. Simply a Pod or the rest of the object.

You have to create any one of them to deploy any one of the application on a kubernetes cluster. = Objects

**Objects**: Except all the workloads, whatever that gets created are know as objects, such as 
- All the workloads itself, 
- Service, (people were getting confused about this service and the object as service)
- ConfigMap/ Secrets,
- Ingress, 
- Network policy, 
- PV, PVC and Storage Classes, etc..  
> **previously, these has been called out as SERVICE - Legacy**

So! all of these here are all **OBJECTS**.
> THIS IS EXPLAINED HERE TO GET HABITUATED AND CLARITY ON THESE OBJECTS AND WORKLOADS.

>**Now, WILL SEE MORE ON PODS**
Will see answers for,
- what
- why (creating one in the first place)
- how (to use)
- benefits of a pod (rather than container tools)
- and more..

---
#### 2. POD overview
POD overview:
After getting know all about the Kubernetes Architecture and setting up, we gotta see at first is PODS.

Why? 
PODS: Whatever you seek to deploy on a Kubernetes Cluster, it going to end up in a POD. Any workload, any objects, any components, will be all in a POD. That Pod creates containers. 
> SO IT IS ESSNTIAL AND NECCESSARY TO LEARN MORE ABOUT **PODS** IN DEEP. if this is not understandable, you'll never become devops guy deploying applications on a Kubernetes Cluster.

 Will see Pods in detail, 
> [!NOTE] **What is a POD**
> -  Pod **creates a logical layer** 
> to group 1 or more containers =  to have common network + shared storage.
> - Pod has a unique IP address assigned from `--pod-network-cidr=10.244.0.0/16`
> - Containers inside those POD talks to each other on `localhost` domain or `127.0.0.1`, since container share the same network stack. 
> - Containers share data inside POD.
> - In general, we need to create containers inside POD 
> which are dependent, not different application. 
1. why a logical layer to create one or more containers? to have common network and storage. will see that in brief.
2. here, this argument in specific, `--pod-network-cidr=10.244.0.0/16`, when we executed `kubeadm init` command, to create single, multi or HA nodes for kubernetes setup. here `--pod-network-cidr=ip/block`. **If i want to reach the application inside the POD, have to use POD IP Address not the container IP Address.**
The Pods IP Address is an ephemeral IP ADDRESS - Not fixed to the pod. If Pod gets removed so the IP Address too. If a new one gets deployed, it won't be or get the same even when you deploy the exact same app. Just like DHCP where IP gets assigned in random. Here, it does the same but under a CIDR Block. but still that isn't fixed.
3. Containers will talk to each other mostly on loopback interface -  `localhost` `127.0.0.1`. no matter the machine such as bare metal, vm, cloud instance or containers. - Loop back interface. **CONTAINERS INSIDE THE POD WILL TALK VIA `localhost` or `127.0.0.1` as you call it .**
WHY? e.g.: Inside a POD you have multiple containers for an application or any workload, ALL OF THESE SHARE THE SAME NAMESPACE. ESPECIALLY NETWORK NAMESPACE. SO, IF ==YOU WANT TO COMMUNICATE TO AN APPLICATION OUTSIDE THE POD, WE NEED **POD's IP ADDRESS**, *NOT CONTAINERS*. INSIDE THAT POD, CONTAINERS WILL TALK TO EACH OTHER ON `localhost` or `127.0.0.1`.==
4. Incase if we have multiple containers inside a POD, I might want to replicate data among all the containers inside the pod. Here comes the **SHARED STORAGE**. (As we discussed in 1st Point). To achieve shared storage, have to use the concept of POD. 
-> The reason why we use PODs not plain containers is this. Common Network and Shared storage.
5. One app in a POD which can be either in a single or multi-containers, not a whole different application. Containers that are relevant to the application is recommended. Though, you can but IT WILL BE CLUNKY if it is all different. 
###### Architecture of a POD:
1) **Common Network:**
![[Pasted image 20240821105001.png]]
POD -> Common network namespace -> Containers under that namespace (sharing same network which can talk bidirectional via `localhost`, not matter how many containers in a pod, shares the same namespace). 
WHY? Benefits of Common network. If need to communicate outside the POD, can use **POD IP Address.** 
- Communication within or between containers - `localhost` x `127.0.0.0`
- Communication outside containers - `Pod IP Address`


2) **Shared Storage:**
![[Pasted image 20240821105656.png]]
POD -> Pod network namespace -> Containers under that namespace -> Shares same volume which is a Shared Storage. 
Shared Storage - Containers that are sitting inside the POD shares data between the containers in common. 
WHY? Benefits of Shared Storage. Data that comes to that POD which can be shared in common between containers inside that POD. 
eg: One container that hold or contains some data within, It will be housed on or under that common volume which is shared storage. Which that can be used by another container too. Sharing User credentials for IAM between containers services for a common application for example.

==Having Common Network and Shared storage for one or more containers in a namespace, we can achieve the concept of POD. The reason for using PODs not plain containers in Kubernetes is this.

>**Pod is simply a logical layer. Programmatically, It is simply having one container or grouping multiple container together to have a common network and shared storage. ==**

---
#### 3.Integrating VS code with k8s cluster
######  Integrating VS code with K8s Cluster - [Doc](obsidian://open?vault=tutorialHell&file=Orchestration%2Fk8engineers.com%2FKubernetes-Deep-Dive%2Foffl-raw-docs%2Fkubernetes%20deepdive%2Fsec3-k8s-pods%2F2.Integrating%20VS%20code%20with%20K8s%20Cluster.pdf)
SETUP:
1) 2 VMs,
- One Control Plane - 2vCPU, 2GB Ram, 12GB Storage
- One worker Node - same
2) Visual Studio Code
3) Install Remote-SSH Extension
**Objective**: WANT TO WRITE KUBERNETES YAML FILES FROM HERE FOR ALL THE ESSENTIAL SETUP ON THESE VIRTUAL MACHINES.

Taking VM in remote and making YAML Manifests instead of inside the machine since it is only CLI. So, Write in local , Execute in remote

Visual Studio Code -> SSH Icon -> Open SSH Configuration File -> open terminal and land in `c:\users\username\.ssh\` 
```
ssh-keygen.exe
<enter>empty
y
<enter>empty - no passphrase
```
Key got generated. Copy public key fingerprint to that machine. 
```
ssh-copy-key username@ip
```
> DOING **PASSWORD-LESS KEY-BASED AUTH** instead of **PASSWORD BASED AUTH**

SSH -> Connect to Host -> add hosts `username@ip` -> add config -> Connect back to host. Voila!

**SMOOTH AS BUTTER!**

---
#### 4. Overview on k8s objects creation using imperative and declarative approach

Overview on k8s objects creation using imperative and declarative approach

**APPROCHES TO CREATE OBJECTS ON K8s**: there are two approaches,
`kubectl` : 
e.g.: executing `kubectl`, need to create some objects on our Kubernetes Cluster. Not just pod by deployment, services, state sets and more. HOW TO CREATE SUCH OBJECTS?
- **Declarative approach** -  Declaring the same by giving all the required objects by writing it in a `YAML` File.
This YAML will be given via `kubectl`to the `api-server` that takes cares of the rest by receiving it, doing schema validation and passed information to the `etcd` that talks to  its relevant components in order to create all the same which we've declared in the manifests.
- **Imperative approach -**  Creating resources from the CLI itself. giving it all in our CLI as commands to create objects. No YAML or JSON files excepts systems, services and components needs and manifests. 
eg: If using 
- docker with cli - imperative approach
- writing docker-compose file in YAML - Declarative approach.
Here in `kubectl`, can use both of these approaches. 

###### Understanding Declarative Approach: (!IMP - using irl)
![[Pasted image 20240821182425.png]]
```YAML
apiVersion: v1
kind: Pod
metadata:
 name: nginx-demo
 labels:
  app: nginx
  env: prod
spec:
 containers:
   -name: nginx
    image: nginx:lts #latesttag
   ports:
   - name: nginx-port
     containerPort: 80
     protocol: TCP
   env:
    -name: USERNAME
     value: "admin"
   resources:
    limits:
      cpu: "0.2"
      memory: "500Mi" #Ki,Mi,Gi,Ti
     requests:
      cpu: "0.2"
      memory: "200Mi"
    volumeMounts:
     -name: nginx-data
      mountPath: /var/www/html
volumes:
 - name: nginx-data
   emptyDir: {}
```
1. `apiVersion`: ==version of the object. max v3==, *version of what are all the objects that are going to get created.* (will see more on the same by how to confirm and verify the same)
2. `kind`: Object/ Resource name -> ==Kind of object== ,*what object that i want to create*
3. `metadata`:  ==metadata to the object for labeling each, useful for filtering== *name that i want to assign to my POD*, *adding additional information by labeling the object for further identification and filtration by specifying relevant key word.* volumes, containers, init-containers and more. Specifying how many containers i want to create, all the relevant parameters to that containers, - naming it, image, assigning port, passing env variables, resources specs, volume mounting
- name: nginx-demo
- labels: (***app***: nginx, ***env***: prod)
5. `specs`: ==container specifications to be handles by the POD== *specifying what are all the things that i want to create*, 
- containers:
-  -- name: nginx
-  - image: nginx:lts ==tag==
-  - ports (***-name***: nginx-port, ***containerPort***: 80, ***protocol***: TCP) ==container ports section== 
- - env (***-name***: db_username , ***value***: "admin") ==container environment variables== 
- - resources: (***limits***: ***cpu***: "1" ***memory***: "500Mi", ***requests***: ***cpu***: "0.5" ***memory***: "200Mi" ) ==container resource allocation== 
- - volumeMounts: (***-name***: nginx-data, ***mountPath***: /var/www/html) ==container volume mounting point== 
- `volumes`: ==shared volume for POD Containers== 
- --name: nginx-data
- emptyDir:  {}
ALL `compose.yaml` CONTENT.

###### Understanding Imperative Approach: (opt)
Imperative approach is all the same by giving commands. creating each by imperative will be much of a pain So, Why Imperative even exist. 
Helps in,
- smaller operations
- frequent object creation
- patchwork
- operation intensive works
- depends on the circumstances and condition, usage may vary.
BUT IN MOST OF OUR TIME, **==DECLARATIVE APPROACH IS STAGNENT==**. REST OF THE CONDITIONS, imperative.
---
#### 5.POD creation using Declarative approach
Creating YAML Manifests in the VM. Declarative approach.

Before moving forward, will check on these prerequisites,
1) become a `root` user, to use `kubectl` in ease.
```
sudo -i
```
2) setup up all the kubernetes base setup. 
- all the updates
- reboot after initial spin up
- network configuration for `containerd`, `kubernetes-cri` 
- apply the configuration
- installing `containerd` 
- docker's `gpg`, `apt resources`, `containerd` installation and config, `restart` service
- `crictl` config
- installing `kubeadm`, `kubectl`, `kubelet`
- `gpg` keys and `apt` Dir for them
- `enable` and `hold` it all.
- `kubeadm init` for Control plane, `kubeadm join` for Worker node
-setup all done from here until you get something like this.

```
kubeadm init --apiserver-advertise-address=192.168.0.219 --cri-socket=/run/containerd/containerd.sock --pod-network-cidr=10.244.0.0/16
```

Your Kubernetes control-plane has initialized successfully!
To start using your cluster, you need to run the following as a regular user:
```
kubeadm init --apiserver-advertise-address=192.168.0.219 --cri-socket=/run/containerd/containerd.sock --pod-network-cidr=10.244.0.0/16
```
```
mkdir -p $HOME/.kube
sudo cp -i /etc/kubernetes/admin.conf $HOME/.kube/config
sudo chown $(id -u):$(id -g) $HOME/.kube/config
```
Alternatively, if you are the root user, you can run:
```
export KUBECONFIG=/etc/kubernetes/admin.config
```
You should now deploy a pod network to the cluster.
Run 
```
kubeadm apply -f <pod-network>.yaml
```
with one of the options listed at: https://kubernetes.io/docs/concepts/cluster-administration/addons/
Then you can join any number of worker nodes by running the following on each as root:
```
kubeadm join 192.168.0.219:6443 --token xljegf.3d6nt61xmqk33qns \ --discovery-token-ca-cert-hash sha256:968164ba40787980ba10adf93b13abe8adf22e6a10b65a3fad0fdc574941ce7b
```
if any error in the process, 
- clear cache
- rm /.kube/config and respawn it
- ensure disabling `swap` memory
- make sure that `kubelet` is running, enable if not.
- also ensure, kube `cni` configured properly
- end, `kubeadm reset`.
```shell
sudo systemctl enable --now kubelet
```
```shell
kubectl create -f https://raw.githubusercontent.com/projectcalico/calico/v3.28.0/manifests/tigera-operator.yaml

wget https://raw.githubusercontent.com/projectcalico/calico/v3.28.0/manifests/custom-resources.yaml 

nano custom-resources.yaml ##change-cidr
```
```
kubectl apply -f custom-resources.yaml 
```
```
mkdir -p $HOME/.kube
sudo cp -i /etc/kubernetes/admin.conf $HOME/.kube/config
sudo chown $(id -u):$(id -g) $HOME/.kube/config
```
> `kubeadm init` again

**MOVING FORWARD WITH *DECLARATIVE APPROACH* BY USING A `YAML` FILE.** 

vscode -> open folder -> Home folder + Open the same in terminal too.
> STORING ALL THE YAML MANIFESTs IN THE HOME DIRECTORY, now the DECLARATIVE APPROACH of creating PODS.

```
cd ~
mkdir manifests
cd manifests
mkdir PODs
cd PODs
touch hexrapp.yaml
```
ALL IN `vs-code`

says: 
- NAME  - system default name gets applied
- STATUS  - Ready or not-ready
- ROLES - control plane or worker node
- AGE - days of creation
- VERSION - version
- INTERNAL-IP - IP that we configured
- EXTERNAL-IP - will be configured after worker gets joined
- OS-IMAGE - Ubuntu or any image
- KERNEL-VERSION - Kernel's version 
- CONTAINER-RUNTIME - cri used in k8s, containerd.docker or whatever runtimes.

##### Objective:
> **Here in this cluster, I want to create a POD using declarative approach.**
THE OBJECTIVE NOW HERE IS TO CREATE PODs IN THE NODE.

###### To-do: 
- Create a `yaml` file
- where? ~/manifests/pod/ -> manage manifests files in a separate directory 
- touch `app.yaml`
- also store the file in root too. 
- sudo -i
```
mkdir /home/username/.kube
cp .kube/config /home/username/.kube
chown -R testuser: /home/testuser/.kube
exit #root
```
now execute kubectl commands.
```
kubectl get no -o wide
```
> NOW, THE USER CAN ALSO ABLE TO GET THE WORK DONE BY EXECUTING `kubectl` COMMANDS with the help of that config file here and there!

###### Parameters:
Here, What to use for POD creation? check the same: Writing the same on a `YAML` file -> pass it to `kubectl` -> that to the `kube-api-server`
```yaml
apiVersion: v1
kind: Pod
metadata:
  name: hexrapp-demo
  labels:
    app: nginx
    env: prod
spec:
  containers:
  - name: hexrapp
    image: nginx
    ports:
    - containerPort: 8080
```
As we saw previously,  ++to find all the relevant options to declare in `yaml` file, refer context given below for your reference,
1. **apiVersion**
defining `apiVersion` - what version of object i have to use,
```
apiVersion: v1
```

2. **kind**
`kind` - kind of object to use to create the object 
> here it is **`Pod`**. refer that in `api-resources` to declare the version of it.
```
kind: Pod
```

3. **metadata**
naming and labelling the `pod` using `metadata`, two spaces. adding labels by passing parameters. naming it & labelling it-
> # you can pass multiple params to those labels, which doesn't affect the pod behavior, doesn't change the application in anyways. labels are all just metadata. just an additional info to the pods. To filter, to gather information and more. nothing will happen if it didn't even exist but `name` is mandatory.
> labels for `Pod` the list goes on. versions, annotations, namespaces and so many parameters which you can declare as per your requirement. here we are good with `name` and `labels`.

`metadata`: `
  `name`: appname, # any name, mandatory
   `labels`: # are optional
		app: nginx
	    env: prod
```yaml
metadata:
  name: hexrapp-demo
  labels:
    app: nginx
    env: prod
```

4.  **spec**
specifications, that you want to pass for your `Pod`. First is nothing but `containers`. Declaring all the containers that I want to create. 2spaces + hyphen.
>Since, Pod is just a logical layer, `containers` are the same as we use with common network and shared storage, under the layer of `Pod` 

`spec`:
  `containers`: 
     - `name`: frontend # name of the container, not the pod since we gave it in `metadata`
       `image`: nginx:lts # `image` of this container + `version` as tag
       `ports`: # though not important but mandatory
       - `containerPort`: 80 # default nginx port, even if isn't declared, proceeds with default.
       `volumeMount`:
       - `name`: hexr-vol # name of the volume
        `mountpath`: /var/www/html # volume path, whatever gets here saved in 
        `volumes`:
         - name: hexrapp # name of the volume
         emptyDir: {} 
  # parameter that binds the container's volume to the pods shared storage volume. will make sense when we work with this, will see more in future. 
```yaml
spec:
  containers:
  - name: hexrapp
    image: nginx
    ports:
    - containerPort: 80
    volumeMount:
    - name: hexrapp
      mountPath: /var/www/html
  volumes:
  - name: hexrapp
    emptyDir: {}
```
**How to apply the `yaml` file, to create the pod**, simply practicing declarative approach by applying this `yaml` file to the kubernetes cluster.  HOW TO DO THAT? this `yaml` file has to get to the `kube-apiserver` HOW?
`kubectl` - **`create`**, **`apply`**. x2
1) Does a **schema validation** first,
2) Applies the `yaml` file next.
3) pod gets created.

```
kubectl create -f /path/of/hexrapp.yaml
```
```
kubectl apply -y hexrapp.yaml
```
**To verify such pods**
```
kubectl get po/pod/pods #shows pods
kubectl describe po/pod/pods #describes pod in detail 
```
Here, 
- Name: hexrapp # name of the pod
- Ready: 0/1 # expected, actual (running and ready) state vs desired state
- Status: Pending/Running/Stopped # status
- Restarts: 0 
- Age: # pod created time 
> `ready` has to be 1/1 equal to = `status` if it is in Running state.


>++ TROUBLESHOOT IF ANY ERROR OCCURS OR IF ANY MISMATCH OCCURS.
>Possible errors and troubleshoot methods:
>`describe` pods and nodes and check the message troubleshooting the same.
>curl the IP.
> - taints and tolerations
> - worker node misconfigurations
> - network misconfigurations such as `CIDR` or `IP`.
TAINTS AND TOLERATIONS:
```yaml
apiVersion: v1
kind: Pod
metadata:
  name: hexrapp
  labels:
    app: nginx
    env: prod
spec:
  serviceAccountName: default
  containers:
    - name: hexrapp
      image: nginx
      ports:
        - containerPort: 80
      volumeMounts:
        - name: kube-api-access-k662f
          mountPath: /var/run/secrets/kubernetes.io/serviceaccount
          readOnly: true
        - name: hexrapp
          mountPath: /var/www/html
  tolerations:
    - key: "node-role.kubernetes.io/control-plane"
      operator: "Exists"
      effect: "NoSchedule"
  volumes:
    - name: hexrapp
      emptyDir: {}
    - name: kube-api-access-k662f
      projected:
        sources:
          - serviceAccountToken:
              expirationSeconds: 3607
          - configMap:
              name: kube-root-ca.crt
          - downwardAPI: {}

```
```
kubectl describe node <node-name> #verify metrics of the node

kubectl apply -f hexrapp-pod.yaml #apply pod manifest to create

kubectl get po -A -o wide # check pods detyail  brief
```

**SUCCESSFULLY LAUNCHED PODS ON WORKER NODES VIA CONTROL PLANE** using `kubectl`
>**BIG FUCKING DEAL! 
>but a basic pod creation using declarative approach by writing a declarative YAML File in order to create an object/resource in Kubernetes cluster with the help of `kubectl`  **

###### Context:
###### 1. `api-versions
> check it all by using `kubectl api-versions` command to declare relevant versions to the configs or objects.
THESE ARE ALL THE API-VERSIONS TO USE WHEN CREATING OBJECTS ON K8s CLUSTERS. NO MATTER THE CLUSTER or SYSTEMS anything such as AWS, Azure, GCP, and more.
```
kubectl api-versions
```
```
admissionregistration.k8s.io/v1
apiextensions.k8s.io/v1
apiregistration.k8s.io/v1
apps/v1
authentication.k8s.io/v1
authorization.k8s.io/v1
autoscaling/v1
autoscaling/v2
batch/v1
certificates.k8s.io/v1
coordination.k8s.io/v1
crd.projectcalico.org/v1
discovery.k8s.io/v1
events.k8s.io/v1
flowcontrol.apiserver.k8s.io/v1
flowcontrol.apiserver.k8s.io/v1beta3
networking.k8s.io/v1
node.k8s.io/v1
operator.tigera.io/v1
policy/v1
projectcalico.org/v3
rbac.authorization.k8s.io/v1
scheduling.k8s.io/v1
storage.k8s.io/v1
v1
```
shows all the available `api-versions` , might be for each component. THESE ARE ALL THE APIs TO USE TO CREATE **OBJECTS** IN KUBEs **CLUSTER**. No matter the platform whether it is on-prem, local, aws, azure or any other managed service as a matter of fact. ALL ARE SAME IN ALL PLATFORMS. 

also,
###### 2. `api-resources
```
kubectl api-resources 
```

| NAME                                                                                                                     | SHORTNAMES                                    | APIVERSION                      | NAMESPACED | KIND                                 |
| ------------------------------------------------------------------------------------------------------------------------ | --------------------------------------------- | ------------------------------- | ---------- | ------------------------------------ |
| bindings                                                                                                                 |                                               | v1                              | true       | Binding                              |
| componentstatuses                                                                                                        | cs                                            | v1                              | false      | ComponentStatus                      |
| configmaps                                                                                                               | cm                                            | v1                              | true       | ConfigMap                            |
| endpoints                                                                                                                | ep                                            | v1                              | true       | EndPoints                            |
| events                                                                                                                   | ev                                            | v1                              | true       | Event                                |
| limitranges                                                                                                              | limits                                        | v1                              | true       | LimitRange                           |
| namespaces                                                                                                               | ns                                            | v1                              | false      | Namespace                            |
| nodes                                                                                                                    | no                                            | v1                              | false      | Node                                 |
| persistentvolumeclaims                                                                                                   | pvc                                           | v1                              | true       | PersistentVolumeClaim                |
| persistentvolumes                                                                                                        | pv                                            | v1                              | false      | PersistentVolume                     |
| pods                                                                                                                     | po                                            | v1                              | true       | Pod                                  |
| podtemplates                                                                                                             |                                               | v1                              | true       | PodTemplate                          |
| replicationcontrollers                                                                                                   | rc                                            | v1                              | true       | ReplicationController                |
| resourcequotas                                                                                                           | quota                                         | v1                              | true       | ResourceQuota                        |
| secrets                                                                                                                  |                                               | v1                              | true       | Secret                               |
| serviceaccounts                                                                                                          | sa                                            | v1                              | true       | ServiceAccount                       |
| services                                                                                                                 | svc                                           | v1                              | true       | Service                              |
| mutatingwebhookconfigurations                                                                                            |                                               | admissionregistration.k8s.io/v1 | false      | MutatingWebhookConfiguration         |
| validatingadmissionpolicies                                                                                              |                                               | admissionregistration.k8s.io/v1 | false      | ValidatingAdmissionPolicy            |
| validatingadmissionpolicybindings                                                                                        |                                               | admissionregistration.k8s.io/v1 | false      | ValidatingAdmissionPolicyBinding     |
| validatingwebhookconfigurations                                                                                          |                                               | admissionregistration.k8s.io/v1 | false      | ValidatingWebhookConfiguration       |
| customresourcedefinitions                                                                                                | crd,crds                                      | s.k8s.io/v1                     | false      | apiextensionCustomResourceDefinition |
| apiservices                                                                                                              |                                               | apiregistration.k8s.io/v1       | false      | APIService                           |
| controllerrevisions                                                                                                      |                                               | apps/v1                         | true       | ControllerRevision                   |
| daemonsets                                                                                                               | ds                                            | apps/v1                         | true       | DaemonSet                            |
| deployments                                                                                                              | deploy                                        | apps/v1                         | true       | Deployment                           |
| replicasets                                                                                                              | rs                                            | apps/v1                         | true       | ReplicaSet                           |
| statefulsets                                                                                                             | sts                                           | apps/v1                         | true       | StatefulSet                          |
| selfsubjectreviews                                                                                                       |                                               | authentication.k8s.io/v1        | false      | SelfSubjectReview                    |
| tokenreviews                                                                                                             |                                               | authentication.k8s.io/v1        | false      | TokenReview                          |
| localsubjectaccessreviews                                                                                                |                                               | authorization.k8s.io/v1         | true       | LocalSubjectAccessReview             |
| selfsubjectaccessreviews                                                                                                 |                                               | authorization.k8s.io/v1         | false      | SelfSubjectAccessReview              |
| selfsubjectrulesreviews                                                                                                  |                                               | authorization.k8s.io/v1         | false      | SelfSubjectRulesReview               |
| subjectaccessreviews                                                                                                     |                                               | authorization.k8s.io/v1         | false      | SubjectAccessReview                  |
| horizontalpodautoscalers                                                                                                 | hpa                                           | autoscaling/v2                  | true       | HorizontalPodAutoscaler              |
| cronjobs                                                                                                                 | cj                                            | batch/v1                        | true       | CronJob                              |
| jobs                                                                                                                     |                                               | batch/v1                        | true       | Job                                  |
| certificatesigningrequests                                                                                               | csr                                           | certificates.k8s.io/v1          | false      | CertificateSigningRequest            |
| leases                                                                                                                   |                                               | coordination.k8s.io/v1          | true       | Lease                                |
| bgpconfigurations                                                                                                        |                                               | crd.projectcalico.org/v1        | false      | BGPConfiguration                     |
| bgpfilters                                                                                                               |                                               | crd.projectcalico.org/v1        | false      | BGPFilter                            |
| bgppeers                                                                                                                 |                                               | crd.projectcalico.org/v1        | false      | BGPPeer                              |
| blockaffinities                                                                                                          |                                               | crd.projectcalico.org/v1        | false      | BlockAffinity                        |
| caliconodestatuses                                                                                                       |                                               | crd.projectcalico.org/v1        | false      | CalicoNodeStatus                     |
| clusterinformations                                                                                                      |                                               | crd.projectcalico.org/v1        | false      | ClusterInformation                   |
| felixconfigurations                                                                                                      |                                               | crd.projectcalico.org/v1        | false      | FelixConfiguration                   |
| globalnetworkpolicies                                                                                                    |                                               | crd.projectcalico.org/v1        | false      | GlobalNetworkPolicy                  |
| globalnetworksets                                                                                                        |                                               | crd.projectcalico.org/v1        | false      | GlobalNetworkSet                     |
| hostendpoints                                                                                                            |                                               | crd.projectcalico.org/v1        | false      | HostEndpoint                         |
| ipamblocks                                                                                                               |                                               | crd.projectcalico.org/v1        | false      | IPAMBlock                            |
| ipamconfigs                                                                                                              |                                               | crd.projectcalico.org/v1        | false      | IPAMConfig                           |
| ipamhandles                                                                                                              |                                               | crd.projectcalico.org/v1        | false      | IPAMHandle                           |
| ippools                                                                                                                  |                                               | crd.projectcalico.org/v1        | false      | IPPool                               |
| ipreservations                                                                                                           |                                               | crd.projectcalico.org/v1        | false      | IPReservation                        |
| kubecontrollersconfigurations                                                                                            |                                               | crd.projectcalico.org/v1        | false      | KubeControllersConfiguration         |
| networkpolicies                                                                                                          |                                               | crd.projectcalico.org/v1        | false      | NetworkPolicy                        |
| networksets                                                                                                              |                                               | crd.projectcalico.org/v1        | true       | NetworkSet                           |
| endpointslices                                                                                                           |                                               | discovery.k8s.io/v1             | true       | EndpointSlice                        |
| events                                                                                                                   | ev                                            | events.k8s.io/v1                | true       | Event                                |
| flowschemas                                                                                                              |                                               | flowcontrol.apiserver.k8s.io/v1 | false      | FlowSchema                           |
| prioritylevelconfigurations                                                                                              |                                               | flowcontrol.apiserver.k8s.io/v1 | false      | PriorityLevelConfiguration           |
| ingressclasses                                                                                                           |                                               | networking.k8s.io/v1            | false      | IngressClass                         |
| ingresses                                                                                                                | ing                                           | networking.k8s.io/v1            | true       | Ingress                              |
| networkpolicies                                                                                                          |                                               | networking.k8s.io/v1            | true       | NetworkPolicy                        |
| runtimeclasses                                                                                                           |                                               | node.k8s.io/v1                  | false      | RuntimeClass                         |
| apiservers                                                                                                               |                                               | operator.tigera.io/v1           | false      | APIServer                            |
| imagesets                                                                                                                |                                               | operator.tigera.io/v1           | false      | ImageSet                             |
| installations                                                                                                            |                                               | operator.tigera.io/v1           | false      | Installation                         |
| tigerastatuses                                                                                                           |                                               | operator.tigera.io/v1           | false      | TigeraStatus                         |
| poddisruptionbudgets                                                                                                     | pdb                                           | policy/v1                       | true       | PodDisruptionBudget                  |
| bgpconfigurations                                                                                                        | bgpconfig,bgpconfigs                          | projectcalico.org/v3            | false      | BGPConfiguration                     |
| bgpfilters                                                                                                               |                                               | projectcalico.org/v3            | false      | BGPFilter                            |
| bgppeers                                                                                                                 |                                               | projectcalico.org/v3            | false      | BGPPeer                              |
| blockaffinities                                                                                                          | blockaffinity,affinity,affinities             | projectcalico.org/v3            | false      | BlockAffinity                        |
| caliconodestatuses                                                                                                       | caliconodestatus                              | projectcalico.org/v3            | false      | CalicoNodeStatus                     |
| clusterinformations                                                                                                      | clusterinfo                                   | projectcalico.org/v3            | false      | ClusterInformation                   |
| felixconfigurations                                                                                                      | felixconfig,felixconfigs                      | projectcalico.org/v3            | false      | FelixConfiguration                   |
| globalnetworkpolicies                                                                                                    | gnp,cgnp,calicoglobalnetworkpolicies          | projectcalico.org/v3            | false      | GlobalNetworkPolicy                  |
| globalnetworksets                                                                                       GlobalNetworkSet |                                               | projectcalico.org/v3            | false      | GlobalNetworkSet                     |
| hostendpoints                                                                                                            | hep,heps                                      | projectcalico.org/v3            | false      | HostEndpoint                         |
| ipamconfigurations                                                                                                       | ipamconfig                                    | projectcalico.org/v3            | false      | IPAMConfiguration                    |
| ippools                                                                                                                  |                                               | projectcalico.org/v3            | false      | IPPool                               |
| ipreservations                                                                                                           |                                               | projectcalico.org/v3            | false      | IPReservation                        |
| kubecontrollersconfigurations                                                       project                              |                                               | calico.org/v3                   | false      | KubeControllersConfiguration         |
| networkpolicies                                                                                                          | cnp,caliconetworkpolicy,caliconetworkpolicies | projectcalico.org/v3            | false      | NetworkPolicy                        |
| networksets                                                                                                              | netsets                                       | projectcalico.org/v3            | true       | NetworkSet                           |
| profiles                                                                                                                 |                                               | projectcalico.org/v3            | false      | Profile                              |
| clusterrolebindings                                                                                                      |                                               | rbac.authorization.k8s.io/v1    | false      | ClusterRoleBinding                   |
| clusterroles                                                                                                             |                                               | rbac.authorization.k8s.io/v1    | false      | ClusterRole                          |
| rolebindings                                                                                                             |                                               | rbac.authorization.k8s.io/v1    | true       | RoleBinding                          |
| roles                                                                                                                    |                                               | rbac.authorization.k8s.io/v1    | true       | Role                                 |
| priorityclasses                                                                                                          | pc                                            | scheduling.k8s.io/v1            | false      | PriorityClass                        |
| csidrivers                                                                                                               |                                               | storage.k8s.io/v1               | false      | CSIDriver                            |
| csinodes                                                                                                                 |                                               | storage.k8s.io/v1               | false      | CSINode                              |
| storageclasses                                                                                                           | sc                                            | storage.k8s.io/v1               | false      | StorageClass                         |
| volumeattachments                                                                                                        |                                               | storage.k8s.io/v1               | false      | VolumeAttachment                     |
to list all the versions of all these objects to create the manifest to spin objects declaratively. 
```
kubectl api-resources | grep ^pods
```
Shows the information, such as name, shortname, apiVersion, namespaced, kind. 
>Here, the `version` and `kind` matters in terms of creating pods.

---
#### 6.POD creation using Imperative approach
###### Pods creation using imperative approach - [Doc](obsidian://open?vault=tutorialHell&file=Orchestration%2Fk8engineers.com%2FKubernetes-Deep-Dive%2Foffl-raw-docs%2Fkubernetes%20deepdive%2Fsec3-k8s-pods%2F3.Pods%20creation%20using%20imperative%20approach.docx)
Will cover creating a pod using imperative approach simply by passing commands via the cli.
will delete the pod and respawn for our practice sake.
```
kubectl delete po <podname> / --all(not recommended espeacially in prod env)
```

> [!NOTE] small note about `namespaces`
> if you do,
> ```
> kubectl get po -o wide
> ```
> it will tell you that,
> *No resources found in default namespace.*
> What is namespace means here? - Namespace - an isolated environment. if you create any object or resource, it will housed under default namespace.

WILL SEE MORE ON THAT IN FUTURE.
++

| obj                        | Docker declarative     | Docker imperative | K8s declarative | K8s imperative         |
| -------------------------- | ---------------------- | ----------------- | --------------- | ---------------------- |
| **Creating  an image/pod** | Dockerfile             | `docker build`    | manifests       | `kubectl create/apply` |
| **Running an image/pod**   | Docker desktop or extn | `docker run`      | manifests       | `kubectl run`          |
| **Executing a pod**        | terminal, vscode, etc  | `docker exec`     | manifests       | `kubectl exec`         |
similar to that!

Now back to topic,
###### Creating POD using imperative approach
```
kubectl run --help
```
refer the same in brief and will see that here one by one. 
**USAGE/SYNTAX:**
```
  kubectl run NAME --image=image [--env="key=value"] [--port=port] [--dry-run=server|client] [--overrides=inline-json]
[--command] -- [COMMAND] [args...] [options]
```
also, you can refer, 
```
kubectl options
```
for list of global command-line options that you can make use out of. ALL THE OPTIONS/PARAMETERS THAT WE CAN PASS.

**WILL SEE CREATING A RESOURCE BY RUNNING THE SAME (CMDs) - IMPERATIVE APPROACH**
JUST LIKE WE DO IN DOCKER.
```
kubectl run -it <podname> --image=<imgname:v> --restart=Always/onfailure/never  
```
  ```
kubectl run -itd hexrapp --image=nginx:1.26.2 --restart=Always
```
>nginx:lts - didn't work for me. so, specify versions for not messing things up. WILL THROW A `IMAGEPULLBACK` ERROR. ++ -d, didn't work 
>Flags, -i = interactive, -t = tty, terminal, -- bash -> shell access / execution

**TO CHECK LOGS OF A POD:**
```
kubectl logs <podname>
```
**OF MULTIPLE PODS:**
```
kubectl logs podname1 podname2 .. podnamen
```

HERE,NOW YOU CAN ABLE TO CREATE OBJECTS/RESOURCES USING IMPERATIVE APPROACH. (From the cli)

BUT,
> **DECLARATIVE APPROACH IS HIGHLY RECOMMENDED**

WHY?
- Operational Intensive
- Too much clutter
- Can't comply to manage multiple containers
- Can't recall the parameters again and again, where it is easy to save configs in a file.
- parameters clutter again and again in every execution, peculiar and relevant for each and every relevant options.
>**basic pod creation using imperative approach by passing commands and parameters in order to create an object/resource in Kubernetes cluster with the help of `kubectl` **

###### Assessing/ Executing/ Working inside a POD 
```
kubectl run -it hexrapp --image=nginx:1.26.2 --restart=Always -- bash
```
gets you inside the POD. cant use `kubectl`, `systemctl` or `service` commands here.
EXECUTE NGINX POD $SHELL TO ENABLE NGINX.
syntax: nginx, -g = global, "" - params
```
nginx -g "daemon off; "
```
**NGINX TURNED ON**

Leave it as a process and open up an another terminal to exec
```
curl <podIP>
```
Prints nginx default page. + can see the request received in the side(logs).
After playing,
SIMPLY YOU CAN DISPOSE IT ALL. 
`ctrl + c`, `ctrl + d` and 
```
kubectl delete po <podname> 
```
> **THIS IS HOW YOU WORK WITH IMPERATIVE APPROACH**. SEE THE PAIN OF DOING ALL OF THESE. 
> **DECLARATIVE APPROACH IS A WISE CHOICE. SIMPLY WRITE IT ALL THAT WHAT YOU WANT TO CREATE, SIMPLY PASS THAT YAML AND VIOLA!**

 ***DECLARATIVE APPROACH = ALIGNS WELL WITH THE ORECHESTRATION CONCEPT***
 
---
####  7.POD creation workflow
###### Pod Creation Workflow -[Doc](obsidian://open?vault=tutorialHell&file=Orchestration%2Fk8engineers.com%2FKubernetes-Deep-Dive%2Foffl-raw-docs%2Fkubernetes%20deepdive%2Fsec3-k8s-pods%2F4.Pod%20Creation%20Workflow.docx)
![[Pasted image 20240827163448.png]]
TILL NOW WE HAVE SEEN CREATING PODS in both declarative and imperative method. **WILL SEE THE WORKFLOW OF A POD CREATION:** (some basic things to know how things works under the hood while we create pods).
**WHAT HAPPENS WHEN WE PASS COMMAND?** 
```
kubectl create/apply -f file.yaml`
```
>**AND HOW CRI CREATES CONTAINERS AND STUFF? AND
>HOW WE GET THE RESPONSE OR ACKNOWLEDGEMENT?**

REFERRING TO THE DIAGRAM:  (WE HAVE ALL THE RELEVANT COMPONENTS THAT EXIST INBETWEEN IN TERMS OF CREATING A POD)
`kubectl`  < - >  `kube-apiserver`  < - >  `etcd`  < - >  `scheduler`  < - > `kubelet`  < - > `containerd`

**DECODING THE FLOWCHART:**
INPUT TO CREATE POD:
1) USER -> `kubectl`  --> `kube-apiserver`
-- **user** passes --> **pod creation request** will be sent via `kubectl` --> to `kube-apiserver`
2) Now, `kube-apiserver` --> will verify with `etcd` 
`etcd`will respond back to `apiserver` . 
-- (**to check and return data whether the resource already exist or not**)
if already exists, spins up the POD!
if the resource doesn't exist, 
3) `etcd` will store the config --> and respond back to `apiserver`
4) `apiserver` --> will pass it to the `scheduler`
-- `scheduler` will analyze based on some metrics and algorithm to picks the suitable node to deploy the POD.
After deciding it, `scheduler` --> to `apiserver`, where that information gets stored in `etcd`
>`etcd` can't talk to `scheduler` , likewise `scheduler` can't talk to `etcd` too. the reason why the `scheduler`  --> passed info to `apiserver`  -x- not the `etcd`
4) after storing the information from `apiserver` to `etcd` and acknowledges back, `apiserver` --> will talk back to `scheduler` as passes the acknowledgement to create `pods` to  any one of the relevant node, says back to `apiserver` back. ``
> all these decision has been done here, ON THE CONTROL PLANE NODE.
5) After getting all that info from `Control Plane`'s `api-server` --> `kubelet` service, 
`kubelet` --> will pass command via `crictl` to `containerd or any cri` creating all the relevant `PODs`
6) after all `PODs` has been created, `containerd or any cri` --> will send back the **acknowledgement** to `kubelet`
7) again, `kubelet` --> will pass that acknowledgement info to `apiserver`
8) `apiserver` --> stores that info too in `etcd`
9) `etcd` --> sends back the acknowledgement to `apisever` 
10) `apiserver` --> will send info to `kubelet` and 
11) `apiserver` --> WILL SEND and we will be RECEIVING THE CONSOLE OUTPUT THAT THE **PODS HAS BEEN CREATED, FAILED OR PENDING**
12) ALL DONE!
Now we can take care working with the rest as 
```
kubectl get pod -o wide -A
kubectl describe pod
```

---
#### 8. POD resource allocation CPU and Memory + Extra parameters
**POD resource allocation CPU and Memory + Extra parameters such as environment variables, resource blocks and more**

NOW, WILL DIG DEEPER INTO MORE PARAMETERS/FACTORS such as
1) allocating resources
 - CPU
 - Memory
 - storage and more
2) Environment variables
3) Resource limiting
4) Volume mounting in brief
5) Other blocks, such as
- Resource block
- port block
![[Pasted image 20240828112459.png]]
```
apiVersion: v1
kind: Pod
metadata:
  name: hexrapp
  labels: 
    app: hexrapp
    env: prod
spec:
  containers:
    - name: hexrapp
      image: nginx
      ports:
        - containerPort: 80
      env:
        - name: DB_USERNAME
          value: "admin"
      resources:
        limits: 
          cpu: "500m"
          memory: "500Mi"
        requests:
          cpu: "200m"
          memory: "200Mi"
      volumeMounts:
        - name: hexrapp
          mountPath: /var/www/html
  volumes:
    - name: hexrapp
      emptyDir: {}
```
![[Pasted image 20240828112413.png]]
**Volume Block Demystification Overview:**
We have just worked on it bluntly. Will have an insight on how this works.
1) Volume of container's - Container Volume
-- which the name, and the `WORKDIR` path has been defined
2) Volume of Pod's - Shared Pod Volume
**Referring to the diagram:**
Container volume - named here as mounts that the name itself says that.
```
     volumeMounts:
      - name: hexr-vol
        mountPath: /var/wwww/html
```
Volume mount path that which of the directory that you want to expose.
-> Inside the container as we speak the CONTAINER VOLUME, we are performing mounting a `WORKDIR` folder which is exposed that to be mounted with
-> The Shared storage - Pod Volume. Mounted to what volume? 
   ```
volumes: 
    - name: hexr-vol
      emptyDir: {}
```
Volume of Pod's = Shared Storage - Pod Volume.  
>TO MAKE THE CONTAINER WORK WITH THE POD SHARED VOLUME, BOTH OF THE VOLUME NAMES MUST BE IDENTICAL. SAME STRING
> 
**Note: IN ORDER TO ASSESS THE CONTAINER VOLUME ON THE POSHARED VOULME TOO, IT IS MUST THAT *NAME* OF BOTH OF THE VOLUME  SHOULD BE IDENTICAL TO BE MAPPED** 
++
One more point to be noted that, in the 
POD Volume: emptyDir{} -> means that, wherever the pod gets launched on a particular HOST MACHINE and NODE, All these data must gets stored in some `WORKDIR` . For a Pod running in a node, the data get stored in a location - `kubelet`, the default working directory of a `kubelet` is `/var/lib/kubelet/`. Here all the POD related data gets stored here.  -> which is mapped as `emptyDir{}` here.
**WILL SEE MORE IN BRIEF WHEN WE DO THINGS HANDS-ON**.

BACK TO TOPIC! 
###### APPLYING THE SAME YAML TO UNDERSTAND POD RESOURCE ALLOCATION IN BRIEF.
RECALLING THE PERVIOUS `hexrapp.yaml` POD MANIFEST TO CONTNUE THE SAME.
```
apiVersion: v1
kind: Pod
metadata:
  name: hexrapp
  labels: 
    app: hexrapp
    env: prod
spec:
  containers:
    - name: hexrapp
      image: nginx
      ports:
        - containerPort: 80
      resources:
        limits: 
          cpu: "500m"
          memory: "500Mi"
        requests:
          cpu: "200m"
          memory: "200Mi"
      volumeMounts:
        - name: hexrapp
          mountPath: /var/www/html
  volumes:
    - name: hexrapp
      emptyDir: {}
```
WE USED THIS MANIFEST TO LAUNCH A POD with One single container.
*Lets reuse the same and add parameters, sections, blocks, environment variables and resource allocation on top of it* How?
**All these params applied to Container section**
1) **Port block**
```
ports: #optional, proceeds with the default
 - name: hexr-port #naming the port block
   containerPort: 80 #port number specifiacatino
   protocol: TCP #none changes if not declared, goes default
```
2) **Environment Variables:**  
```
env:
  - name: DB_USER #name of the env var
    value: "admin" #value to that var
#creds to login after a successfull container launch, print env
```
3) **Resource Block:** by default, no resource limit has been defined. Amount of CPU and Memory to be utilized min or max (which is taken care of `cgroup` under the hood) Limiting the resource for the container. 
Why? One set of application must not get affected due to the over utilization of this app.
**Guide to resource limit:**
**limits**: # container should not use more than this, limit **(MAX LIMIT)**
1) cpu: "500m" # or 0.5  or 50% # 1core = 1000m(icrons) - 100%util 
2) memory: "500Mi" # Ki, Mi, Gi, Ti 
**requests**:  # how much resources to be allocated, in-demand **(MIN LIMIT)** required
1)  cpu: "200m" # core allocated
2) memory: "200Mi"  # memory allocated
```
resources:
  limits: 
    cpu: "500m"
    memory: "500Mi"
  requests:
    cpu: "200m"
    memory: "200Mi"
```
4) **volumes:**
> HOST: `emptyDir: {}`<-mounted-> `/var/www/html` - WORKDIR of the container

**PASS THE MANIFEST TO SEE THE RESULTS.**
```
kubectl create -f app.yaml
kubectl get po -o wide
curl <podIP>
```
CHECK ALL THE METRICS PASSED,

**TO DO THAT, NEED TO LOG INTO THE CONTAINER. HOW TO LOG INTO THE CONTAINER**
```
kubectl exec -it hexrapp -c <containername(opt)> -- /bin/bash
```
VOILA! LOGGED IN! -> base machine of nginx -> `debian`  - basic cmds can be ran.
1) **environment variables:**
```
env
printenv
```
 All the ENV VAR can be shown. can find `DB_USER=admin`
2) **resources:**
```
kubectl describe no <nodenameorskip>
kubectl describe po <ponameorskip>
```
shows the specs and can verify the resources given there
3) **volumes:**
```
kubectl get po <containername> -o wide
sudo ls -l /var/lib/kubelet/pods
```
will show, ALL THE VOLUMES OF EACH OF THE PODS. **BUT, What we have is name, but here everything is in IDs. HOW TO NAVIGATE THIS**? IDs - UID
```
kubectl get po -o yaml
```
all these data came from `etcd` FYI.  Here, will filter out the volume as we need it.
```
kubectl get po -o yaml | grep "uid"
```
you get the `uid` of the pod.
```
sudo ls -l /var/lib/kubelet/pods/"uid"
```
THIS IS THE VOLUME MOUNT BETWEEN THE `container` and the `KUBELET`. Inside that `uid`,
```
kubernetes.io-empty-dir
kubernetes.io-projected
```
```
cd kubernetes.io-empty-dir
ls
<volume's-name>
```
>**TLDR**: SINCE BOTH ARE MOUNTED BIDIRECTIONAL, WHATEVER DATA WILL POPS UP ON, BOTH
>- /var/www/html - container
>- /var/lib/kubelet/pods/uid/kubernetes.io-empty-dir/volume-name/ -system

WILL SEE MORE ON THIS IN PRACTICE.

---
####  9. POD multi-container with shared volume
###### Multi-container pod with Shared Volume - [Doc](obsidian://open?vault=tutorialHell&file=Orchestration%2Fk8engineers.com%2FKubernetes-Deep-Dive%2Foffl-raw-docs%2Fkubernetes%20deepdive%2Fsec3-k8s-pods%2F5.Multicontainer%20pod%20with%20Shared%20Volume.docx)

SPINNING MULTI-CONTAINER POD WITH SHARED VOLUME. SAME AS BEFORE SPINNING A SINGLE CONTAINER IN A POD BUT HERE THERE ARE MULTIPLE BUT XTRA VOLUME FOLDERS.

Will see a pictorial view:
![[Pasted image 20240830105402.png]]
Here, We have a node. Inside that,
Node --> Pod with an IP (with two containers)--> nginx + alpine } -> 
> If we got to access the container, no container IP can be resolved since each of the container can be communicated, only via the POD IP alone. Inside that IP, both can talk to each other only on its own `locahost`s or `127.0.0.1`. 
==req -> PodIP -> nginxport80 + alpine22 -> container's data lands on its own volumes -> which will be stored also in the `volumemount` under systems.==

e.g.: here, we have a pod with two containers. one nginx which serves data, one alpine which writes data. each `workdir` gets mounted on system's volume mount.
- nginx port80-> /usr/share/nginx/html -> /var/lib/kubelet/podUID/volumes
- alpine port80-> /var/tmp/`whateverdata` -> /var/lib/kubelet/podUID/volumes
=> based on this logic, will deploy an application works this way which helps and used in many cases. POD with multi-container.

**Moral: A single storage volume can be stored and serve data to multiple containers. Just like other infra models such as VMs and On-prems. Same logic can be applied to multi-container pod**

Will see that in practice. 
`multicontainer-pod.yaml` 
> Note: without defining apiVersion, rest of the parameters such as `apiVersion`, `kind`, `metadata` and `spec` can't be defined and wont be passed to K8s. 
```
apiVersion: v1
kind: Pod
metadata:
  name: hexrapp
  labels:
    app: hexrapp
    type: web
spec:
  containers:
  - name: hexrapp-writer
    image: alpine
    command: ["/bin/sh"]
    args: ["-c", "while true; do date >> /var/tmp/index.html; sleep 10; done"]
    volumeMounts:
    - name: hexrapp-shared-volume
      mountPath: /var/tmp
  - name: hexrapp-server
    image: nginx
    ports:
    - containerPort: 80
    volumeMounts:
    - name: hexrapp-shared-volume
      mountPath: /usr/share/nginx/html
  volumes:                  
  - name: hexrapp-shared-volume
    emptyDir: {}
```
>   containers:
	   - ...
	    command: ["/bin/sh", "-c", "while true; do sleep 30; done"]
	    args: ["-c", "while true; do echo $(date) >> /var/tmp/hexrapp.log; sleep 1; done"]

> Here in this image, if i have to perform any actions such as passing cmds, running processes, functions of jobs. it is possible using `command` param under `spec`. 
> 
> This can be a single or a combo of commands. It is more like **`ENTRYPOINT`** in Dockerfile, if we compare the same with docker. As same, there is an another named **`args`** param which works similar to **`CMD`**.

`command: ["/bin/sh", "-c", "while true; do sleep 30; done"]`
`args: ["-c", "while true; do date >> /var/tmp/index.html; sleep 10; done"]`

**Decoding this param:**  
**`command`**: helps us access the shell of that image. **/bin/bash**
**`args`**: runs cmd's inside that image since we accessed the shell. in this, we are looking forward to run a command every ten seconds. to apply that logic,
**running an infinity while loop running a command with setting up a timer.**  
    **`args`**: ["-c", "while true; do date >> /var/tmp/index.html; sleep 10; done"]
1. `"-c", `= indicates that we are passing cmds
2. `"while true` = "cmd of while loop starting and declare the same where it is true in condition. 
3. `;`is followed by next steps.
4. `do date` = do  - passes command that whatever we declare. here we pass date. 
5. `>> /var/tmp/index.html;`  = passing that to where?, >> - towards a file or dir. 
6. `sleep 10;` - sleep command helps pause running a command for the time we define. here 10 seconds. every 10secs the command gets executed.
7. `done"` - closing that loop following by.

Now, lets check the rest.
```
kubectl get po -o wide #checking pod status
curl ip #check output, prints date and time every 10secs
kubectl describe po podname #shows all the details
```
while describing the pod:
`kubectl describe po podname`
shows all the container details, there in 









---
####  10. Handling containers in POD using crictl(restart container POD)
###### Handling containers in Pod using crictl - [Doc](obsidian://open?vault=tutorialHell&file=Orchestration%2Fk8engineers.com%2FKubernetes-Deep-Dive%2Foffl-raw-docs%2Fkubernetes%20deepdive%2Fsec3-k8s-pods%2F6.Handling%20containers%20in%20Pod%20using%20crictl_.docx)



---
#### 11. Access POD application outside cluster(hostPort)
###### Access pod application outside cluster(hostport) - [Doc](obsidian://open?vault=tutorialHell&file=Orchestration%2Fk8engineers.com%2FKubernetes-Deep-Dive%2Foffl-raw-docs%2Fkubernetes%20deepdive%2Fsec3-k8s-pods%2F7.Access%20pod%20application%20outside%20cluster(hostport).docx)



---
####  12. POD initContainers Introduction
###### Pod initContainers - [Doc](obsidian://open?vault=tutorialHell&file=Orchestration%2Fk8engineers.com%2FKubernetes-Deep-Dive%2Foffl-raw-docs%2Fkubernetes%20deepdive%2Fsec3-k8s-pods%2F8.Pod%20initContainers.docx)



---

####  13. POD initContainers



---
####  14. POD Lifecycle: restart policy
###### Pod Lifecycle Restart Policy - [Doc](obsidian://open?vault=tutorialHell&file=Orchestration%2Fk8engineers.com%2FKubernetes-Deep-Dive%2Foffl-raw-docs%2Fkubernetes%20deepdive%2Fsec3-k8s-pods%2F9.Pod%20Lifecycle%20Restart%20Policy.docx)



---
####  15. Static POD (controlled by Kubelet)
###### Static Pod (Controlled by kubelet) - [Doc](obsidian://open?vault=tutorialHell&file=Orchestration%2Fk8engineers.com%2FKubernetes-Deep-Dive%2Foffl-raw-docs%2Fkubernetes%20deepdive%2Fsec3-k8s-pods%2F10.Static%20Pod%20(Controlled%20by%20kubelet).docx)



---
####  16.Challenges of standalone POD applications
###### Challenges of Standalone Pod - [Doc](obsidian://open?vault=tutorialHell&file=Orchestration%2Fk8engineers.com%2FKubernetes-Deep-Dive%2Foffl-raw-docs%2Fkubernetes%20deepdive%2Fsec3-k8s-pods%2F11.Challenges%20of%20Standalone%20Pod.docx)



---