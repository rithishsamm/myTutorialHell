---

---
***
### Section 3: K8s Pods
#### 1.  Kubernetes Objects overview
###### Kubernetes Overview & Pod- [Doc](obsidian://open?vault=tutorialHell&file=Orchestration%2Fk8engineers.com%2FKubernetes-Deep-Dive%2Foffl-raw-docs%2Fkubernetes%20deepdive%2Fsec3-k8s-pods%2F1.Kubernetes%20Overview%20%26%20Pod%20Introduction.docx)
#### 2. POD overview
#### 3.Integrating VS code with k8s cluster
######  Integrating VS code with K8s Cluster - [Doc](obsidian://open?vault=tutorialHell&file=Orchestration%2Fk8engineers.com%2FKubernetes-Deep-Dive%2Foffl-raw-docs%2Fkubernetes%20deepdive%2Fsec3-k8s-pods%2F2.Integrating%20VS%20code%20with%20K8s%20Cluster.pdf)
#### 4.Overview on k8s objects creation using imperative and declarative approach
#### 5.POD creation using Declarative approach
#### 6.POD creation using Imperative approach
###### Pods creation using imperative approach - [Doc](obsidian://open?vault=tutorialHell&file=Orchestration%2Fk8engineers.com%2FKubernetes-Deep-Dive%2Foffl-raw-docs%2Fkubernetes%20deepdive%2Fsec3-k8s-pods%2F3.Pods%20creation%20using%20imperative%20approach.docx)
####  7.POD creation workflow
###### Pod Creation Workflow -[Doc](obsidian://open?vault=tutorialHell&file=Orchestration%2Fk8engineers.com%2FKubernetes-Deep-Dive%2Foffl-raw-docs%2Fkubernetes%20deepdive%2Fsec3-k8s-pods%2F4.Pod%20Creation%20Workflow.docx)
#### 8. POD resource allocation CPU and Memory
####  9.POD multi-container with shared volume
###### Multi-container pod with Shared Volume - [Doc](obsidian://open?vault=tutorialHell&file=Orchestration%2Fk8engineers.com%2FKubernetes-Deep-Dive%2Foffl-raw-docs%2Fkubernetes%20deepdive%2Fsec3-k8s-pods%2F5.Multicontainer%20pod%20with%20Shared%20Volume.docx)
####  10.Handling containers in POD using crictl(restart container POD)
###### Handling containers in Pod using crictl - [Doc](obsidian://open?vault=tutorialHell&file=Orchestration%2Fk8engineers.com%2FKubernetes-Deep-Dive%2Foffl-raw-docs%2Fkubernetes%20deepdive%2Fsec3-k8s-pods%2F6.Handling%20containers%20in%20Pod%20using%20crictl_.docx)
#### 11.Access POD application outside cluster(hostPort)
###### Access pod application outside cluster(hostport) - [Doc](obsidian://open?vault=tutorialHell&file=Orchestration%2Fk8engineers.com%2FKubernetes-Deep-Dive%2Foffl-raw-docs%2Fkubernetes%20deepdive%2Fsec3-k8s-pods%2F7.Access%20pod%20application%20outside%20cluster(hostport).docx)
####  12.POD initContainers Introduction
###### Pod initContainers - [Doc](obsidian://open?vault=tutorialHell&file=Orchestration%2Fk8engineers.com%2FKubernetes-Deep-Dive%2Foffl-raw-docs%2Fkubernetes%20deepdive%2Fsec3-k8s-pods%2F8.Pod%20initContainers.docx)
####  13.POD initContainers
####  14.POD Lifecycle: restart policy
###### Pod Lifecycle Restart Policy - [Doc](obsidian://open?vault=tutorialHell&file=Orchestration%2Fk8engineers.com%2FKubernetes-Deep-Dive%2Foffl-raw-docs%2Fkubernetes%20deepdive%2Fsec3-k8s-pods%2F9.Pod%20Lifecycle%20Restart%20Policy.docx)
####  15.Static POD (controlled by Kubelet)
###### Static Pod (Controlled by kubelet) - [Doc](obsidian://open?vault=tutorialHell&file=Orchestration%2Fk8engineers.com%2FKubernetes-Deep-Dive%2Foffl-raw-docs%2Fkubernetes%20deepdive%2Fsec3-k8s-pods%2F10.Static%20Pod%20(Controlled%20by%20kubelet).docx)
####  16.Challenges of standalone POD applications
###### Challenges of Standalone Pod - [Doc](obsidian://open?vault=tutorialHell&file=Orchestration%2Fk8engineers.com%2FKubernetes-Deep-Dive%2Foffl-raw-docs%2Fkubernetes%20deepdive%2Fsec3-k8s-pods%2F11.Challenges%20of%20Standalone%20Pod.docx)
---

#### 1.  Kubernetes Objects overview
###### Kubernetes Overview & Pod- [Doc](obsidian://open?vault=tutorialHell&file=Orchestration%2Fk8engineers.com%2FKubernetes-Deep-Dive%2Foffl-raw-docs%2Fkubernetes%20deepdive%2Fsec3-k8s-pods%2F1.Kubernetes%20Overview%20%26%20Pod%20Introduction.docx)

Kubernetes Objects overview:
**Terminologies**:
> **Objects**: also previously has been named or called as service, components. Primarily, it is called as Kubernetes objects. 
>  IF I WANT TO CREATE ANYTHING ON KUBERNETES CLUSTER, CALLED AS OBJECTS. 

Will see what are all the objects that are available in Kubernetes Cluster. Objects can also be named as **workloads**.  

> ultimately we're using Kubernetes Cluster to deploy applications. While deploying, we use objects to make things happen.


E.g.:  
#### Approaches to deploy application on K8s Cluster,
using objects or workloads such as pods, config maps, secrets, network policy's, and other relevant resources -> a.k.a objects. 

Eg: 
Workloads  -->  Pod, Replication Controller/ReplicaSet, Deployment Set, DaemonSet, Stateful sets and Jobs.
These are named as Workloads because, you are deploying application on Kubernetes Cluster are of simply these objects. Simply a Pod or the rest of the object.

You have to create any one of them to deploy any one of the application on a kubernetes cluster. = Objects

**Approaches to deploy applications on kubernetes**

| Objects                 | Workloads                                |
| ----------------------- | ---------------------------------------- |
| All Workloads           | Pods                                     |
| Service                 | Replication Controller / Replication Set |
| Configmaps/Secrets      | Deployment                               |
| Ingress                 | Daemonset                                |
| Network Policy          | Statefulset                              |
| PV, PVC & Storage class | Jobs and Cron Jobs                       |
| Many More               |                                          |
Use any one of them to create your application. Each workloads offers different set of features - **workloads**

**Objects**: Except all the workloads, whatever that gets created are know as objects, such as 
- All the workloads itself, 
- Service, (people were getting confused about this service and the object as service)
- ConfigMap/ Secrets,
- Ingress, 
- Network policy, 
- PV, PVC and Storage Classes, etc..  
> **previously, these has been called out as SERVICE - Legacy**

So! all of these here are all **OBJECTS**.
> THIS IS EXPLAINED HERE TO GET HABITUATED AND CLARITY ON THESE OBJECTS AND WORKLOADS.

>**Now, WILL SEE MORE ON PODS**
Will see answers for,
- what
- why (creating one in the first place)
- how (to use)
- benefits of a pod (rather than container tools)
- and more...
---
#### 2. POD overview
POD overview:
After getting know all about the Kubernetes Architecture and setting up, we gotta see at first is PODS.

Why? 
PODS: Whatever you seek to deploy some workload on a Kubernetes Cluster, it going to end up in a POD. Any workload, any objects, any components, will be all in a POD. That Pod creates containers. ****
> SO IT IS ESSNTIAL AND NECCESSARY TO LEARN MORE ABOUT **PODS** IN DEEP. if this is not understandable, you'll never become devops guy deploying applications on a Kubernetes Cluster.

 Will see Pods in detail, 
> [!NOTE] **What is a POD**
> -  Pod **creates a logical layer** 
> to group 1 or more containers =  to have **common network** + **shared storage.**
> - Pod has a unique IP address assigned from `--pod-network-cidr=10.244.0.0/16`
> - Containers inside those POD talks to each other on `localhost` domain or `127.0.0.1`, since container share the same network stack. 
> - Containers share data inside POD.
> - In general, we need to create containers inside POD 
> which are dependent, not different application. 
1. why a logical layer to create one or more containers? to have common network and storage. will see that in brief.
2. here, this argument in specific, `--pod-network-cidr=10.244.0.0/16`, when we executed `kubeadm init` command, to create single, multi or HA nodes for kubernetes setup. here `--pod-network-cidr=ip/block`. **If i want to reach the application inside the POD, have to use POD IP Address not the container IP Address.**
The Pods IP Address is an ephemeral IP ADDRESS - Not fixed to the pod. If Pod gets removed so the IP Address too. If a new one gets deployed, it won't be or get the same even when you deploy the exact same app. Just like DHCP where IP gets assigned in random. Here, it does the same but under a CIDR Block. but still that isn't fixed.
3. Containers will talk to each other mostly on loopback interface -  `localhost` `127.0.0.1`. no matter the machine such as bare metal, vm, cloud instance or containers. - Loop back interface. **CONTAINERS INSIDE THE POD WILL TALK VIA `localhost` or `127.0.0.1` as you call it .**
WHY? e.g.: Inside a POD you have multiple containers for an application or any workload, ALL OF THESE SHARE THE SAME NAMESPACE. ESPECIALLY NETWORK NAMESPACE. SO, IF ==YOU WANT TO COMMUNICATE TO AN APPLICATION OUTSIDE THE POD, WE NEED **POD's IP ADDRESS**, *NOT CONTAINERS*. INSIDE THAT POD, CONTAINERS WILL TALK TO EACH OTHER ON `localhost` or `127.0.0.1`.==
4. Incase if we have multiple containers inside a POD, I might want to replicate data among all the containers inside the pod. Here comes the **SHARED STORAGE**. (As we discussed in 1st Point). To achieve shared storage, have to use the concept of POD. 
-> The reason why we use PODs not plain containers is this. Common Network and Shared storage.
5. One app in a POD which can be either in a single or multi-containers, not a whole different application. Containers that are relevant to the application is recommended. Though, you can but IT WILL BE CLUNKY if it is all different. 
###### Architecture of a POD:
1) **Common Network:**
![[Pasted image 20240821105001.png]]
POD -> Common network namespace -> Containers under that namespace (sharing same network which can talk bidirectional via `localhost`, not matter how many containers in a pod, shares the same namespace). 
WHY? Benefits of Common network. If need to communicate outside the POD, can use **POD IP Address.** 
- Communication within or between containers - `localhost` x `127.0.0.0`
- Communication outside containers - `Pod IP Address`


2) **Shared Storage:**
![[Pasted image 20240821105656.png]]
POD -> Pod network namespace -> Containers under that namespace -> Shares same volume which is a Shared Storage. 
Shared Storage - Containers that are sitting inside the POD shares data between the containers in common. 
WHY? Benefits of Shared Storage. Data that comes to that POD which can be shared in common between containers inside that POD. 
eg: One container that hold or contains some data within, It will be housed on or under that common volume which is shared storage. Which that can be used by another container too. Sharing User credentials for IAM between containers services for a common application for example.

==Having Common Network and Shared storage for one or more containers in a namespace, we can achieve the concept of POD. The reason for using PODs not plain containers in Kubernetes is this.

>**Pod is simply a logical layer. Programmatically, It is simply having one container or grouping multiple container together to have a common network and shared storage. ==**

---
#### 3.Integrating VS code with k8s cluster
######  Integrating VS code with K8s Cluster - [Doc](obsidian://open?vault=tutorialHell&file=Orchestration%2Fk8engineers.com%2FKubernetes-Deep-Dive%2Foffl-raw-docs%2Fkubernetes%20deepdive%2Fsec3-k8s-pods%2F2.Integrating%20VS%20code%20with%20K8s%20Cluster.pdf)
SETUP:
1) 2 VMs,
- One Control Plane - 2vCPU, 2GB Ram, 12GB Storage
- One worker Node - same
2) Visual Studio Code
3) Install Remote-SSH Extension
**Objective**: WANT TO WRITE KUBERNETES YAML FILES FROM HERE FOR ALL THE ESSENTIAL SETUP ON THESE VIRTUAL MACHINES.

Taking VM in remote and making YAML Manifests instead of inside the machine since it is only CLI. So, Write in local , Execute in remote

Visual Studio Code -> SSH Icon -> Open SSH Configuration File -> open terminal and land in `c:\users\username\.ssh\` 
```
ssh-keygen.exe
<enter>empty
y
<enter>empty - no passphrase
```
Key got generated. Copy public key fingerprint to that machine. 
```
ssh-copy-key username@ip
```
> DOING **PASSWORD-LESS KEY-BASED AUTH** instead of **PASSWORD BASED AUTH**

SSH -> Connect to Host -> add hosts `username@ip` -> add config -> Connect back to host. Voila!

**SMOOTH AS BUTTER!**

---
#### 4. Overview on k8s objects creation using imperative and declarative approach

> [!NOTE] **TO ACCESS `KUBECTL` as a normal user:**
> -> ./kube/config -> **KUBE CONFIG FILE** TO ACCESS K8S
> ```
> sudo -i
> mkdir /home/username/.kube
> cp .kube/config /home/username/.kube/
> chown -R username: /home/username/.kube
> ```
> and logout as root
> and switch to normal user
> and check `kubectl`
> ```
> kubectl get no
> kubectl get po -o wide -A
> ```
###### Overview on k8s objects creation using imperative and declarative approach

Declarative Approach - yaml manifest
Imperative - All by commands

**APPROCHES TO CREATE OBJECTS ON K8s**: there are two approaches,
`kubectl` : 
e.g.: executing `kubectl`, need to create some objects on our Kubernetes Cluster. Not just pod by deployment, services, state sets and more. HOW TO CREATE SUCH OBJECTS?
- **Declarative approach** -  Declaring the same by giving all the required objects by writing it in a `YAML` File.
This YAML will be given via `kubectl`to the `api-server` that takes cares of the rest by receiving it, doing schema validation and passed information to the `etcd` that talks to  its relevant components in order to create all the same which we've declared in the manifests.
- **Imperative approach -**  Creating resources from the CLI itself. giving it all in our CLI as commands to create objects. No YAML or JSON files excepts systems, services and components needs and manifests. 
eg: If using 
- docker with cli - imperative approach
- writing docker-compose file in YAML - Declarative approach.
Here in `kubectl`, can use both of these approaches. 

###### Understanding Declarative Approach: (!IMP - using irl)
![[Pasted image 20240821182425.png]]
```YAML
apiVersion: v1
kind: Pod
metadata:
 name: nginx-demo
 labels:
  app: nginx
  env: prod
spec:
 containers:
   -name: nginx
    image: nginx:lts #latesttag
   ports:
   - name: nginx-port
     containerPort: 80
     protocol: TCP
   env:
    -name: USERNAME
     value: "admin"
   resources:
    limits:
      cpu: "0.2"
      memory: "500Mi" #Ki,Mi,Gi,Ti
     requests:
      cpu: "0.2"
      memory: "200Mi"
    volumeMounts:
     -name: nginx-data
      mountPath: /var/www/html
volumes:
 - name: nginx-data
   emptyDir: {}
```
1. `apiVersion`: ==version of the object. max v3==, *version of what are all the objects that are going to get created.* (will see more on the same by how to confirm and verify the same)
2. `kind`: Object/ Resource name -> ==Kind of object== ,*what object that i want to create*.
3. `metadata`:  ==metadata to the object for labeling each, useful for filtering== *name that i want to assign to my POD*, *adding additional information by labeling the object for further identification and filtration by specifying relevant key word.* volumes, containers, init-containers and more. Specifying how many containers i want to create, all the relevant parameters to that containers, - naming it, image, assigning port, passing env variables, resources specs, volume mounting
- name: nginx-demo
- labels: (***app***: nginx, ***env***: prod)
5. `specs`: ==container specifications to be handles by the POD== *specifying what are all the things that i want to create*, 
- containers:
-  -- name: nginx
-  - image: nginx:lts ==tag==
-  - ports (***-name***: nginx-port, ***containerPort***: 80, ***protocol***: TCP) ==container ports section== 
- - env (***-name***: db_username , ***value***: "admin") ==container environment variables== 
- - resources: (***limits***: ***cpu***: "1" ***memory***: "500Mi", ***requests***: ***cpu***: "0.5" ***memory***: "200Mi" ) ==container resource allocation== 
- - volumeMounts: (***-name***: nginx-data, ***mountPath***: /var/www/html) ==container volume mounting point== 
- `volumes`: ==shared volume for POD Containers== 
- --name: nginx-data
- emptyDir:  {}
ALL `compose.yaml` CONTENT.

###### Understanding Imperative Approach: (opt)
Imperative approach is all the same by giving commands. creating each by imperative will be much of a pain So, Why Imperative even exist. 
Helps in,
- smaller operations
- frequent object creation
- patchwork
- operation intensive works
- depends on the circumstances and condition, usage may vary.
BUT IN MOST OF OUR TIME, **==DECLARATIVE APPROACH IS STAGNENT==**. REST OF THE CONDITIONS, imperative.
---
#### 5.POD creation using Declarative approach
Creating YAML Manifests in the VM. Declarative approach.

> **Pod is just a logical layer, `containers` are the same as we use with common network and shared storage**

Before moving forward, will check on these prerequisites,
1) become a `root` user, to use `kubectl` in ease.
```
sudo -i
```
2) setup up all the kubernetes base setup. 
- all the updates
- reboot after initial spin up
- network configuration for `containerd`, `kubernetes-cri` 
- apply the configuration
- installing `containerd` 
- docker's `gpg`, `apt resources`, `containerd` installation and config, `restart` service
- `crictl` config
- installing `kubeadm`, `kubectl`, `kubelet`
- `gpg` keys and `apt` Dir for them
- `enable` and `hold` it all.
- `kubeadm init` for Control plane, `kubeadm join` for Worker node
-setup all done from here until you get something like this.

```
kubeadm init --apiserver-advertise-address=192.168.0.219 --cri-socket=/run/containerd/containerd.sock --pod-network-cidr=10.244.0.0/16
```

Your Kubernetes control-plane has initialized successfully!
To start using your cluster, you need to run the following as a regular user:
```
kubeadm init --apiserver-advertise-address=192.168.0.219 --cri-socket=/run/containerd/containerd.sock --pod-network-cidr=10.244.0.0/16
```
```
mkdir -p $HOME/.kube
sudo cp -i /etc/kubernetes/admin.conf $HOME/.kube/config
sudo chown $(id -u):$(id -g) $HOME/.kube/config
```
Alternatively, if you are the root user, you can run:
```
export KUBECONFIG=/etc/kubernetes/admin.config
```
You should now deploy a pod network to the cluster.
Run 
```
kubeadm apply -f <pod-network>.yaml
```
with one of the options listed at: https://kubernetes.io/docs/concepts/cluster-administration/addons/
Then you can join any number of worker nodes by running the following on each as root:
```
kubeadm join 192.168.0.219:6443 --token xljegf.3d6nt61xmqk33qns \ --discovery-token-ca-cert-hash sha256:968164ba40787980ba10adf93b13abe8adf22e6a10b65a3fad0fdc574941ce7b
```
if any error in the process, 
- clear cache
- rm /.kube/config and respawn it
- ensure disabling `swap` memory
- make sure that `kubelet` is running, enable if not.
- also ensure, kube `cni` configured properly
- end, `kubeadm reset`.
```shell
sudo systemctl enable --now kubelet
```
```shell
kubectl create -f https://raw.githubusercontent.com/projectcalico/calico/v3.28.0/manifests/tigera-operator.yaml

wget https://raw.githubusercontent.com/projectcalico/calico/v3.28.0/manifests/custom-resources.yaml 

nano custom-resources.yaml ##change-cidr
```
```
kubectl apply -f custom-resources.yaml 
```
```
mkdir -p $HOME/.kube
sudo cp -i /etc/kubernetes/admin.conf $HOME/.kube/config
sudo chown $(id -u):$(id -g) $HOME/.kube/config
```
> `kubeadm init` again

**MOVING FORWARD WITH *DECLARATIVE APPROACH* BY USING A `YAML` FILE.** 

vscode -> open folder -> Home folder + Open the same in terminal too.
> STORING ALL THE YAML MANIFESTs IN THE HOME DIRECTORY, now the DECLARATIVE APPROACH of creating PODS.

```
cd ~
mkdir manifests
cd manifests
mkdir PODs
cd PODs
touch hexrapp.yaml
```
ALL IN `vs-code`

says: 
- NAME  - system default name gets applied
- STATUS  - Ready or not-ready
- ROLES - control plane or worker node
- AGE - days of creation
- VERSION - version
- INTERNAL-IP - IP that we configured
- EXTERNAL-IP - will be configured after worker gets joined
- OS-IMAGE - Ubuntu or any image
- KERNEL-VERSION - Kernel's version 
- CONTAINER-RUNTIME - cri used in k8s, containerd.docker or whatever runtimes.
##### Objective:
> **Here in this cluster, I want to create a POD using declarative approach.**
THE OBJECTIVE NOW HERE IS TO CREATE PODs IN THE NODE.

###### To-do: 
- Create a `yaml` file
- where? ~/manifests/pod/ -> manage manifests files in a separate directory 
- touch `app.yaml`
- also store the file in root too. 
- sudo -i
```
mkdir /home/username/.kube
cp .kube/config /home/username/.kube
chown -R testuser: /home/testuser/.kube
exit #root
```
now execute kubectl commands.
```
kubectl get no -o wide
```
> NOW, THE USER CAN ALSO ABLE TO GET THE WORK DONE BY EXECUTING `kubectl` COMMANDS with the help of that config file here and there!

###### Parameters:
Here, What to use for POD creation? check the same: Writing the same on a `YAML` file -> pass it to `kubectl` -> that to the `kube-api-server`

**before all of these. will see in advance about api-versions and api-resources for better understanding about what are we going to work with:**

###### API versions vs API Resources:
- `api-resources` - **resources available in the k8s to use.**
are the resources that are available and can be used in the Kubernetes Environment. 
-  `apiversions` - **versions of the resources to be able to use in k8s**
are the versions of resources that are can be used in the kubernetes environment to make it wor.
 So, while referring to use resources and the versions for the K8s environment. Here is the way.
###### Context:
###### 1. `api-versions
> check it all by using `kubectl api-versions` command to declare relevant versions to the configs or objects.
THESE ARE ALL THE API-VERSIONS TO USE WHEN CREATING OBJECTS ON K8s CLUSTERS. NO MATTER THE CLUSTER or SYSTEMS anything such as AWS, Azure, GCP, and more.
```
kubectl api-versions
```
```
admissionregistration.k8s.io/v1
apiextensions.k8s.io/v1
apiregistration.k8s.io/v1
apps/v1
authentication.k8s.io/v1
authorization.k8s.io/v1
autoscaling/v1
autoscaling/v2
batch/v1
certificates.k8s.io/v1
coordination.k8s.io/v1
crd.projectcalico.org/v1
discovery.k8s.io/v1
events.k8s.io/v1
flowcontrol.apiserver.k8s.io/v1
flowcontrol.apiserver.k8s.io/v1beta3
networking.k8s.io/v1
node.k8s.io/v1
operator.tigera.io/v1
policy/v1
projectcalico.org/v3
rbac.authorization.k8s.io/v1
scheduling.k8s.io/v1
storage.k8s.io/v1
v1
```
shows all the available `api-versions` , might be for each component. THESE ARE ALL THE APIs TO USE TO CREATE **OBJECTS** IN KUBEs **CLUSTER**. No matter the platform whether it is on-prem, local, AWS, azure or any other managed service as a matter of fact. ALL ARE SAME IN ALL PLATFORMS. 
also,

###### 2. `api-resources
```
kubectl api-resources 
```
```
kubectl api-resources | grep "resource"
```
and to filter out and find what version to enter into manifest. comes in handy

| NAME                                                                                                                     | SHORTNAMES                                    | APIVERSION                      | NAMESPACED | KIND                                 |
| ------------------------------------------------------------------------------------------------------------------------ | --------------------------------------------- | ------------------------------- | ---------- | ------------------------------------ |
| bindings                                                                                                                 |                                               | v1                              | true       | Binding                              |
| componentstatuses                                                                                                        | cs                                            | v1                              | false      | ComponentStatus                      |
| configmaps                                                                                                               | cm                                            | v1                              | true       | ConfigMap                            |
| endpoints                                                                                                                | ep                                            | v1                              | true       | EndPoints                            |
| events                                                                                                                   | ev                                            | v1                              | true       | Event                                |
| limitranges                                                                                                              | limits                                        | v1                              | true       | LimitRange                           |
| namespaces                                                                                                               | ns                                            | v1                              | false      | Namespace                            |
| nodes                                                                                                                    | no                                            | v1                              | false      | Node                                 |
| persistentvolumeclaims                                                                                                   | pvc                                           | v1                              | true       | PersistentVolumeClaim                |
| persistentvolumes                                                                                                        | pv                                            | v1                              | false      | PersistentVolume                     |
| pods                                                                                                                     | po                                            | v1                              | true       | Pod                                  |
| podtemplates                                                                                                             |                                               | v1                              | true       | PodTemplate                          |
| replicationcontrollers                                                                                                   | rc                                            | v1                              | true       | ReplicationController                |
| resourcequotas                                                                                                           | quota                                         | v1                              | true       | ResourceQuota                        |
| secrets                                                                                                                  |                                               | v1                              | true       | Secret                               |
| serviceaccounts                                                                                                          | sa                                            | v1                              | true       | ServiceAccount                       |
| services                                                                                                                 | svc                                           | v1                              | true       | Service                              |
| mutatingwebhookconfigurations                                                                                            |                                               | admissionregistration.k8s.io/v1 | false      | MutatingWebhookConfiguration         |
| validatingadmissionpolicies                                                                                              |                                               | admissionregistration.k8s.io/v1 | false      | ValidatingAdmissionPolicy            |
| validatingadmissionpolicybindings                                                                                        |                                               | admissionregistration.k8s.io/v1 | false      | ValidatingAdmissionPolicyBinding     |
| validatingwebhookconfigurations                                                                                          |                                               | admissionregistration.k8s.io/v1 | false      | ValidatingWebhookConfiguration       |
| customresourcedefinitions                                                                                                | crd,crds                                      | s.k8s.io/v1                     | false      | apiextensionCustomResourceDefinition |
| apiservices                                                                                                              |                                               | apiregistration.k8s.io/v1       | false      | APIService                           |
| controllerrevisions                                                                                                      |                                               | apps/v1                         | true       | ControllerRevision                   |
| daemonsets                                                                                                               | ds                                            | apps/v1                         | true       | DaemonSet                            |
| deployments                                                                                                              | deploy                                        | apps/v1                         | true       | Deployment                           |
| replicasets                                                                                                              | rs                                            | apps/v1                         | true       | ReplicaSet                           |
| statefulsets                                                                                                             | sts                                           | apps/v1                         | true       | StatefulSet                          |
| selfsubjectreviews                                                                                                       |                                               | authentication.k8s.io/v1        | false      | SelfSubjectReview                    |
| tokenreviews                                                                                                             |                                               | authentication.k8s.io/v1        | false      | TokenReview                          |
| localsubjectaccessreviews                                                                                                |                                               | authorization.k8s.io/v1         | true       | LocalSubjectAccessReview             |
| selfsubjectaccessreviews                                                                                                 |                                               | authorization.k8s.io/v1         | false      | SelfSubjectAccessReview              |
| selfsubjectrulesreviews                                                                                                  |                                               | authorization.k8s.io/v1         | false      | SelfSubjectRulesReview               |
| subjectaccessreviews                                                                                                     |                                               | authorization.k8s.io/v1         | false      | SubjectAccessReview                  |
| horizontalpodautoscalers                                                                                                 | hpa                                           | autoscaling/v2                  | true       | HorizontalPodAutoscaler              |
| cronjobs                                                                                                                 | cj                                            | batch/v1                        | true       | CronJob                              |
| jobs                                                                                                                     |                                               | batch/v1                        | true       | Job                                  |
| certificatesigningrequests                                                                                               | csr                                           | certificates.k8s.io/v1          | false      | CertificateSigningRequest            |
| leases                                                                                                                   |                                               | coordination.k8s.io/v1          | true       | Lease                                |
| bgpconfigurations                                                                                                        |                                               | crd.projectcalico.org/v1        | false      | BGPConfiguration                     |
| bgpfilters                                                                                                               |                                               | crd.projectcalico.org/v1        | false      | BGPFilter                            |
| bgppeers                                                                                                                 |                                               | crd.projectcalico.org/v1        | false      | BGPPeer                              |
| blockaffinities                                                                                                          |                                               | crd.projectcalico.org/v1        | false      | BlockAffinity                        |
| caliconodestatuses                                                                                                       |                                               | crd.projectcalico.org/v1        | false      | CalicoNodeStatus                     |
| clusterinformations                                                                                                      |                                               | crd.projectcalico.org/v1        | false      | ClusterInformation                   |
| felixconfigurations                                                                                                      |                                               | crd.projectcalico.org/v1        | false      | FelixConfiguration                   |
| globalnetworkpolicies                                                                                                    |                                               | crd.projectcalico.org/v1        | false      | GlobalNetworkPolicy                  |
| globalnetworksets                                                                                                        |                                               | crd.projectcalico.org/v1        | false      | GlobalNetworkSet                     |
| hostendpoints                                                                                                            |                                               | crd.projectcalico.org/v1        | false      | HostEndpoint                         |
| ipamblocks                                                                                                               |                                               | crd.projectcalico.org/v1        | false      | IPAMBlock                            |
| ipamconfigs                                                                                                              |                                               | crd.projectcalico.org/v1        | false      | IPAMConfig                           |
| ipamhandles                                                                                                              |                                               | crd.projectcalico.org/v1        | false      | IPAMHandle                           |
| ippools                                                                                                                  |                                               | crd.projectcalico.org/v1        | false      | IPPool                               |
| ipreservations                                                                                                           |                                               | crd.projectcalico.org/v1        | false      | IPReservation                        |
| kubecontrollersconfigurations                                                                                            |                                               | crd.projectcalico.org/v1        | false      | KubeControllersConfiguration         |
| networkpolicies                                                                                                          |                                               | crd.projectcalico.org/v1        | false      | NetworkPolicy                        |
| networksets                                                                                                              |                                               | crd.projectcalico.org/v1        | true       | NetworkSet                           |
| endpointslices                                                                                                           |                                               | discovery.k8s.io/v1             | true       | EndpointSlice                        |
| events                                                                                                                   | ev                                            | events.k8s.io/v1                | true       | Event                                |
| flowschemas                                                                                                              |                                               | flowcontrol.apiserver.k8s.io/v1 | false      | FlowSchema                           |
| prioritylevelconfigurations                                                                                              |                                               | flowcontrol.apiserver.k8s.io/v1 | false      | PriorityLevelConfiguration           |
| ingressclasses                                                                                                           |                                               | networking.k8s.io/v1            | false      | IngressClass                         |
| ingresses                                                                                                                | ing                                           | networking.k8s.io/v1            | true       | Ingress                              |
| networkpolicies                                                                                                          |                                               | networking.k8s.io/v1            | true       | NetworkPolicy                        |
| runtimeclasses                                                                                                           |                                               | node.k8s.io/v1                  | false      | RuntimeClass                         |
| apiservers                                                                                                               |                                               | operator.tigera.io/v1           | false      | APIServer                            |
| imagesets                                                                                                                |                                               | operator.tigera.io/v1           | false      | ImageSet                             |
| installations                                                                                                            |                                               | operator.tigera.io/v1           | false      | Installation                         |
| tigerastatuses                                                                                                           |                                               | operator.tigera.io/v1           | false      | TigeraStatus                         |
| poddisruptionbudgets                                                                                                     | pdb                                           | policy/v1                       | true       | PodDisruptionBudget                  |
| bgpconfigurations                                                                                                        | bgpconfig,bgpconfigs                          | projectcalico.org/v3            | false      | BGPConfiguration                     |
| bgpfilters                                                                                                               |                                               | projectcalico.org/v3            | false      | BGPFilter                            |
| bgppeers                                                                                                                 |                                               | projectcalico.org/v3            | false      | BGPPeer                              |
| blockaffinities                                                                                                          | blockaffinity,affinity,affinities             | projectcalico.org/v3            | false      | BlockAffinity                        |
| caliconodestatuses                                                                                                       | caliconodestatus                              | projectcalico.org/v3            | false      | CalicoNodeStatus                     |
| clusterinformations                                                                                                      | clusterinfo                                   | projectcalico.org/v3            | false      | ClusterInformation                   |
| felixconfigurations                                                                                                      | felixconfig,felixconfigs                      | projectcalico.org/v3            | false      | FelixConfiguration                   |
| globalnetworkpolicies                                                                                                    | gnp,cgnp,calicoglobalnetworkpolicies          | projectcalico.org/v3            | false      | GlobalNetworkPolicy                  |
| globalnetworksets                                                                                       GlobalNetworkSet |                                               | projectcalico.org/v3            | false      | GlobalNetworkSet                     |
| hostendpoints                                                                                                            | hep,heps                                      | projectcalico.org/v3            | false      | HostEndpoint                         |
| ipamconfigurations                                                                                                       | ipamconfig                                    | projectcalico.org/v3            | false      | IPAMConfiguration                    |
| ippools                                                                                                                  |                                               | projectcalico.org/v3            | false      | IPPool                               |
| ipreservations                                                                                                           |                                               | projectcalico.org/v3            | false      | IPReservation                        |
| kubecontrollersconfigurations                                                       project                              |                                               | calico.org/v3                   | false      | KubeControllersConfiguration         |
| networkpolicies                                                                                                          | cnp,caliconetworkpolicy,caliconetworkpolicies | projectcalico.org/v3            | false      | NetworkPolicy                        |
| networksets                                                                                                              | netsets                                       | projectcalico.org/v3            | true       | NetworkSet                           |
| profiles                                                                                                                 |                                               | projectcalico.org/v3            | false      | Profile                              |
| clusterrolebindings                                                                                                      |                                               | rbac.authorization.k8s.io/v1    | false      | ClusterRoleBinding                   |
| clusterroles                                                                                                             |                                               | rbac.authorization.k8s.io/v1    | false      | ClusterRole                          |
| rolebindings                                                                                                             |                                               | rbac.authorization.k8s.io/v1    | true       | RoleBinding                          |
| roles                                                                                                                    |                                               | rbac.authorization.k8s.io/v1    | true       | Role                                 |
| priorityclasses                                                                                                          | pc                                            | scheduling.k8s.io/v1            | false      | PriorityClass                        |
| csidrivers                                                                                                               |                                               | storage.k8s.io/v1               | false      | CSIDriver                            |
| csinodes                                                                                                                 |                                               | storage.k8s.io/v1               | false      | CSINode                              |
| storageclasses                                                                                                           | sc                                            | storage.k8s.io/v1               | false      | StorageClass                         |
| volumeattachments                                                                                                        |                                               | storage.k8s.io/v1               | false      | VolumeAttachment                     |
to list all the versions of all these objects to create the manifest to spin objects declaratively. 
```
kubectl api-resources | grep ^pods
```
Shows the information, such as name, shortname, apiVersion, namespaced, kind. 
>Here, the `version` and `kind` matters in terms of creating pods.

**Now will get into creating a pod manifest:**
```yaml
apiVersion: v1
kind: Pod
metadata:
  name: hexrapp-demo
  labels:
    app: nginx
    env: prod
spec:
  containers:
  - name: hexrapp
    image: nginx
    ports:
    - containerPort: 8080
    volumeMount:
     - name: nginx-data
       mountPath: /var/www/html
    
```
As we saw previously,  ++to find all the relevant options to declare in `yaml` file, refer context given below for your reference,
1. **apiVersion**
defining `apiVersion` - what version of object i have to use,
```
apiVersion: v1
```

2. **kind**
`kind` - kind of object to use to create the object 
> here it is **`Pod`**. refer that in `api-resources` to declare the version of it.
```
kind: Pod
```

3. **metadata**
naming and labelling the `pod` using `metadata`, two spaces. adding labels by passing parameters. naming it & labelling it-
> # you can pass multiple params to those labels, which doesn't affect the pod behavior, doesn't change the application in anyways. labels are all just metadata. just an additional info to the pods. To filter, to gather information and more. nothing will happen if it didn't even exist but `name` is mandatory.
> labels for `Pod` the list goes on. versions, annotations, namespaces and so many parameters which you can declare as per your requirement. here we are good with `name` and `labels`.

`metadata`: `
  `name`: appname, # any name, mandatory (!imp)
   `labels`: # are optional (even if u dont give these, nothing will happen)
		app: nginx
	    env: prod
```yaml
metadata:
  name: hexrapp-demo
  labels:
    app: nginx
    env: prod
```

4.  **spec**
specifications, that you want to pass for your `Pod`. First is nothing but `containers`. Declaring all the containers that I want to create. 2spaces + hyphen.
>Since, Pod is just a logical layer, `containers` are the same as we use with common network and shared storage, under the layer of `Pod` 

`spec`:
  `containers`: 
     - `name`: frontend # name of the container, not the pod since we gave it in `metadata`
       `image`: nginx:lts # `image` of this container + `version` as tag
       `ports`: # though not important but mandatory
       - `containerPort`: 80 # default nginx port, even if isn't declared or other port given, it proceeds only with the default.
       `volumeMount`:
       - `name`: hexr-vol # name of the volume
        `mountpath`: /var/www/html # volume path, whatever gets here saved in 

   `volumes`:
         - name: hexrapp # name of the volume
         emptyDir: {} 

in-terms of storage:
```
volumeMount:
	name: hexr-vol 
    mountpath: /var/www/html
volumes:
  - name: hexrapp-vol
    emptyDir: {}
```
the volume of the declared container -> is mounted -> to the Volume of the POD which is - Host Volume and gets binded standalone on that POD using `emptyDir` *which binds whatever container that we declare within that POD*
here, mountPath: /var/www/httml -> binds to Volume: hexrapp with emptyDir:{} -> gets the bind input

```yaml
spec:
  containers:
  - name: hexrapp
    image: nginx
    ports:
    - containerPort: 80
    volumeMounts:
    - name: hexrapp-vol
      mountPath: /var/www/html
  volumes:
  - name: hexrapp-vol
    emptyDir: {}
```
 parameter that binds the container's volume to the pods shared storage volume. will make sense when we work with this, will see more in future. 

Process of creating pod in declarative approach by applying this `.yaml` file.
```
kubectl apply -f pathofthe/pod.yaml

kubectl create -f pathofthe/pod.yaml 

#both works!
```
Difference between `kubectl create` vs `kubectl apply`:
`create`: 
`apply`: 

```
kubectl describe node <node-name> #verify metrics of the node

kubectl apply -f hexrapp-pod.yaml #apply pod manifest to create

kubectl get po -A -o wide # check pods detail  brief
```

**How to apply the `yaml` file, to create the pod**, simply practicing declarative approach by applying this `yaml` file to the kubernetes cluster.  HOW TO DO THAT? this `yaml` file has to get to the `kube-apiserver` HOW?
`kubectl` - **`create`**, **`apply`** x2
1) Does a **schema validation** first,
2) Applies the `yaml` file next,
3) pod gets created
```
kubectl create -f pathofthe/pod.yaml
```
```
kubectl apply -y pathofthe/pod.yaml
```
-f - file

**To verify such pods**
```
kubectl get po/pod/pods #shows pods
kubectl describe po/pod/pods #describes pod in detail 
```
Here, 
- Name: hexrapp # name of the pod
- Ready: 0/1 # expected, actual (running and ready) state vs desired state
- Status: Pending/Running/Stopped # status
- Restarts: 0 
- Age: # pod created time 
> `ready` has to be 1/1 equal to = `status` if it is in Running state.

>++ TROUBLESHOOT IF ANY ERROR OCCURS OR IF ANY MISMATCH OCCURS.
```
kubectl get po -o wide
kubectl describe po
kubectl events po
curl <podIP>
```

>Possible errors and troubleshoot methods:
>`describe` pods and nodes and check the message troubleshooting the same.

>curl the IP.


> - taints and tolerations
> - worker node misconfigurations
> - network misconfigurations such as `CIDR` or `IP`.

TAINTS AND TOLERATIONS:
```yaml
apiVersion: v1
kind: Pod
metadata:
  name: hexrapp
  labels:
    app: nginx
    env: prod
spec:
  serviceAccountName: default
  containers:
    - name: hexrapp
      image: nginx
      ports:
        - containerPort: 80
      volumeMounts:
        - name: kube-api-access-k662f
          mountPath: /var/run/secrets/kubernetes.io/serviceaccount
          readOnly: true
        - name: hexrapp
          mountPath: /var/www/html
  tolerations:
    - key: "node-role.kubernetes.io/control-plane"
      operator: "Exists"
      effect: "NoSchedule"
  volumes:
    - name: hexrapp-vol
      emptyDir: {}
    - name: kube-api-access-k662f
      projected:
        sources:
          - serviceAccountToken:
              expirationSeconds: 3607
          - configMap:
              name: kube-root-ca.crt
          - downwardAPI: {}

```
###### to delete a pod:
```
kubectl delete po <podname> #deleting by podname
kubectl delete -f pathofthe/pod.yaml #deleting by manifest
```

**SUCCESSFULLY LAUNCHED PODS ON WORKER NODES VIA CONTROL PLANE** using `kubectl`
>**BIG FUCKING DEAL! 
>but a basic pod creation using declarative approach by writing a declarative YAML File in order to create an object/resource in Kubernetes cluster with the help of `kubectl`  **

> [!NOTE] ##### FINAL file - BASIC NGINX POD MANIFEST
> ###### podapp.yaml
> ```yaml
apiVersion: v1
kind: Pod
metadata:
  name: podapp-nginx
  labels:
    app: podapp-nginx
    env: prod
    image: nginx
spec:
  containers:
    - name: podapp-nginx
      image: nginx:latest
      ports:
        - containerPort: 80
      volumeMounts:
        - name: podapp-vol
          mountPath: /var/www/html
  volumes:
    - name: podapp-vol
      emptyDir: {}

---

#### 6.POD creation using Imperative approach
###### Pods creation using imperative approach - [Doc](obsidian://open?vault=tutorialHell&file=Orchestration%2Fk8engineers.com%2FKubernetes-Deep-Dive%2Foffl-raw-docs%2Fkubernetes%20deepdive%2Fsec3-k8s-pods%2F3.Pods%20creation%20using%20imperative%20approach.docx)
Will cover creating a pod using imperative approach simply by passing commands via the cli.

We will explore ways to quickly access command references, k8s-object-specific commands, helpful aliases, and command completion. But first, how are command strings built?

before that,
will delete the pod and respawn for our practice sake.
```
kubectl delete po <podname> 
/ --all(not recommended espeacially in prod env)
```

> [!NOTE] small note about `namespaces`
> if you do,
> ```
> kubectl get po -o wide
> ```
> it will tell you that,
> *No resources found in default namespace.*
> What is namespace means here? 
> **Namespace** - an isolated environment. if you create any object or resource, it will housed under default namespace.

WILL SEE MORE ON THAT IN FUTURE.
++

| obj                        | Docker declarative     | Docker imperative | K8s declarative | K8s imperative         |
| -------------------------- | ---------------------- | ----------------- | --------------- | ---------------------- |
| **Creating  an image/pod** | Dockerfile             | `docker build`    | manifests       | `kubectl create/apply` |
| **Running an image/pod**   | Docker desktop or extn | `docker run`      | manifests       | `kubectl run`          |
| **Executing a pod**        | terminal, vscode, etc  | `docker exec`     | manifests       | `kubectl exec`         |
similar to that! 

Now back to topic,
###### Creating POD using imperative approach
```
kubectl run --help
```
refer the same in brief and will see that here one by one. says how to use the same via `cli`

###### Command Line Syntax:
The syntax:
-> English and Chinese are `subject-verb-object (SVO)` languages.
-> Hindi and Korean are `subject-object-verb (SOV)` languages.

**If kubectl were a language, it would be a `kubectl + verb + object/[name optional] + flag` (kvof) language 😄**

![[Pasted image 20241203184416.png]]
Also similar to languages, the best way to learn and absorb grammar is by using it in context and not memorizing long verb and object lists.

**gentle reminder:**
> _ℹ️ If you are stuck and want to quickly reference the existing Kubernetes objects in any Kubernetes version run_ `_kubectl api-resources_`_._

*Commands are built by choosing that `action [verb]` you want to apply to the desired Kubernetes `resource [object]` usually followed by the name of the resource additionally you have a large array of `filters [flags]` that can be applied to the command which will determine the final scope and output.*
*![[Pasted image 20241203184512.png]]*

*Let’s look at a simple example of command building using the commonly used `get` verb to retrieve all resources in the namespace named as your`app`, and the output is in `yaml` format:*
```
kubectl get all --namespace app -o yaml
```

*gentle reminder:*
>*ℹ️ If you come across a Kubernetes resource that you haven’t heard of before or need a refresher use_ `_kubectl explain [resource-name]_` to get an in-terminal description and usage instructions.*

***USAGE/SYNTAX:***
```
  kubectl run NAME --image=image [--env="key=value"] [--port=port] [--dry-run=server|client] [--overrides=inline-json]
[--command] -- [COMMAND] [args...] [options]
```
*also, you can refer,* 
```
kubectl options
```
*for list of global command-line options that you can make use out of. ALL THE OPTIONS/PARAMETERS THAT WE CAN PASS.*

###### Working imperatively
When working in Kubernetes environments your tasks are many, anything from deploying new apps, troubleshooting faulty resources, inspecting usage, and much more. Later on, we will explore how useful declarative ways of working are more appropriate for defining and deploying workloads, but for everything else we have our arsenal of useful imperative Kubernetes commands at the ready.

Simple commands to get us started are:

**WILL SEE CREATING A RESOURCE BY RUNNING THE SAME (CMDs) - IMPERATIVE APPROACH**
JUST LIKE WE DO IN DOCKER.
```
kubectl run -it <podname> --image=<imgname:v> --restart=Always/onfailure/never  
```

```
kubectl run -it hexrapp --image=nginx:latest --restart=Always
```

**TO CHECK LOGS OF A POD:**
```
kubectl logs <podname>
```
not the IP Address.

**OF MULTIPLE PODS:**
```
kubectl logs podname1 podname2 .. podnamen
```
HERE,NOW YOU CAN ABLE TO CREATE OBJECTS/RESOURCES USING IMPERATIVE APPROACH. (From the cli)

BUT,
> **DECLARATIVE APPROACH IS HIGHLY RECOMMENDED**

WHY?
- Operational Intensive
- Too much clutter
- Can't comply to manage multiple containers
- Can't recall the parameters again and again, where it is easy to save configs in a file.
- parameters clutter again and again in every execution, peculiar and relevant for each and every relevant options.
>**basic pod creation using imperative approach by passing commands and parameters in order to create an object/resource in Kubernetes cluster with the help of `kubectl` **

>nginx:latest. if nothing works, specify versions for not messing things up. WILL THROW A `IMAGEPULLBACK` ERROR. ++ -d, didn't work 
>Flags, -i = interactive, -t = tty, terminal, -- bash -> shell access / execution

>To take imperative commands to the next step know that you can modify resources by using the [TUI](https://en.wikipedia.org/wiki/Text-based_user_interface#:~:text=In%20computing%2C%20text%2Dbased%20user,before%20the%20advent%20of%20bitmapped) editor:  
By running `kubectl edit -n [namespace] [resource-name]` a text editor like the one below will open. Make you edits and close the editor just like in [vim](https://www.vim.org/) by running `ESC + :q!`.

###### ACCESSING / ASSESSING A SHELL OF A POD's CONTAINER:
```
kubectl run -it <podname> --image=nginx:lts --restart=Always -- bash
```
*gets you into the shell of the POD's Container.* 
*cant use `kubectl`, `systemctl` or `service` commands here since it is whole other system.*

**test the container:**
```
curl localhost
```
will not run: due to the app is not running in the container.  To run the nginx inside the container,  gotta use some `cli` commands but can't use systemctl or service commands here. SINCE IT IS A CONTAINER NOT A SYSTEM ITSELF.
	Here, will be starting the `nginx`

EXECUTE NGINX POD $SHELL TO ENABLE NGINX.
syntax: nginx, -g = global, "" - params
```
nginx -g "daemon off; "
```
**NGINX TURNED ON**. and starts to run the web application.

Leave it as a process and open up an another terminal to exec IN PARELLEL & WATCH LOGS:
```
curl <podIP>
```
Prints nginx default page. + can see the request received in the side(logs).

> **We have successfully seen creating a Pod Object using both DECLARATIVE and IMPERATIVE**

After playing,
SIMPLY YOU CAN do a Cleanup / DISPOSE IT ALL. 
before that, exit the shell session, deletes only if u exit.

`ctrl + c`, `ctrl + d` and 
```
kubectl delete po <podname> 
kubectl delete po --all
```
```
kubectl get po -o wide #to verify the cleanup
```
> **THIS IS HOW YOU WORK WITH IMPERATIVE APPROACH**. SEE THE PAIN OF DOING ALL OF THESE. 
> **DECLARATIVE APPROACH IS A WISE CHOICE. SIMPLY WRITE IT ALL THAT WHAT YOU WANT TO CREATE, SIMPLY PASS THAT YAML AND VIOLA!**

 ***DECLARATIVE APPROACH = ALIGNS WELL WITH THE ORECHESTRATION CONCEPT***
 
---
####  7.POD creation workflow
###### Pod Creation Workflow -[Doc](obsidian://open?vault=tutorialHell&file=Orchestration%2Fk8engineers.com%2FKubernetes-Deep-Dive%2Foffl-raw-docs%2Fkubernetes%20deepdive%2Fsec3-k8s-pods%2F4.Pod%20Creation%20Workflow.docx)
TILL NOW WE HAVE SEEN CREATING PODS in both declarative and imperative method. **WILL SEE THE WORKFLOW OF A POD CREATION:** (some basic things to know how things works under the hood while we create pods).

Referring to the K8s Architecture: we have gone through referring that **there are multiple components exist in both the control plane and worker node - which eventually talks to each other.**

Just the basics: WILL UNDERSTAND IN BRIEF ABOUT THE SAME: AND ANSWERING TO THE QUESTIONS OF:
- what happens when we create or try to create a POD?
- What K8s does?
- **WHAT TF HAPPENS WHEN WE PASS INPUT TO `kubectl` to create or whatever to an object BTS in K8s?** 
-  How the input has been passed to the `cri-runtime` to create a container
- And how you get back the response or acknowledgment?

Will refer the flow diagram for the same:   (WE HAVE ALL THE RELEVANT COMPONENTS THAT EXIST INBETWEEN IN TERMS OF CREATING A POD)
![[Pasted image 20240827163448.png]]
 
`kubectl`  < - >  `kube-apiserver`  < - >  `etcd`  < - >  `scheduler`  < - > `kubelet`  < - > `containerd` / or `docker` if it is used as **`cri`**

**DECODING THE FLOWCHART:**
INPUT TO CREATE POD:
1) USER -> `kubectl`  --> `kube-apiserver`
*(---create Pod--->)*
-- **user** passes --> **pod creation request** will be sent via `kubectl` --> to `kube-apiserver`
2) Now, `kube-apiserver` --> will verify with `etcd` 
*(---write --->)*
3) `etcd` will store the config and respond back -->  to `apiserver`
*<---* )*user if exist/`api-server`
-- (to check and return data whether the resource already exist or not)
if already exists, spins up the POD!
if the resource doesn't exist, 
4) `apiserver` to user - creating those pods
*(<----) - already exists*
5) `apiserver` --> will pass it to the `scheduler`
*(---watch (new pod)--->)*
-- `scheduler` will analyze based on some metrics and algorithm to picks the suitable node to deploy the POD.
6) After deciding it, `scheduler` --> to `apiserver`, where that information 
*(<---bind pod---)*
gets stored in -> `etcd`. 
*(---write-->)*
>`etcd` can't talk to `scheduler` , likewise `scheduler`  to `etcd` too. 
>the reason why the `scheduler`  --> passed info to `apiserver`  -x- not to `etcd`, due to security + nature/design of K8s + distributed systems since the `apiserver` is the brain and mediator between the K8s Components.

7) after storing the information, `etcd`  acknowledges back  -> to `apiserver` 
*(<---)*
8) `apiserver` --> will talk back to `scheduler` - passes the acknowledgement -> to create `pods` at any on of the nodes' `kubelet` to  any one of the relevant worker node, relevant `Pod` acknowledges  back to `apiserver` back that the **Pod Creation process** is going to get initiated. 
*(---->)*
> all these decision has been done here, ON THE CONTROL PLANE NODE.
9) After getting all that info from `Control Plane`'s `api-server` --> `kubelet` service,
*(---watch(bound pod)--->)*
`kubelet` --> will pass command via `crictl` -> `containerd or any cri` 
*(---crictl--->)*
creating all the relevant `PODs`.
10) after all `PODs` has been created, `containerd or any cri` --> will send back the **acknowledgement** to `kubelet`.
(<----)
11) again, `kubelet` --> will pass that acknowledgement info to `apiserver`
*(<---update pod status---)*
12) `apiserver` --> stores that info too in `etcd`
*(---write --->)*
13) `etcd` --> sends back the acknowledgement to `apisever` 
*(<---)*
14) `apiserver` --> will send info to `kubelet` and 
*(---->)*
15) `apiserver` --> Will send and we will be RECEIVING THE CONSOLE OUTPUT THAT THE **PODS HAS BEEN CREATED, FAILED OR PENDING**
16) ALL DONE!
Now we can take care working with the rest as 
```
kubectl get pod -o wide -A
kubectl describe pod
```


---

#### 8. POD resource allocation CPU and Memory + Extra parameters
**POD resource allocation CPU and Memory + Extra parameters such as environment variables, resource blocks and more**

previously on pod manifest, we have seen apiVersion, kind, metadata, spec, in that containers in name, tag, ports, container volumes
but not seen **PORTS, ENV-VAR, Resource Block + Shared Volumes/Storage.**

NOW, WILL DIG DEEPER INTO MORE PARAMETERS/FACTORS such as
1) allocating resources
 - CPU
 - Memory
 - storage and more
2) Environment variables
3) Resource limiting (for containers inside the POD which doesn't effect the node)
4) Volume mounting in brief
5) Other blocks, such as
- Resource block
- port block
![[Pasted image 20240828112459.png]]
```
apiVersion: v1
kind: Pod
metadata:
  name: hexrapp
  labels: 
    app: hexrapp
    env: prod
spec:
  containers:
    - name: hexrapp
      image: nginx
      ports:
        - containerPort: 80
      env:
        - name: DB_USERNAME
          value: "admin"
      resources:
        limits: 
          cpu: "500m"
          memory: "500Mi"
        requests:
          cpu: "200m"
          memory: "200Mi"
      volumeMounts:
        - name: hexrapp
          mountPath: /var/www/html
  volumes:
    - name: hexrapp
      emptyDir: {}
```
![[Pasted image 20240828112413.png]]
**Volume Block Demystification Overview:**
We have just worked on it bluntly. Will have an insight on how this works.
Thumb rule: Name of the container volumes = Name of the Pod Volume
1) Volume of container's - Container Volume
-- which the name, and the `WORKDIR` path has been defined
2) Volume of Pod's - Shared Pod Volume
**Referring to the diagram:**
Container volume - named here as mounts that the name itself says that.
```
     volumeMounts:
      - name: hexr-vol
        mountPath: /var/wwww/html
```
Volume mount path that which of the directory that you want to expose.
-> Inside the container as we speak the CONTAINER VOLUME, we are performing mounting a `WORKDIR` folder which is exposed that to be mounted with
-> The Shared storage - Pod Volume. Mounted to what volume? 
   ```
volumes: 
    - name: hexr-vol
      emptyDir: {}
```
Volume of Pod's = Shared Storage - Pod Volume.  
>TO MAKE THE CONTAINER WORK WITH THE POD SHARED VOLUME, BOTH OF THE VOLUME NAMES MUST BE IDENTICAL. SAME STRING
> 
**Note: IN ORDER TO ASSESS THE CONTAINER VOLUME ON THE POSHARED VOULME TOO, IT IS MUST THAT *NAME* OF BOTH OF THE VOLUME  SHOULD BE IDENTICAL TO BE MAPPED** 

++

Apart from Containers: the resources and data of a POD such as Network, Volume, Libraries that helps running that container, cri and more -> are administrated by `kubelet`
One more point to be noted that, in the 
POD Volume: emptyDir{} -> means that, wherever the pod gets launched on a particular HOST MACHINE and NODE, All these data must gets stored in some `WORKDIR` . For a Pod running in a node, the data get stored in a location - `kubelet`, the default working directory of a `kubelet` is `/var/lib/kubelet/`. Here all the POD related data gets stored here.  -> which is mapped as `emptyDir{}` here.
**WILL SEE MORE IN BRIEF WHEN WE DO THINGS HANDS-ON**.

BACK TO TOPIC! 
###### APPLYING THE SAME YAML TO UNDERSTAND POD RESOURCE ALLOCATION IN BRIEF.
RECALLING THE PERVIOUS `hexrapp.yaml` or other as per ur convenience - POD MANIFEST TO CONTNUE THE SAME.
```
apiVersion: v1
kind: Pod
metadata:
  name: hexrapp
  labels: 
    app: hexrapp
    env: prod
spec:
  containers:
    - name: hexrapp
      image: nginx
      ports:
        - containerPort: 80
      resources:
        limits: 
          cpu: "500m"
          memory: "500Mi"
        requests:
          cpu: "200m"
          memory: "200Mi"
      volumeMounts:
        - name: hexrapp
          mountPath: /var/www/html #or /usr/share/nginx/html
  volumes:
    - name: hexrapp
      emptyDir: {}
```
WE USED THIS MANIFEST TO LAUNCH A POD with One single container.
*Lets reuse the same and add parameters, sections, blocks, environment variables and resource allocation on top of it* How?
**All these params applied to Container section**
1) **Port block**
```
ports: #optional, proceeds with the default
 - name: hexr-port #naming the port block
   containerPort: 80 #port number specifiacatino
   protocol: TCP #none changes if not declared, goes default
```
2) **Environment Variables:**  
```
env:
  - name: DB_USER #name of the env var
    value: "admin" #value to that var
#creds to login after a successfull container launch, print env
```
3) **Resource Block:** by default, no resource limit has been defined. Dynamic Usage - Amount of CPU and Memory to be utilized **min** or **max** limited to the available resources of the container.  -> (which is taken care of `cgroup` under the hood) 
Why? 
**One set of application must not affect the other set of resources or components due to the over utilization of the container we deployed in the POD. Either both the control plane or compute plane.**


**Guide to `Resource` limit: Two params:**
1) **Limits** 
2) **Requests** 

**limits**: - **maximum that you can use - CPU and Memory** 
#.container should not use more than the given limit **(MAX LIMIT)**
1) cpu: "500m" # core or percent or micron **0.5**/ **50%** # **1**core/**1000m**(icrons)/**100%** util
2) memory: "500Mi" # **Ki** -kb, **Mi**-mb, **Gi**-gb, **Ti**-tb 
**requests**: **minimum limit has to be allowed/allocated for that resource**
#.how much resources to be allocated, in-demand **(MIN LIMIT)** required
1)  cpu: "200m" # core allocated
2) memory: "200Mi"  # memory allocated
```
resources:
  limits: 
    cpu: "500m"
    memory: "500Mi"
  requests:
    cpu: "200m"
    memory: "200Mi"
```
4) **volumes:**
> ==**WHATEVER COMMANDS YOU ARE PASSING FROM HERE, DO THEM IN YOUR WORKER NODE. So obvious, that the PODs Components such as containers, network, plugins, volumes and more**. Since all that stuff lies there. ==

> HOST: `emptyDir: {}`<-mounted-> `/var/www/html` - WORKDIR of the container

**volumeMount**: #.from containers-mounting this one to emptyDir{} + whatever data written here will be appear -> also in emptyDir{}
**volumes**: #.volume of Path's container volume gets mounted to emptyDir{} a.k.s Host Volume
```
  containers:
      volumeMounts:
        - name: podapp-vol
          mountPath: /var/www/html 
  volumes:
    - name: podapp-vol
      emptyDir: {} #volume of Path's container volume gets mounted to emptyDir{} a.k.s HostVolume
```
will see more on Volumes to work in brief with storage solutions.

**PASS THE MANIFEST TO SEE THE RESULTS.**
```
kubectl create -f app.yaml
kubectl get po -o wide
curl <podIP>
```
CHECK ALL THE METRICS PASSED,
For monitoring the usage:
```
kubectl top nod
kubectl top pod
```

**TO DO THAT, NEED TO LOG INTO THE CONTAINER. HOW TO LOG INTO THE CONTAINER**
```
kubectl exec -it podname -c <containername(if multiple con inside a pod)> -- bash
```
VOILA! LOGGED IN! -> base machine of nginx -> `debian`  - basic cmds can be ran. + check the specs that are applied in the POD manifest. 

1) **environment variables:**
```
env
printenv
```

 All the ENV VAR can be shown. can find `DB_USER=admin`
2) **resources:**
```
kubectl describe no #prints all the nodes
kubectl describe no <nodenameo>
kubectl describe po -A#prints all the pods 
kubectl describe po <poname>
```
no:
- all the pods and its utilization that are running in that node
- specs of that node
- events and logs of the node
po:
- spec of that po, the containers inside that Pod
- info about the pod
- events and logs of the pod

specifically:  the running / deployed workload's information. such as
- utilization
- limits
- events 
- logs
- specs
- network information
- node/pod/workload information

shows the specs and can verify the resources given there. 
3) **volumes:**
```
kubectl get po <containername> -o wide
sudo ls -l /var/lib/kubelet/pods
```
the directory says that it is a pod but those are, ALL THE VOLUMES OF EACH OF THE PODS. **BUT, What we have is name, but here everything is in IDs. HOW TO NAVIGATE THIS**? IDs - UID
```
kubectl get po -o yaml
```
all these data came from `etcd` FYI.  Here, will filter out the volume as we need it.
```
kubectl get po -o yaml | grep "uid"
kubectl get podname -o yaml | grep "uid"
```
you get the `uid` of the pod.
```
sudo ls -l /var/lib/kubelet/pods/"uid"
```

THIS IS THE VOLUME MOUNT BETWEEN THE `container` and the `KUBELET`. Inside that `uid`,
```
kubernetes.io-empty-dir
kubernetes.io-projected
```
```
cd kubernetes.io-empty-dir
ls
<volume's-name>
```
>**TLDR**: SINCE BOTH ARE MOUNTED BIDIRECTIONAL, WHATEVER DATA WILL POPS UP ON, BOTH
>- /var/www/html - container
>- /var/lib/kubelet/pods/uid/kubernetes.io-empty-dir/volume-name/ -system

**The Rabbit hole of Volume:**
```
ls -l /var/lib/kubelet/pods/ #volumes of the pod in UID
ls -l /var/lib/kubelet/pods/"uid"/ #components of that po
ls -l /var/lib/kubelet/pods/"uid"/volumes #volumes of po
ls -l /var/lib/kubelet/pods/"uid"/volumes/k8s~empty-dir #volume mount of the container to the pod
ls -l /var/lib/kubelet/pods/"uid"/volumes/k8s~empty-dir/VolName(notTheMount)/ #.here's where all the data will be stored!
```
in abstract:
```
sudo ls -l /var/lib/kubelet/pods/c51418d3-09a8-472b-95d1-1bce87ebe02f/volumes/kubernetes.io~empty-dir/podapp-vol/
```
#.here's where all the data will be stored! whichever gets generated in that Container. stored here in this POD volume too with the help of Volume mounting.

Here, we mostly see the folder is totally empty because, 
the directory `/var/www/html` is an empty dir. and we have no data there! 
also you can have `/usr/share/nginx/html` which is nginx's default dir. to see some content.
``` 
containers:
      volumeMounts:
        - name: hexrapp #name of the mount
          mountPath: /var/www/html 
#or /usr/share/nginx/html
  volumes:
    - name: hexrapp #name of the volume
      emptyDir: {}
```
Will fix this!

What will do is, will delete the pod for now and rewrite to make the proper mount to happen since there is the default working directory is a mismatch.

Will mount the same with `/usr/share/nginx/html`

**So, Make the changes as you want in that POD Manifest File.** by replacing volumes of the container as:
```
containers:
      volumeMounts:
        - name: hexrapp #name of the mount
          mountPath: /usr/share/nginx/html
  volumes:
    - name: hexrapp #name of the volume
      emptyDir: {}
```
To apply the changes, You can simply go do `kubectl delete po podname`. But,
>Note: Instead of deleting a POD. You can also replace it. So, simply replace by passing 
```
kubectl replace --force -f pathofthe/pod.yaml
```
again surf the volume and go do the same.
```
kubectl get po -o yaml | grep "uid"
kubectl get podname -o yaml | grep "uid"
ls -l /var/lib/kubelet/pods/"uid"/volumes/kubernetes.io~empty-dir/podapp-vol/
```
might wonder that both of these are same. But the thing is, It is empty on the POD too. Still the same but the default directory is the step we have gone through. What is means?

What happens in the background is. since the default directory is empty itself!, you cant `curl` nothing. try `curl` the IP.
```
kubectl get po -o wide
curl IP
```
Prints `403 Forbidden`. It supposed to print `Welcome to nginx` right?

To make that happen, simply create the same. for the volume mount to get successful.
```
touch index.html
nano index.html
```
```
<h1>Welcome to K8s PodApp Pod's Container's Nginx's Volume from EmptyDir{}</h1>
#or anything else
```
```
curl IP
```
 **Working! :). Means whatever that we have loaded here in terms of data we stored here `emptyDir{}`**, **WILL BE SPAWNED IN THE CONTAINER TOO - `/usr/shared/nginx/html`!**
 
WILL SEE MORE ON THIS IN PRACTICE. **More about Shared Volume.** here we have seen with only one container. Will see that in brief in detail doing the same with **Multiple containers**.
> [!NOTE] ##### FINAL file - BASIC NGINX POD WITH RESOURCE LIMITING - MANIFEST
> ###### podappx.yaml
>```yaml
apiVersion: v1
kind: Pod
metadata:
  name: podapp-nginx
  labels:
    app: podapp-nginx
    env: prod
    image: nginx
spec:
  containers:
    - name: podapp-nginx
      image: nginx:latest
      ports:
        - name: http
          containerPort: 80
          protocol: TCP
      env:
        - name: DB_USER
          value: "root"
      resources:
        limits: #max limit
          cpu: "700m" #can be in percentage%/micron-mi/core-numberValue
          memory: "500Mi" #also for Ki for Kilobytes, Mi for Megabytes, Gi for Gigabytes, Ti for Terabytes
        requests: #min limit
          cpu: "200m"
          memory: "200Mi"
      volumeMounts:
        - name: podapp-vol
          mountPath: /usr/share/nginx/html #mounting this one to emptyDir{} + whatever data written here will be appear -> also in emptyDir{}
  volumes:
    - name: podapp-vol
      emptyDir: {} #volume of Path's container volume gets mounted to emptyDir{} a.k.s HostVolume

---
####  9. POD multi-container with shared volume
###### Multi-container pod with Shared Volume - [Doc](obsidian://open?vault=tutorialHell&file=Orchestration%2Fk8engineers.com%2FKubernetes-Deep-Dive%2Foffl-raw-docs%2Fkubernetes%20deepdive%2Fsec3-k8s-pods%2F5.Multicontainer%20pod%20with%20Shared%20Volume.docx)

SPINNING MULTIPLE CONTAINERS IN A POD + WILL SEE THE SAME SHARED VOLUME WITH IT. SAME AS BEFORE SPINNING A SINGLE CONTAINER IN A POD BUT HERE THERE ARE MULTIPLE BUT XTRA VOLUME FOLDERS.

Will see a pictorial view:
![[Pasted image 20240830105402.png]]
Here is the Node Where our Pods have been launched. More probably the **Worker Node**. Except single-node setup if not. Will take the worker node as we are doing it. So, here for instance, what we have is,

Worker node -> Pod -> two-containers: 1) `nginx`, 2) `alpine`
- ==**As a user, if you want to access the POD. You will be ping or reaching out to the IP of the POD, not the containers.** ==. Now, 
- ==Here, The POD will be resolving the IP address and redirecting that request to the relevant Container.==
1) here, it gets routed to `nginx` since it is a web server and runs on port 80 serving data to the end-user.
 2) and what about `alpine` container. no application. just a system to run specific jobs. Here, it writes/produces data from a specific working directory, for example. `/var/tmp/index.html`
 Will see how that rolls out.
 - Here, whatever **data** that gets produced in the `/var/tmp/index.html`, will mount it to the Shared volume of that POD. 
**`/var/lib/kubelet/"uid"/volumes/emptydir/containerVolumes`**
- Now, the container's Volume `/var/share/nginx/html` is also  mounted to the Shared volume of that POD as well.
> **==which means, whatever pops in that Volume can be accessed in between the container as well.==**

So, `alpine` image is writing the data here in `/var/tmp/index.html`
-> 
that directory has been mounted to the same shared POD volume in
**`/var/lib/kubelet/"uid"/volumes/emptydir/containerVolumes`**`
-> which is also mounted to the `nginx` container to serve the data since the volume of that container `/var/share/nginx/html` is shared with PODs Shared Volume too.

**==MULTIPLE CONTAINERS CAN SHARE THE VOLUME TO SERVE THE DATA!==**

**Will see this in practice by applying the logic of launching an application that one container produces the data -> shares the same in POD volume 
-> which spawns into container which serves the data.**

***
	*We have a node. Inside that,*
	*Node --> Pod with an IP (with two containers)--> nginx + alpine } ->* 
> 	*If we got to access the container, no container IP can be resolved since each of the container can be communicated, only via the POD IP alone. Inside that IP, both can talk to each other only on its own `locahost`s or `127.0.0.1`.* 
	*==req -> PodIP -> nginxport80 + alpine22 -> container's data lands on its own volumes -> which will be stored also in the `volumemount` under systems.==*
	*e.g.: here, we have a pod with two containers. one nginx which serves data, one alpine which writes data. each `workdir` gets mounted on system's volume mount.*
	*- nginx port80-> /usr/share/nginx/html -> /var/lib/kubelet/podUID/volumes*
	*- alpine port80-> /var/tmp/`whateverdata` -> /var/lib/kubelet/podUID/volumes*
	*=> based on this logic, will deploy an application works this way which helps and used in many cases. POD with multi-container.*
	***Moral: A single storage volume can be stored and serve data to multiple containers. Just like other infra models such as VMs and On-prems. Same logic can be applied to multi-container pod***
***

Will see that in practice. 
`multicontainer-pod.yaml` 

> Note: without defining apiVersion, rest of the parameters such as `apiVersion`, `kind`, `metadata` and `spec` can't be defined and wont be passed to K8s. 

```
apiVersion: v1
kind: Pod
metadata:
  name: hexrapp
  labels:
    app: hexrapp
    type: web
spec:
  containers:
  - name: hexrapp-writer
    image: alpine
    command: ["/bin/sh"]
    args: ["-c", "while true; do date >> /var/tmp/index.html; sleep 10; done"]
    volumeMounts:
    - name: hexrapp-shared-volume
      mountPath: /var/tmp
  - name: hexrapp-server
    image: nginx
    ports:
    - containerPort: 80
    volumeMounts:
    - name: hexrapp-shared-volume
      mountPath: /usr/share/nginx/html
  volumes:                  
  - name: hexrapp-shared-volume
    emptyDir: {}
  containers:
	   - command: ["/bin/sh", "-c", "while true; do sleep 30; done"]
	    args: ["-c", "while true; do echo $(date) >> /var/tmp/hexrapp.log; sleep 1; done"]
```

> Here in this image, if i have to perform any actions such as passing cmds, running processes, functions of jobs. it is possible using `command` param under `spec`. 
> 
> This can be a single or a combo of commands. It is more like **`ENTRYPOINT`** in Dockerfile, if we compare the same with docker. As same, there is an another named **`args`** param which works similar to **`CMD`**.

`command: 
	["/bin/sh", "-c", "while true; do sleep 30; done"]`
`args: 
	["-c", "while true; do date >> /var/tmp/index.html; sleep 10; done"]`

**Decoding this param:**  
**`command`**: helps us access the shell of that image. **/bin/bash**
**`args`**: runs cmd's inside that image since we accessed the shell. in this, we are looking forward to run a command every ten seconds. to apply that logic,
**running an infinity while loop running a command with setting up a timer.**
    **`args`**: ["-c", "while true; do date >> /var/tmp/index.html; sleep 10; done"]
1. `"-c", `= indicates that we are passing cmds
2. `"while true` = "cmd of while loop starting and declare the same where it is true in condition. 
3. `;`is followed by next steps.
4. `do date` = do  - passes command that whatever we declare. here we pass date. 
5. `>> /var/tmp/index.html;`  = passing that to where?, >> - towards a file or dir. 
6. `sleep 10;` - sleep command helps pause running a command for the time we define. here 10 seconds. every 10secs the command gets executed.
7. `done"` - closing that loop following by.

Now, lets check the rest.
```
kubectl get po -o wide #checking pod status
curl ip #check output, prints date and time every 10secs
kubectl describe po podname #shows all the details
```
while describing the pod:
`kubectl describe po podname` shows all the container details. We have spun up two containers, **one generates data** and **one serves it**. 
- 1 container runs a while loop printing date every ten seconds passes it to the **/var/tmp/index.html** file. whatever that stores in that `index.html` file (which that i sotre it in that volume), gets used on.
- another 1 container which prints that data to web.
-> this became possible with the help of `volumeMount` which shares data in between.
>THE MULTI-CONTAINER POD WITH VOLUME SHARING HAS BEEN SUCCESSFULLY DEMONSTRATED.

---
####  10. Handling containers in POD using crictl(restart container POD)
###### Handling containers in Pod using crictl - [Doc](obsidian://open?vault=tutorialHell&file=Orchestration%2Fk8engineers.com%2FKubernetes-Deep-Dive%2Foffl-raw-docs%2Fkubernetes%20deepdive%2Fsec3-k8s-pods%2F6.Handling%20containers%20in%20Pod%20using%20crictl_.docx)
Will re-spin a container. Instead of deleting, will recreating the same. So that we can't loose any data since they are coupled with each other.
```
kubectl describe po -o wide
kubectl describe po podname
```
in containers section, restart-counts shows zero for both the containers FYI.  will see count after restart. it will be counted. 

HOW TO RESTART A CONTAINER: (told you that container related actions can be performed by `crictl` since it is managed by `containerd` which is a cri itself.
```
crictl ps #shows running container details
crictl stop <containerid>
crictl rm <containerid>
crictl ps
```
 If i perform removing or deleting a container, which it wont get deleted. IT'S DESIRED STATE.
```
kubectl describe po podname 
```
check container and shows the restart has been occurred. But not supposed to losse data still. CHECK!
```
kubectl get po -o wide
curl podIP
```
viola! data is not lost. 
>**BENEFIT OF THE SHARED VOLUME**: is even if i recreate the same containers, the volumes will be persistent and resilient remaining the same!.

**CHECK SHARED VOLUME:**
```
kubectl get po -o yaml | grep uid
sudo ls -l /var/lib/kubelet/pods/<podUID>/volumes/kubernetes.io~empty-dir/<sharedVolInYAML>/index.html 
```

 If the Pod gets deleted, no mounts will get popped up yet. if gets recreated, it will pops back up too under the pod. if
```
kubectl delete po <podName>
sudo ls -l /var/lib/kubelet/pods/<podUID>/volumes/kubernetes.io~empty-dir/<sharedVolInYAML>/index.html 

#will throw: No such file or directory
```
nothing will pop-up. which will not be present in the other side too. 

---
#### 11. Access POD application outside cluster(hostPort)
###### Access pod application outside cluster(hostPort) - [Doc](obsidian://open?vault=tutorialHell&file=Orchestration%2Fk8engineers.com%2FKubernetes-Deep-Dive%2Foffl-raw-docs%2Fkubernetes%20deepdive%2Fsec3-k8s-pods%2F7.Access%20pod%20application%20outside%20cluster(hostport).docx)

**Use case:** ==production like environment making the application publicly availed to the users delivering and assessing application outside the kubernetes clusters where all our applications deployed as a pod.== If i want to access application outside the kubernetes clusters if my applications are deployed as a pod inside a container. HOW TO MAKE THIS PUBLICLY AVAILABLE TO THE PUBLIC USERS. 

In this below given diagram,
- A user outside would like to assess our nginx application for example.
- On the other hand, we have a user inside the node (control plane) assessing our application from inside via the **pod IP** address and **the port number.** -> nginx:80 either into the **control plane node** or the worker **node**. 
*Here, the user inside the cluster can be able to login to the application simply by accessing the PODs IP and via the port number. WHICH IS FINE. BUT*  

**HOW TO DO THE SAME MAKING THAT APPLICATION AVAILABLE TO USER WHO IS SITTING OUTSIDE THE CLUSTER.** **A normal client user how is not a part inside the K8s cluster?**
==Ans: Port Forwarding (just like in docker)== AND ALL OF THESE CAN BE HANDLED AND WILL BE DONE WITHIN THE `PORT` SECTION.

![[Pasted image 20240909175410.png]]
Making this possible by binding the host machine's port the control plane node. Will take port:8090 for example. 

Can make this possible by binding, **Host**'s port number <=> **Control plane node**'s Port Number. 
> **enp0s3Host**:8090 < - = - >  **PodIP**:nginx'sPort80

Will see how to implement the same IRL by deploying multi-container pod and binding it with the host in practice.

These are some of the fields with the description we can implement in the container ports section. writing port section on a yaml file, these fields/parameters are mandatory and to be grasped.

BEFORE WRITING YAML FOR THE IMPLEMENTATION, For **Port** Section, we can write all these below given parameters, 
`containerPort`: int datatype: **Applications Port that is listening on that container**
`hostIP`: int datatype: **The host machine's IP Address.** 
`hostPort`: int datatype: **The host machine's Port** Bind port: this side (left) of the port -> **8080**:80 <- node port that which the application is running. 
THESE HAS BEEN TAKEN CARE OF **`kube-proxy`.** 
`name`: string datatype: entries **name** that can be defined in the port section
`protocol`: int datatype:  Can define **TCP/UDP** here.
WILL SEE ALL THAT BY WRITING ONE.

| Field                    | Description                                                                                                   |
| ------------------------ | ------------------------------------------------------------------------------------------------------------- |
| Container Port (integer) | Port number to expose on the pods ip address<br>This must be a valid port number, 0 < x< 65536                |
| hostIP (string)          | What host IP to bind the external port to.                                                                    |
| hostPort (integer)       | Port number to expose on the host. This must be a valid port number, 0 < x< 65536                             |
| Name (string)            | This must be an IANA_SVC_NAME and unique within the pod. Name of the port can be referred to by the services. |
| Protocol (string)        | Protocols for the port must be UDP, TCP or SCTP. The Default is TCP.                                          |
For practice, we will be putting all these on TOP of our last **multi-container PODs manifests**. 
HOW TO EXPOSE THE APPLICATION OUTSIDE THE K8s CLUSTER: Here,
The app has been running as `nginx` out of K8s Cluster. 
```
apiVersion: v1
kind: Pod
metadata:
  name: hexrapp
  labels:
    app: hexrapp
    type: web
spec:
  containers:
  - name: hexrapp-writer
    image: alpine
    command: ["/bin/sh"]
    args: ["-c", "while true; do date >> /var/tmp/index.html; sleep 10; done"]
    volumeMounts:
    - name: hexrapp-shared-volume
      mountPath: /var/tmp
  - name: hexrapp-server
    image: nginx
    ports:
    - containerPort: 80
    volumeMounts:
    - name: hexrapp-shared-volume
      mountPath: /usr/share/nginx/html
  volumes:                  
  - name: hexrapp-shared-volume
    emptyDir: {}
```

```
#EDIT/MODIFICATION OCCURS UNDER THE `nginx` CONTAINER, `Port` Section
- name: hexrapp-server
  image: nginx
  ports:
  - containerPort: 80
    name: http
    hostPort: 8090
    protocol: TCP 
#this can be left blank too. Proceeds with the default which is TCP itself.
      hostIP: vmorHostIP
#or you can leave this blank since it is default already.
```

All done. Now apply! 
Before proceeding. Do check in prior if there is any port has been already binding with this. Delete the pod we created before if it exists.
```
kubectl get po -o wide
kubectl delete -f po <podyamlFile.yaml>
kubectl create -f <newpodorsameyamlfile.yaml>
```
Cross verify the status
```
kubectl get po -o wide
```

Here,
- If user has to use the app from inside any of the node and `curl` the IP
- If the for a user outside the cluster, 
```
hostIP/8090

#for me, it was like
http://192.168.0.222:8090/
```

Challenges in these:
**Challenge1:** 
If you want to do the same but for a different application similarly or with the same functionality, CANT DO THAT. 
Redeploy or spin up a new application with the same port number on the same node, NO POSSIBLE.
**Challenge2:** 
Now it is all up and running. all good. If i happened to recreate the pod,  Can't guarantee that these will work just fine on the same node. Might run in any of the control plane or compute plane node.
How do i change the IP address in the frontend or to the clients/users outside the cluster.

>NOT A FEASIBLE WAY OR SOLUTION TO LAUNCH OR EXPOSE THE APPLICATION IN OR OUT ON A K8s CLUSTER. 

IRL, WE WONT BE LAUNCHING THESE PODS LIKE THIS BLUNTLY WITHOUT DEFINING ALL THE SPECIFICS THAT WHERE IT HAS TO BE RAN.

> NOT THE RIGHT SOLUTION IN REALTIME. WILL SEE THE BEST PRACTICES IN BRIEF.
---
####  12. POD initContainers Introduction
###### Pod initContainers - [Doc](obsidian://open?vault=tutorialHell&file=Orchestration%2Fk8engineers.com%2FKubernetes-Deep-Dive%2Foffl-raw-docs%2Fkubernetes%20deepdive%2Fsec3-k8s-pods%2F8.Pod%20initContainers.docx)

There are few types of containers that do exist in this K8s Environment, such as:
- **init Containers**
- **Sidecar Containers**
- **Ephemeral Containers**
initContainer - Initializing containers before anything.

We saw how to deploy apps using **pod** and exposing the same in and out of the K8s Cluster by defining all that in a container section on the PODs Manifest YAML file.

Now, will see another section inside that POD - **`initContainer`** Pod.
###### initContainer Pod:
What it is? Purpose? Benefits?
- **initContainer** will run/create before **app container** defined under containers section.
> - when we write pod manifest, we write specs and under that, will define all the containers. 
> If we does the same ==writing **initContainers** defining **spec** section ==and containers under it, **the initContainer will be created first**, after that the rest of them gets spined up. REASON TO CALL `initContainer` as initializing containers.
> ==**THE FIRST CONTAINERS GETS CREATED WHEN WE LAUNCH A POD.**==

- `initContainers` will not be in running state. it will be shown as completed state and gets terminated showing exit code as 0.
-**`initContainers`** always run to completion.
-Each **`initContainer`** must complete successfully before the next one starts.
whenever you launch a pod with **`initContainers`** written in it.
- Work similar to regular containers but each `initContainer` must be completed successfully before next one starts. One at a time one by one completely. 
-e.g.: In a pod manifest writing a multi-container pod configuration, you declare one initContainer and one regular container with all the same specs defined. `initContainer` will get created first one by one, after that the rest will be done.
- if `initContainer` fails, **kubelet** will keep on restarts the `initContainer` until it gets succeeded unless if there any `restartPolicy` **= never** applied to it.
- all the specs and stuff which has been applied to regular containers can be applied to `initContainers` too. except **probes** and **lifecycle**. 

###### initContainers Use cases:
how this is beneficial to us?
1) to do some initial setup before actual applications/containers to get deployed. e.g.: for an nginx application, i make changes or modify some parameter in the containers and such. SUCH ACTION CAN BE APPLIED in the **`initContainers`** since these things will get launched first.  
2) to run scripts, code and processes as a part or before the initialization of the actual application get created or deployed. things which are operational intensive and not has to be a part of a regular application deployment can be declared here as a separate process or as a job.  
3) to do kernel changes or application specific uses cases such as running distroless containers and such. 
4) `initContainer`s can be ran n number of times. No strings attached, no restrictions just like regular containers. 

---
####  13. POD initContainers

###### How to write one? 
**NO ROCKET SCIENCE!** 
**JUST SIMPLE PARAMETERS GETS CHANGED HERE!** will see that in practice. 
Will get this by referring the diagram and understand how this `initContainer` works inside a POD in detail.
![[Pasted image 20240911181845.png]]
Here we are trying to create a Pod which has its own IP that communicates to your application where the frontier container is `nginx` here who serves content by acting as a server listening port 80. this how the communication happens. 

Here, we have an another container. WHY IS THIS HERE AND WHAT IS THE PURPOSE OF IT. 

**Use case**: It has to contact my github, pull essential files and has to be stored in **/var/tmp** directory inside the container. 
WHICH IS MOUNTED TO THE **SHARED VOLUME** by default and it will be existed here too in the mounted directory which is
**/var/lib/kubelet/pods/'podUID'/volumes/kubernetes:emptyDir/**. 
	Once the process/job of pulling and storing the same gets completed, the **`initContainer`**'s status will be shown as and goes into completed state and gets terminated showing exit code as 0. IT IS A ONE TIME PROCESS AND AFTER IT GET COMPLETED, WE COULD ACCESS THE SAME.
where the `nginx`container will be shown as **Running** state in status since it is an application container.

Since the shared volume has the data which has been spawned by the `initContainer` which be seen in the `nginx` container's `WORKDIR` which is **/usr/share/nginx/html** too. which end up delivering the same content on the server too.
###### writing the same in a .yaml file. all the same but xtra parameters for initContainers:
```YAML
apiVersion: v1
kind: Pod
metadata:
  name: init-container-pod
  labels:
    app: init-container-pod
    image: nginx
    type: web
    object: initContainer
spec:
#container-section
  containers:
  - name: web-container
    image: nginx
    imagePullPolicy: IfNotPresent #always, IfNotPresent, Never
    ports:
    - containerPort: 80
      name: http
      protocol: TCP
      hostPort: 8091
      hostIP: 192.168.0.222
    volumeMounts:
    - name: init-container-volume
      mountPath: /usr/share/nginx/html
#initContainer-section
#these containers will run first during pod initialization and will be stopped after pod creation
  initContainers:
  - name: init-container
    image: alpine
    command:
    - wget
    - -O
    - /var/tmp/index.html
    - https://github.com/sudheerduba/initContainer_demo/blob/main/index.html?raw=true #copy paste url + add ?raw=true to get raw content
    volumeMounts:
    - name: init-container-volume
      mountPath: /var/tmp
#where these containers secton has been written under spec section
#volumemount-section
  volumes:
  - name: init-container-volume
    emptyDir: {}
```
> - Will passing this pod manifest, `initcontainers` will get launched and gets into **completed** stage and done. 
> - After this one gets completed successfully, the rest of the containers will get launched and will be in **running** state.

**Here, will specifically focus on `initContainer`** 
```yaml
  initContainers:
  - name: init-container
    image: alpine
    command:
    - wget
    - -O
    - /var/tmp/index.html
    - https://github.com/sudheerduba/initContainer_demo/blob/main/index.html?raw=true #copy paste url + add ?raw=true to get raw content
    volumeMounts:
    - name: init-container-volume
      mountPath: /var/tmp
```

**Let see the same in practice passing this manifest to the cluster**: where we seek to launch two containers with a shared volume. one complete launching an `initContainer` first pulling a repo and and put that in its mounted working directory to get the container to a completed state and a regular `nginx` container where it serves the data which has been pulled and spawn by the `initContainer`.
```
apiVersion: v1
kind: Pod
metadata:
  name: init-container-pod
  labels:
    app: init-container-pod
    image: nginx
    type: web
    object: initContainer
spec:
  containers:
  - name: web-container
    image: nginx
    ports:
    - containerPort: 80
      name: http
      protocol: TCP
      hostPort: 8091
      hostIP: 192.168.0.222
    volumeMounts:
    - name: init-container-volume
      mountPath: /usr/share/nginx/html
#these containers will run first during pod initialization and will be stopped after pod creation
  initContainers:
  - name: init-container
    image: alpine
    command:
    - wget
    - -O
    - /var/tmp/index.html
    - https://github.com/sudheerduba/initContainer_demo/blob/main/index.html?raw=true #copy paste url + add ?raw=true to get raw content
    volumeMounts:
    - name: init-container-volume
      mountPath: /var/tmp
  volumes:
  - name: init-container-volume
    emptyDir: {}
```
```
kubectl create -f initContainer-pod.yaml
```
pod gets created. verify the POD
```
kubectl get po -o wide
curl <PodIP>
```
Here, there is no sign of the `initContainers` right? to verify the same, do
```
kubectl describe po <PodName>
```
 showing:
     State:          Terminated
      Reason:       Completed
      Exit Code:    0
which means the initContainer 's objective has been successfully met. Job is done and gets deleted. Also, the events can be seen there too telling what are all have been happening over the process. To see the process in brief:
```
kubectl logs <podname> -c <initcontainername>
```
*Connecting to github.com (20.207.73.82:443)*
	*Connecting to github.com (20.207.73.82:443)*
	*Connecting to raw.githubusercontent.com (185.199.109.133:443)*
	*saving to '/var/tmp/index.html'*
	*index.html           100% |******************************| 52  0:00:00 ETA*
	*'/var/tmp/index.html' saved*
```
curl 10.244.114.58 
```
If any occurs while configuring the `initContainer` pod, it will show like
```
NAME             singlenode22                               
READY            0/1
STATUS           Init:0/1 or Error
RESTARTS         0 or 1(3s ago)
AGE              5h26m
IP               10.244.114.58
NODE             singlenode22 
NOMINATED NODE   none
READINESS GATES  none
```
can verify the same errors in logs by giving,
```
kubectl describe po <podname> 
kubectl log <podname> -c <containername>
```
describe: shows what sup! events, errors, state, reason, logs and the exit code:1
logs: shows what sup with the resources specifically.
> and troubleshoot the same, make changes and redeploy!
```
kubeetl replace --force -f initContainerPodManifestFile.yaml
kubectl get po -o wide
```
and you are good to go and check by `curl`ing it!

---
####  14. POD Lifecycle: restart policy
###### Pod Lifecycle Restart Policy - [Doc](obsidian://open?vault=tutorialHell&file=Orchestration%2Fk8engineers.com%2FKubernetes-Deep-Dive%2Foffl-raw-docs%2Fkubernetes%20deepdive%2Fsec3-k8s-pods%2F9.Pod%20Lifecycle%20Restart%20Policy.docx)

same spec but more about `restart-policy` just like in `docker-compose`.
>THE `pod-restart` policy will be defined on pod level, not on container level.

**What are all the supported policies that we have?**
1) never
2) always - default
3) On Failure

| Restart Policy    | Description                                                                                                                                                      |
| ----------------- | ---------------------------------------------------------------------------------------------------------------------------------------------------------------- |
| --restart=Never   | Containers will not be restarted by kubelet due to any exit code >=0. Kubelet won't restart if app fails to work.                                                |
| --restart=Always  | Kubelet will try to restart containers exited with >=0 (failed or stopped due to any reason). To avoid downtime due to any failure caused.                       |
| --restart=Failure | Containers gets started by kubelet only if it gets failed with exit code > 0. Only if any failure. Applies where we need to find the root cause of the failure.  |
**We have few things to be consider when we work with `restart-policy`**:
1) Backoff Delay - restarts containers for every **10s, 20s, 30s..5mins,** if it throws exit code either 0 or 1.  
2) After restarting under backoff delay, it'll watch the container where the event has occurred for **10mins** whether it fails back or not. after 10mins without any fail, backoff delay gets turned off and backs up to 0 and do the same. 
>THIS IS HOW `kubelet` HANDLES THE `restart-policy` FOR OUR CONTAINERS.

> [!NOTE] Note
> 1) Kubelet restarts container with exponential back-off delay of 10s, 20s, 30s
> 2) Kubelet resets the backoff timer for the container, if no problem found for 10mins
###### Will see how to write/apply one: 
##### `always`
(if nothing declared, default policy will get applied with is :`always`) here's the manifest of launching a multi-container pod.
```
apiVersion: v1
kind: Pod
metadata:
  name: hexrapp
  labels:
    app: hexrapp
    type: web
spec:
  containers:
  - name: hexrapp-writer
    image: alpine
    command: ["/bin/sh"]
    args: ["-c", "while true; do date >> /var/tmp/index.html; sleep 10; done"]
    volumeMounts:
    - name: hexrapp-shared-volume
      mountPath: /var/tmp
  - name: hexrapp-server
    image: nginx
    ports:
    - containerPort: 80
      name: http
      hostPort: 8090
      hostIP: 192.168.0.222
      protocol: TCP
    volumeMounts:
    - name: hexrapp-shared-volume
      mountPath: /usr/share/nginx/html
  volumes:                  
  - name: hexrapp-shared-volume
    emptyDir: {}
```
In this multi-container pod, one being a writer saving the data and one serves it the web. since, we haven't applied any `restart-policy` to these. So, **always** as default. 

To check that, simply apply the same and see,
```
kubectl create -f podManifestFile.yaml
kubectl get po -o wide
kubectl describe po <podname>
```
scroll down and you'll find no `restart policy` under that. lol. also in the pod level too. nothing, You will find that printing that YAML of the same.
```
kubectl get po -o yaml 
kubectl get po podname -o yaml | grep restart
```
in the container section, under the spec of it, will find that. 
	restartPolicy: Always

HENCE PROVED THAT IF NO restart-policy is declared, it goes with the default which is `Always`. 
-> WHAT DOES IT MEAN?  Failure by any means exit by 0, exit 0, whatever 0, it will always restarts the container. 

###### check this by causing a failure to this pod:
to do that, will make a slight change in the `yaml` file
```
spec:
  containers:
  - name: hexrapp-writer
    image: alpine
    command: ["/bin/sh"]
    args: ["-c", "while true; do date >> /var/tmp/index.html; sleep 10;exit 0; done"]
```
```
    args: ["-c", "while true; do date >> /var/tmp/index.html; sleep 10;exit 1; done"]
```
which prints the date and terminates the container. 
WITH THIS LOGIC APPLIED, THE CONTAINER MUST RESTART DUE TO IT'S `restart-policy` 
```
kubectl create -f podmanifestfile.yaml
kubectl get po -o wide
kubectl get po -o wide -w
kubectl describe po <podname> 
```
Throws: running and notReady one on one and can see by describing exit code, restart count and policy  proved!
> => IT JUST KEEPS ON STARTING THE CONTAINER ALWAYS.

##### `on-failure;`
JUST ADDING ONE MORE PARAMETER which is `restart-policy` :

> [!NOTE] Title
> these `restart-policy` should be declared only under the spec section NOT THE CONTAINERS. Can be declared above or below container section under spec.

```
spec:
  restartPolicy: OnFailure
  containers:
    args: ["-c", "while true; do date >> /var/tmp/index.html; sleep 10; done"]
  ...
```
restarts if container on failure by any means. 

##### `never;`
JUST MODDING THAT ONE PARAMETER which is `restart-policy` :

> [!NOTE] Title
> these `restart-policy` should be declared only under the spec section NOT THE CONTAINERS. Can be declared above or below container section under spec.

```
spec:
  restartPolicy: Never
  containers:
    args: ["-c", "while true; do date >> /var/tmp/index.html; sleep 10; done"]
  ...
```
never restarts if anything occurs.

---
####  15. Static POD (controlled by Kubelet)
###### Static Pod (Controlled by kubelet) - [Doc](obsidian://open?vault=tutorialHell&file=Orchestration%2Fk8engineers.com%2FKubernetes-Deep-Dive%2Foffl-raw-docs%2Fkubernetes%20deepdive%2Fsec3-k8s-pods%2F10.Static%20Pod%20(Controlled%20by%20kubelet).docx)

Understanding Static Pod for deploying applications on K8s:
>STATIC PODS -> Node centric Pods, the **kube-components** that we use such as etcd, kube-controller, scheduler, and apiserver POD are handled by the node itself, not by `kubectl` are handled by that are in such of these containers.

- Static Pod (is node specific) will be managed by **kubelet daemon** on the appropriate node.
-- so far we've been working pods by communicating the same by passing manifests files via `kubectl` or passing API Calls. Who are accessing and receiving the same? `kube-apiserver`, that validates and scheme checks and passes it to store in `etcd` and to `scheduler` and to compute node's `kubelet` to `cri` to `containers`
- Static Pod will **not be controlled** by kube-apiserver (kube API Calls), by kubelet daemon which runs on specific node.
- Deleting Static Pod through API or Kubectl will not effect the application, where kubelet will recreate it. Even from controlling that POD via `kubectl`, nothing works.
- Static POD names will be added with suffix of node name. 
-- static pod = (podName + nodeName) can understand that it is a static pod. 
	- These Static POD manifest can be defined through **Filesystem-hosted** or **Web-Hosted**, will see such implementations further.
- Kubelet can read default directory `/etc/kubernetes/manifests`, If we place manifest file (either JSON or YAML) to create static POD for **Filesystem-hosted** approach. All the static pod manifest lies here, so whatever file that we put here, the `kubectl` will consider the same as static pods. since it is the default directory for such kube components. => THIS APPROACH OF CREATING STATIC PODS is **Filesystem-hosted** approach.
- Kubelet can download manifest file from the by passing `--manifest-url=<URL>` argument for **Web-hosted** approach. Somewhere in the web, there is a static pod manifest file. If i
```
kubectl create -f --manifest-url=<URL> 
```
downloading that using this and create pod, is an other approach of creating **Static Pod** using Web Based Approach.

---
####  16.Challenges of standalone POD applications
###### Challenges of Pods - [Doc](obsidian://open?vault=tutorialHell&file=Orchestration%2Fk8engineers.com%2FKubernetes-Deep-Dive%2Foffl-raw-docs%2Fkubernetes%20deepdive%2Fsec3-k8s-pods%2F11.Challenges%20of%20Standalone%20Pod.docx)
 Will see about PODS challenges.
The Challenges:
- POD is a basic entity to run application on K8s Cluster. 
> Knowledge about PODs are mandatory in-order to work with or deploy applications or other vectors of concepts such as services, deploymentSet, ReplicaSet, Statefulset, DaemonSet, jobs, Cron, controllers on K8s which ultimate contribute to such POD CREATION process and such where POD is the entity that will be running all these services and the application, so knowing in brief about **PODs** is a basic requirement to get started with.
- POD works as a standalone application in K8s Cluster. Work standalone which it doesn't rely on anything else but `kubelet` in order work in ease.
- PODs are not bind/bounded to specific node (unless we use **nodeName**, **nodeSelector**, **nodeAffinity** etc..,) 
> e.g.: ill be trying to launch a pod, request will be sent to scheduler deciding which node it should deploy the pod in.  If i delete and relaunch the same pod, can't guarantee that i get the same IP and to same node to get deployed into. Unless you register that manifest with parameters like  **nodeName**, **nodeSelector**, **nodeAffinity** etc.  
- POD Lifecycle is not managed by K8s Controller. 
> -- If pod gets deleted by any failure, went wrong while creating, throws error or gets failed and such, K8s wont recreate it. You should be recreate that (with some programmatic and restart policy exceptions). 
- For application to be deployed with HA, POD object is not a right solution
> -- e.g.: For HA Setup, POD object/Standalone isn't the right solution. Instead we should go for ReplicaSet, Replication Controller, Deployment, DaemonSet and such. 
- In real-time environments of K8s Cluster, we don't deploy application as standalone POD
> - no one is going to deploy applications as a standalone pod. Any K8s real time environment, no one is going to use simply standalone PODs in Production. will be using ReplicaSet, Replication Controller, Deployment, DaemonSet and such. due to specific but resilient reasons.
- We should always define containers in POD are application specific. one for one seperately, not to the whole in one.
- Failed PODs should be deleted by us through kubectl or API (**terminated-pod-gc-threshold**)
> -- e.g. if a failure occurs, it will be shown and it will exist in get command unless we remove it. doesn't get removed automatically. unless `terminated-pod-gc-threshold` which is a garbage collection condition is applied. threshold = grace period. 
- If a **Node** dies, the PODs scheduled to that node are **`scheduled for deletion`** after a timeout period. merely 30secs.

These are all the challenges met in standalone PODs.


