- Kubernetes Objects overview
- Kubernetes Overview & Pod Introduction - Doc
#### 2. Â Workload resource replication
###### Kubernetes workload resource replication - [Doc](obsidian://open?vault=tutorialHell&file=Orchestration%2Fk8engineers.com%2FKubernetes-Deep-Dive%2Foffl-raw-docs%2Fkubernetes%20deepdive%2Fsection4-workloads%2F2.%20Kubernetes%20workload%20resource%20repliaction.docx)

So far, We've covered **Workload** and **Workload Resources** and its types. 

**What is Workload resource replication in K8s?**
1) **RELIABILITY**:
To ensure that we have minimum number of PODs running on the cluster for application to be accessible without downtime if any. Any problem with the node where the relevant pod is running; 

eg: we have a standalone pod running on any one of the compute plane pod. Since you know the drawback for standalone pods is that, The pod gets terminated if any failure case by any means. Few minutes after, The respective node gets spawned. 
what happens next? Won't come up automatically. Its is manual passing kubectl or APICalls in order to spin up all these containers in the POD to run in it. which is not in control by K8s itself. THESE ALL WE KNOW!

> Here, i would like to bring in the concept called **`replication`**. which is nothing but, instead of running one single pod,
> RUNNING APPS on MULTIPLE PODs ON THE CLUSTER with more specialized metrics.

eg: CREATING MULTIPLE PODs. 
POD1 , POD2 AND POD3. These pods may or may not run on same, different or on all node whichever `kube-scheduler` chooses the same to deploy such PODs.

> If any one of the Pod goes down for instance. Since we are discussing concept named `Replication` using any of the replica based workload resource such as `replication-controller`, `replicaset` or `deployment`.
> These POD1, 2 and 3 where creating and controlled by them. If the pod gets deleted, automatically these **replication components** will spin up the same in ease with the parameters of **actual and desired state**. eg: if the desired is 3 but exist count is 2, K8s `controller manager` will tap in and send that msg to  `kube-scheduler` to schedule pods to the desired state equals to the actual state.

THESE PRACTICE = **==`REPLICATION`==**, replicating the containers. Meanwhile, the entities that gets replicated is only the **application** not the **data**. Replicating only the template. All the pods that are getting created by your **replication components** tries to ensure that  all the pods get the same container template not the data. The data replication part do exist and it is possible but that's for another topic. All pod should spinned up with the same template which same resource and specs but not the data.

=> THAT IS CONCEPT OF **`REPLICATION`**

 Here in the concept of ==**`reliability`**==, ensure that minimum number of PODs running on the cluster for the application to run without any downtime by any means.
> In the case of LB for these 3pods and if one gets failed, the traffic gets distributed to the rest of those two pods. 

2) **LOAD BALANCING**:
We can add **service object** to Load Balance the traffic so that load will be generated on specific POD running on same node or different node.

Load Balancing is an in-built functionality of K8s which is simply as **`service`**. 
what is K8s service in the nutshell: Service as the name same takes up a service in front and make it to take care of certain jobs. Static resource and id management for PODs, DNS, IPaddr, LB and more to be on the forefront.

BOTH COMBINED, WE CAN SPIN N NUMBER OF PODS WITH `REPLICATION`
AND HANDLE N NUMBER OF TRAFFIC DISTRIBUTED EQUALLY TO ALL THESE PODS WITH `LB`. By using `service` concept.

3) **SCALING**:
If we need to increase or decrease the number of POD for the replicas we can do it on. FYI. 
> as the name says, Scaling Pods.

eg: Three pods. application can handle enough traffic. Later, experiencing an unexpected spike? LB distributing to only one where two got failed. Now, the one is literally getting choked with traffic. 

Here, we can do the replicas in ease on the fly. 

Simply can able to spin up 
- number of pods
- wider the spec
who's gonna do the scale in and scale out? 
Similar to AWS Auto-scaling.

> ALL OF THESE POINTS ARE THE MAIN HIGHLIGHTS THAT WHY REPLICATION AND STUFFS MATTERS!

`KUBERNETES WORKLOAD RESOURCES`
****

 

