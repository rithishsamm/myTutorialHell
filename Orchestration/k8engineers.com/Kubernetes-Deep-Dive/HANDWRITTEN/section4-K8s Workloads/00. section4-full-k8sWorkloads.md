## Section 4: K8S Workloads
### 1. Workload resources introduction
#### Kubernetes Workload Resources introduction - [Doc](obsidian://open?vault=tutorialHell&file=Orchestration%2Fk8engineers.com%2FKubernetes-Deep-Dive%2Foffl-raw-docs%2Fkubernetes%20deepdive%2Fsection4-workloads%2F1.%20Kubernetes%20Workload%20Resources%20introduction.docx)
#### 2.  Workload resource replication
#### Kubernetes workload resource replication - [Doc](obsidian://open?vault=tutorialHell&file=Orchestration%2Fk8engineers.com%2FKubernetes-Deep-Dive%2Foffl-raw-docs%2Fkubernetes%20deepdive%2Fsection4-workloads%2F2.%20Kubernetes%20workload%20resource%20repliaction.docx)
---
##### Section 4: K8S Workloads
### 1. Workload resources introduction
###### Kubernetes Workload Resources introduction - [Doc](obsidian://open?vault=tutorialHell&file=Orchestration%2Fk8engineers.com%2FKubernetes-Deep-Dive%2Foffl-raw-docs%2Fkubernetes%20deepdive%2Fsection4-workloads%2F1.%20Kubernetes%20Workload%20Resources%20introduction.docx)

**K8s Workload resources**:
So far, we've been covering all things about **POD**s. which itself means **Workloads**. 
	Since, we know that a standalone POD running alone in the environment is dangerous, not a best practice and volatile if any major downtime occurs to the node. No workload will be persistent and present after node's recovery. -> the drawbacks of standalone pod, 
	-
	Container and stuff managed by `kubelet` are all fine but no POD creation process is done here. and for node if gets down. youre cooked!
	-> Managing PODs are manual intensive job managing all things manually. 
	-> **can't and should not deploy critical application for my prod.**

**What are all the solutions available in K8s to evade this? what are all the resources available to provision my application instead of deploying things standalone?** -> ==**Workload Resources,**== 

what is that? and how it works different than POD?
- **If standalone**: all things manual
- **If with Workload Resources:** 
Way things works is different, such as 
- Functionality,
- handling the PODs 
- Lifecycle policy is all different.

So far, we been discussed about Workloads, means POD. 
>Previously  we have touched a note that,  
>IT IS NOT A BEST PRACTICE OR PEOPLE DON'T DEPLOY APPLICATIONS ON A SIMPLY STANDALONE, were they are very prone to failures caused by any means. can occur via system error, network error or application error.

>AFTER SUCH CASES? AND THINGS GET RECOVERED AND THE NODE GETS SPAWNED? Will all the pods with our application exists on our node?
**NO! create manually by passing the same to kubectl** -
==DRAWBACK OF STANDALONE PODS== ALL THINGS CONTAINERS ARE TAKEN CARE BY `kubelet` no prob. WHAT IF THE WHOLE NODE GETS BRICKED?

**Concern**: we cannot deploy critical or any applications in such a loose environment.
so Solution? There are solutions. These concerns can be satisfied with the help of a set of **==WORKLOAD RESOURCES==**. 
IS IT DIFFERENT FROM POD? **YES**!

**If standalone**: all things manual
**If with Workload Resources:** 
Way things works is different, such as Functionality, handling the PODs Lifecycle policy is all different. Will see all of that in brief.

What are all the resources available provisioning the same? All the available resources to do manage PODs? 
Context: (replica = multiple identical pods) by `kube-controller manager`.

K8s **Workload Resources** Introduction:
> [!workload resources:] **WORKLOAD RESOURCES:**
> **1) ReplicationController** 
> **2) ReplicaSet**
> **3) Deployment**
> **4) CronJobs**
> **5) DaemonSet**
> **6) Statefulset**
> **7) Jobs**
> **8) (followed by Jobs) -> Auto Clean-up for finished jobs**
VERY VERY IMPORTANT SHIT THAT MANAGES YOUR RESOURCES:

##### 1) `ReplicationController`: utmost first basic set of workload resource
**tldr**: `ReplicationController` - *basic resource to run multiple pods*

 **`ReplicationController`** is a basic choice that helps running multiple pods.  Replicate running identical multi-container application pods.  
In near future, the `ReplicationController` might get deprecated. Alternatively, `ReplicaSet`.

##### 2) `ReplicaSet`: replication-controller on steroids
**tldr**:  `ReplicaSet` - alternate or upgraded version of `ReplicationController` (as same as `rc` except `selectors` which selects labels)

Instead of `ReplicationController`, **`ReplicaSet`** is the go to solution for the same. **ReplicaSet** is a highly recommended to use for the same instead of ReplicationController.  Why? **`ReplicaSet`** is used in the backend with a resource named **`Deployment`**.

Moreover both **`replicaset`** and **`controller`** both in terms of functionality and workings are same **except selecting the labels** - known as `selectors` which selects labels.
> `ReplicaSet` is the advanced feature of `replication-controller`. To use the same, we should be working with `Deployment`.  why?

##### 3) `Deployment`: 
**tldr**: 3) `Deployment`- `replicaset` is used in backend  by `deployment` to create replicas for scale in and out + rollback and rollout versions upgrade and downgrade. can be used in production level. (!important)

**`Deployment`**: the basic component which we use to deploy applications on one or more than one pods.
- scale in scale out pods based on load
- roll back roll out (upgrade and downgrade applications)
In K8s Realtime environment, by default applications gets deployed based on the `deployment` component. not the components which are replica set, controller or pod. 

 `Deployment` is using `ReplicaSet` in the backend to create and deploy replicas with the help of defining actual and desired state. 
 > WTF IS EVEN A REPLICA? -> **==Creating Multiple Pods ad handling them by `kube-controller-manager` ==**, If anything goes wrong with the pod like failure, crash, brick, failed or even deleted either intentional or unintentional. 

 K8s will check a parameter called `desired-state` and `actual-state` in the pod creation manifest. With that, if anything occurs getting created or deleted above or below abiding that states. 
> Deletes if more than desired number gets created or created if there is below that desired number.

THESE ARE MANAGED BY K8s. These works with the help of Replica concept in the backend. Will see more about these in brief.

##### 4) `DaemonSet` : running pods in each and every node in the cluster including replication-controller for specialized use cases.
**tldr**: `DaemonSet` - running at least one POD on each and every node in the cluster, eg: monitoring and stuff. helps running applications on relevant pods in each and every node in the cluster including `replication-controller`. 

Ensures running at least one pod on each and every node. 
> **why or for any specialized purposes?** running pods for monitoring and observatory purposes, such as monitoring, logs, application, load, traffic and more.

For such cases, we will be deploying applications on kubernetes clusters using `DaemonSet`. will see further in detail.

##### 5) `Statefulset`: This way of deploying applications using Statefulset 
**tldr**: `Statefulset` - all these above seen objects are stateless, `statefulset` is to run stateful apps.

All the above covered K8s **Workload Resources** such as `ReplicationController`, `ReplicaSet`, `Deployment` and `DaemonSet` are all **stateless**. You can proceed deploy application statelessly, these are the choices. 

If you want to deploy things applications state fully, `statefulset` is the go to choice. 

##### 6) `Jobs and CronJobs`
**tldr**: `Jobs` /  -  run a specific script/job/program/process 
Jobs and CronJobs are nothing but to run a specific job, batch operation, script, function or process. 
 `CronJobs` - same jobs but run it on a specific time. for purposes such as backups, restores, updates or anything you'd like to do on K8s Cluster.
 
Used for cases such as taking backups, snapshots, restoration and more -Jobs and Cronjobs.
**Job** - Runs a specific task
**CronJobs** - Run specific tasks on specific time or interval. 

##### Auto-cleanup for finished jobs:
**tldr**: (followed by Jobs) -> Auto Clean-up for finished jobs (workspace cleanup)
A functionality. As the name says as you know, cleans up the Jobs leftover. To remove/delete the jobs that are done doing such tasks - `Auto-cleanup for finished jobs`. If not cleaned up, they will persist still in the garbage collection in **`etcd`**. This process named Garbage Collection. 

> ALL THESE **`WORKLOAD RESOURCES`** MENTIONED HERE ARE ALL NOTHING BUT EXIST HER JUST OF ONE CORE MAIN OBJECTIVE ULTIMATELY IS TO **CREATE PODS**. 
> Thou creating pods, which are made possible with the help of all these `WORKLOAD RESOURCES`. - CREATING BUNCH OF PODs in a certain manner.
Not simply a blatant standalone PODS which are named `WORKLOADS` To run all our applications either way.

Will see all of these in detail further. 

---
#### 2.  Workload resource replication
###### Kubernetes workload resource replication - [Doc](obsidian://open?vault=tutorialHell&file=Orchestration%2Fk8engineers.com%2FKubernetes-Deep-Dive%2Foffl-raw-docs%2Fkubernetes%20deepdive%2Fsection4-workloads%2F2.%20Kubernetes%20workload%20resource%20repliaction.docx)

**K8s Workload resource replication introduction**
So far, We've covered **Workload** and **Workload Resources** and its types. 

**What is Replication means for K8s Workload resources? and benefits?**
1) **RELIABILITY**:
To ensure that we have minimum number of PODs running on the cluster for application to be accessible without downtime if any. 

eg: 
we all know about standalone PODs. If a POD running on a node and the node fails and gets back, the PODs are no more, nothing gets back automatically. we gotta spin back up and run the same in fresh . we know all these clutter and challenges doing all things manually.

What if, POD creation is automated and taken care of K8s under the replication concept? How this replication thing works? 
> ==Instead of running one standalone PODs, what if we run these PODs in multiples on the same K8s Cluster with more specialized metrics.!==
This is called **`replication`**
 
eg: CREATING MULTIPLE PODs. 
POD1 , POD2 AND POD3. 
These pods may run on same or different or whatever nodes we have `kube-scheduler` chooses the same to deploy such PODs unless we specifically point it out. even in three different nodes.

These POD 1, 2, and 3. each of one running on three different worker nodes. If any one of the PODs from a random node goes down, With any one of the workload resources uses the concept`Replication` .
Resources like
- replication controller `rc`
- replica set  `rs`
- deployment `deploy`
> if anything gets down or deleted, this spawns the same but new workload will be running on the same or different node.
>  **this `replication` concept consist of a good feature which maintains a desire state as per our convenience. here the parameters name replica=3, actual=3. If mismatches, K8's`kube-controller manager`  comes into picture and maintains the same state no matter what, unless we intervein.** 

THESE PRACTICE = **==`REPLICATION`==**, replicating the containers. Means, these will replicate only the **application** not the **data**. 

if nginx1, 2, 3's `index.html` are each on its own, it wont change the data on its own, changes means replicates only the `spec` template. 
Same 
- name
- image
- port
- resource allocation
	will cover the same replication concept for the data for an other day!

where it ensure running minimum number of pods running on the relevant nodes no the cluster for the application to run without any downtime. 
Additionally, If we put up an `lb` ==Load Balancer== *which is in-built and default*, it simply distributes traffic to POD1, 2 , 3  on repeat. If POD1 goes down, distributes the traffic to POD2 and 3. UNTIL POD1 RECOVERS, NOTHING IS DISRUPTED AND GETS BACK ON TRACK. which is no way possible with the standalone pods. 
> THIS WAY WE ACHIEVE THE **==RELIABILITY==** FACTOR.

2) **LOAD BALANCING**: 
An inbuilt K8s thing. This will fall under the name of `Service` as workload resource name.
`service`? -> K8's inbuilt functionalities are known as **service**.

Typical logic that applies to the concept of  `load balancer`.
as per previous example. running 3pods with an `LB` configured. 

Without `LB`: 
If we run the same without `LB`, user will be pinging to each of the POD one on one with uneven distribution. If one gets down and gets recovered, not guaranteed that you'll end up having same IP. can't configure the IP one on one every time it gets down to get the distribution working.

With `LB`:
User will be pinging to one IP address which gets resolved by the `LB`'s DNS which makes the distribution working even PODs gets up and down or recreated which changes IPs everytime when fails. `LB` deducts and resolves the same not the APP itself everytime to get the user to access the same.
>From the `LB` the traffic gets distributed to multiple PODs even with the above seen scenarios.

BOTH COMBINED, WE CAN SPIN N NUMBER OF PODS WITH THE HELP OF `REPLICATION`. AND HANDLE N NUMBER OF TRAFFIC DISTRIBUTED EQUALLY TO ALL THESE PODS WITH `LB`. By using `SERVICE` concept.

> THIS WAY WE ACHIEVE BOTH **==RELIABILITY==** + `LB` ==LOAD BALANCING== FACTOR.


3) **SCALING**:
As the name speaks for itself. SCALING.

eg:
As we deployed POD 1,2, 3 on different random nodes thought would be sufficient to handle the load.

But, it doesn't. Three pods isn't enough to handle the traffic for the application. What to do?

WE CAN INCREASE THE NUMBER OF PODS. HOW?? WITH THE HELP OF THE CONCEPT OF `SCALING`.

WE CAN SCALE IN AND OUT THE PODs. 
- SCALE OUT - Increase the number of PODs
- SCALE IN - reducing the number of PODs if not needed.
>these, we can simply increase or  the desired state and K8's `kube-controller manager` does the job.

+We can do this scaling automatically with the help of **AUTO SCLAING**
- HORIZONTALLY (HPA - HORIZONTAL POD AUTO-SCALING) and 
- VERTICALLY. (VPA - VERTICAL POD AUTO-SCALING)
--BUT THATs FOR ANOTHER DAY,

> The purpose of SCALING is to increase or decrease the NUMBER OF PODs for the `replicas` to manage the state as per our convenience both up or down, we can do the same on fly!

THIS WAY WE ACHIEVE THE ==SCALING== FACTOR.

THIS IS WHY WE MUST BE USING THIS `REPLICATION` CONCEPT USING THE K8s WORKLOAD RESOURCES TO DEPLOY AND RUN OUR APPS ON K8s CLUSTERS. 

NOW WE KNOW WHY WE SHOULD NOTE BE USING THE STANDALONE PODs BUT WORKLOAD RESOURCES WITH `REPLICATION` CONCEPT FOR OUR REAL TIME MISSION CRITICAL APPLICATIONS ON PROD IN K8s CLUSTER.

---













If we need to increase or decrease the number of POD for the replicas we can do it on. FYI. 
> as the name says, Scaling Pods.

eg: Three pods. application can handle enough traffic. Later, experiencing an unexpected spike? LB distributing to only one where two got failed. Now, the one is literally getting choked with traffic. 

Here, we can do the replicas in ease on the fly. 

Simply can able to spin up 
- number of pods
- wider the spec
who's gonna do the scale in and scale out? 
Similar to AWS Auto-scaling.

> ALL OF THESE POINTS ARE THE MAIN HIGHLIGHTS THAT WHY REPLICATION AND STUFFS MATTERS!

`KUBERNETES WORKLOAD RESOURCES`
****

 

