SectionÂ 12:Â K8S Services
1. Â When to learn Stateful set workload resource?
2. Â Introduction to k8s Service and Types
		Introduction and Overview to k8 services and types - [Doc]
3. Â Overview on need of Service for workload resources
4. Â Cluster IP service: Introduction
5.Â Overview on Service type ClusterIP
		Overview on Service type ClusterIP - [Doc]
1. Service (ClusterIP) with Endpoint
		service (ClusterIP) with Endpoints - [Doc]
2. Service ClusterIP type creation using Imperative approach
		Service ClusterIP type creation using Imperative approach - [Doc]
3. Service selector and pod labels
		Service selector and Pod labels - [Doc]
4. Advanced: Traffic flow from client (POD) to service to target PODs
		Advanced Traffic flow from client (POD) to service to target PODs - [Doc]
5. NodePort service: Introduction
		NodePort service Introduction - [Doc]
6. Overview on Service type NodePort
		Overview on Service type NodePort - [Doc]
7. Service NodePort type creation using Imperative approach
		Service NodePort type creation using Imperative approach - [Doc]
8. Customize service NodePort range and IP addresses range
		 Customize service NodePort range and IP addresses range - [Doc]
9. Â Advanced: Traffic flow from external to node to service to POD
		Advanced Traffic flow from external to node to service to POD - [Doc]
10. Load Balancer service: Introduction
		Load Balancer service Introduction - [Doc]
11. Introduction to MetalLB for On-premises k8s cluster
		Introduction to MetalLB for On-premises k8s cluster - [Doc]
12. Deploying MetalLB on cluster(On-premises)
		Deploying MetalLB on cluster - [Doc]
13. Advanced: Traffic flow while using LoadBalancer service type
		Service Load Balancer type creation using Imperative approach - [Doc]
14. ExternalIP service: Introduction
		ExternalIP service Introduction - [Doc]
15. Overview on Service type ExternalIP
16. ExternalName service: Introduction
		ExternalName service Introduction - [Doc]
17. Overview on Service type ExternalName
18. Headless service: Introduction(ClusterIP: None)
		Headless service Introduction - [Doc]
19. Overview on Service type Headless(ClusterIP: None)
		Overview on Service type Headless - [Doc]

---
# SectionÂ 12:Â K8sServices
# 1. Â When to learn Stateful set workload resource?

![[Pasted image 20250219185128.png]]

So far, we have covered `rc`, `rs`, `deploy`, `ds`, `cj`, `jobs` and in bonus `auto-cleanup`, 

What is left is, **Statefulset** `sts`

Before, diving into this `sts` Statefulset. We have to/should know the difference between **Stateful** and **Stateless** Resources and a basic understanding about **Services** and **Storage - PV and PVC**,
> K8s Component:  ==**Services**== and ==**Volumes**==
Then will be able understand,
- What is Statefulset
- Why is Statefulset
- and all the advantages of using Statefulset Application Deployment on K8s.


Nothing major but the topics of `Headless` services. Before knowing this, first we need to understand what is `service` in K8s and why + `volumeClaimTemplates` same excuse goes here. Need to understand `volumes - pv, pvc`, difference of **static** and **dynamic** volumes for `sts ` before understanding this. 

Thats why we are jumping into services and storage before diving into **K8s Workload Resource:  ==Statefulset `sts`==**

---
# 2. Â Introduction to k8s Service and Types
Introduction and Overview to k8 services and types - [Doc]

**Services**: A way to access a set of PODs managed by the K8s Workload Resources. 
**Storage:** A way to assess and provisions dataset a set of PODs Volumes to be a shared datasets managed by the K8s Workload Resources. 

> K8s Component:  ==**Services**== and ==**Volumes - PV and PVC**==

In these K8s workload Resources, we have seen all these standalone pods, `rc`'s, `rs`'s, `deploy`ments, `ds`'s, `jobs`, `cronjobs` and such. not `sts` yet since we need a brief understanding about **services** and **storage**. 

we have seen all these workloads and stuff but
- HOW DO YOU MANAGE AND DISTRIBUTE THE TRAFFIC? 
- IF A NEED OF A **PROXY** OR **LB**, WHAT ARE ALL THE COMPONENTS THAT WE HAVE?  - **==SERVICES==**

##### Services:
**Services**: A way to access a set of PODs managed by the K8s Workload Resources. 

Will dive into K8s Services and all the services types we have,
1) ClusterIP (default)
2) NodePort 
3) LoadBalancer `LB`
4) ExternalIP
5) ExternalName
6) Headless (ClusterIP: none)


Will be getting started with 
###### ClusterIP(default):
**ClusterIP is the default service type.** Will go in detail about the ClusterIP 
- What is ClusterIP
- Why is it used
- When to use it
- What is DNS
- What is IP Address
- How the DNS assigns and routes
- Who takes care of it
- How `kube-proxy` plays a key roles in distributing the traffic to the workload resources & controlling the network

###### NodePort:
NodePort to accessing the all the applications outside the cluster, 

###### LoadBalancer:
LoadBalancer to distribute the traffic and helps accessing the applications outside the cluster. 

services itself the LoadBalancer `LB`. But if need of an additional Load Balancers for some specified use cases and adding ease of functionality for a complex infrastructure `LB`.  

###### ExternalIP:
Accessing external components from outside the cluster, **ExternalIP** from the/via/through applications running inside the K8s Cluster. 

###### ExternalName:
and the same to communicate to the external resources by using DNS name, **ExternalName**

###### Headless Service (ClusterIP: none):
 important for Statefulset resources. performing all these headlessly. 

WILL DIG DEEPER INTO THIS AND UNDERSTAND WHAT ALL THESE SERVICES MEANS, WHAT TO USE, WHY TO USE AND WHERE TO  USE IT. 

---
# 3. Â Overview on need of Service for workload resources

**THE NEED OF `SERVICE` FOR WORKLOAD RESOURCES.**

Say for example that are having a Workflow of a **`Deployment`** Object:
> **Workflow**: Deployment -> ReplicaSet -> POD
> 	`Deploy` -> creates an `rs` of that -> `rs` which have the `replica+Podtemplate` -> creates all the PODs in all the available nodes, based on the **PodTemplate**

The need of a **service**. Why do we are in need of a service to perform such cases. *eg, as  per this diagram,*

Illustratively,
![[svc.excalidraw]] 

Here, We are trying to create a `deployment` object. an `nginx` application for example. As per the nature of the `deploy` object, -> creates an `rs` -> creates `PODs` with the respective labels with all all the `replicas+podTemplateHash`. As the result of the `deployment` object,

> ==**`deploy -nginx` -> `rs: 3` -> POD1 , POD2, POD3**
**have been created.** ==

The thing is, 
say we have a frontend working on somewhere and  this `deployment` object is the backend. If me as user have to access to these PODs 1, 2 and 3 working as the backend.  
**HOW? WILL WE BE REGISTERING ALL THESE IP TO THE FRONTEND DESPITE BEING THE IP's ARE EPHEMERAL?**
> 	Hypothetically, lets say we done the same. configured each of the POD's IP to the frontend app making the backend accessible and working fine.
unfortunately or by any chance, we have deleted or the PODs goes down and gets recreated/self healed. And the IPs gets changed being **ephemeral**. EVEN IF YOU DO SO KEEPING THINGS UPDATED! **What about rollbacks and rollouts?** will we reconfiguring all these over and over again to the Frontend IF IT HAS `LB` then?

**HOW TO APPROACH THIS ?**  **==Services==**
IF IT IS A MANAGED SERVICES, EACH WILL BE PROVIDING THEIR OWN LOAD BALANCER,
- AWS EKS - ELB
- AZURE AKS - ALB
- GCP GKE - LB 

IF IT IS FOR ON-PREM or SUCH, **K8s Providing its own LB via the services.** Here, in the frontend, it is enough to just refer to the services (with VirtualIP or with DNS Name). and that's enough.

`servicesLB.yaml` + `deploy.yaml`. Work is done. Like
![[svc.excalidrawYaml]]
WHAT IS THE KEY COMPONENT HERE FOR THESE INDIVIDUAL ELEMENTS TO WORK WITH EACH OTHER: **==SELECTOR==**

RULE: **SelectorLabels** eg: `app=A` = **PodLabels** `app=A`
In order for the service's selector labels to identify the PODs by matching the PODs Labels, **!IMP - SELECTOR ==Labels==** of the ***services*** even though its just a metadata, plays a key role in identifying the PODs and provisioning it to work with. 
Regardless of the state, the IPs and the nodes. We will be figure and making thing out with these one. **Services** having an LB *distributing the traffic* in round-robin fashion. 

==The way that we get to have a contact with these are with **VIP** - **VIRTUAL IP** Address or **DNS NAME**.== 
Every IP that gets assigned to a service is know as Virtual IP and also these services will have DNS name. 
- who assigns it
- how it get assigned
- how things work in and out

The easiest way to configure a load balancer is of using `service`, these applies to all the workload supports `replication` and voila! -> CORE VALUE AND USE OF THE SERVICE.

since we are using `replica` concept in our cluster by using the `deploy` -> deriving `rs`  / or straight up `rs`, `rc`, `sts` -> PODs out of it.
In order to distribute to traffic to each, instead of loading all into just one pod. For that, LB. To create an LB in K8s - **==`SERVICE`==**
No matter the number of just one or more. 

WILL COVER ALL THINGS ABOUT:
> K8s Component:  ==**Services**==, and types. 
all the interrogative and exclamatory factors of:
- What is
- Why
- Where to use it
- At what conditions and such

###### **tldr** - service:
THE CONCEPT OF **SERVICE** IS SAME. IT IS ALL ABOUT **==DISTRIBUTION==**, where the service identifies PODs using `labels` which are defined in it's **selector** of Service's manifest which is identical to the PODs `matchLabels` and `matchExpressions` and distribute. 


and will cover all the TYPES OF SERVICEs which are,
1) ClusterIP (default)
2) NodePort 
3) LoadBalancer `LB` for service
4) ExternalIP
5) ExternalName
6) Headless (ClusterIP: none)
with these TYPES OF SERVICES we can create services to do distribution in various kinds for different use case of distribution. 

WILL DIG DEEP INTO THE CRITICAL POINTS,

> K8s Component:  ==**Services**==, and types. 
- A way to provider logical abstraction to allow access to application running on set of PODs
- It will make sure the traffic is split to backend PODs managed by workload resources using **LoadBalancer** mechanism (round robin as default)
- We can create different **services** by passing `.spec.type= [CLusterIP(default), LoadBalancer, nodePort]` etc. 
- Backend PODs are integrated to **service** by means of **`Endpoint object`**
- **Service** has a single point of contact with DNS and backend PODs with unique IP addresses (dynamic)
- **Service** will be reachable on IP address as well, which is called as **ClusterIP** (*Virtual IP address* mechanism)
- **Service** *DNS names* are controlled by **CoreDNS** add-on K8s Cluster
- **Service** identifies backend targets (PODs) by `selector` (`.spec.selector`)
- **Service** can be created with `EndPoint` (default) or *without Endpoint (EndpointSlice Object)*
- Default protocol supported by **service**: `TCP (UDP and SCTP)` 
- **Service** can be identified with 2 Primary modes -> Environment variables and DNS
1) Environment Variables: *{SVCNAME}_SERVICE_HOST and {SVCNAME}_SERVICE_PORT*
2) DNS: `<svcname>.<namespace>.svc.cluster.local`


NOW, Will get to the critical points,
##### INTRODUCTION TO SERVICES:
###### A way to provider logical abstraction to allow access to application running on set of PODs
If we need to send traffic to multiple replicas in the backend of our apps in the cluster - **services**  
uhm why?


###### It will make sure the traffic is split to backend PODs managed by workload resources using **LoadBalancer** mechanism (round robin as default)
makes sure the traffic is split to backend PODs managed by workload resources using **LoadBalancer** mechanism (round robin as default) 

as we previously mentioned, 
`user` -- req --> `service` with LB -- redirects to --> pod1, 2, 3 (derived from `deployment` workload resource) - IN A ROUND ROBIN FASHION EQUALLY DISTRIBUTING THE TRAFFIC ONE ON ONE. 


###### We can create different services by passing `.spec.type=[CLusterIP(default), LoadBalancer, nodePort]` etc. 
default service type = **ClusterIP**
We can create different services by passing 
>`.spec.type`=`serviceType`

If in need of specifying different type of port means, 
```
type: serviceType
```

###### Backend PODs are integrated to **service** by means of **`Endpoint object`**
Services uses a component called **`Endpoint object`** in order to manage the resources shattered around the cluster. 
> will be creating a service `svc` -> `endpoint` object -> POD1, 2, 3

**`Endpoint object`** - knows all the pods we have, if anything or any pod gets down and gets recreated with new information, THE RESPONSIBILITY OF UPDATING THE NEW INFO OF THE NEWLY CREATED POD TO `LB`, DISTRIBUTING BY REDIRECTING THE TRAFFIC TO NEW PODs IS TAKEN CARE OF **`ENDPOINT OBJECT`**, plays a key role of adding and removing pods upgrades and deprecations.  

Like in Jenkins controller having agents. 


###### Service has a single point of contact with DNS and backend PODs with unique IP addresses (dynamic)
every service will have a VIP and DNS name
+
###### Service will be reachable on IP address as well, which is called as ClusterIP (*Virtual IP address* mechanism)

IN ORDER TO REACH OUT TO SERVICE, the Single point of contact (SPOC) for these Service `svc` is the 
- **==CLUSTERIP**
- **==DNS NAME==**
and the reason for this is as we mentioned earlier that there are no IP's and Name are persistent but ephemeral. meaning *temporary name and IP*. 

###### Service *DNS names* are controlled by **CoreDNS** add-on K8s Cluster
and these two SPOC of `svc`, 
- **==CLUSTERIP==** - assigned by `svc` itself,
- **==DNS NAME==** - `svc` dns names are resolved and controlled by a component named CoreDNS.
**CoreDNS**Â is a flexible and fast plugin based add-on DNS server written in Go. It can be used in various environments and integrates with Kubernetes, etcd, and cloud providers.

takes the responsibility of assigning dns names to . 

###### Service identifies backend targets (PODs) by selector (`.spec.selector`)
Service -> identifies Targets with the help of Labels using -> **`Selector`** 

###### Service can be created with `EndPoint` (default) or *without Endpoint (EndpointSlice Object)*
Defining `endpoint` is optional, not so mandatory. works  even if we don't mention it. 

###### Default protocol supported by service: `TCP (UDP and SCTP)` 
Default protocol of service: `TCP (UDP and SCTP)` 

###### Service can be identified with 2 Primary modes -> Environment variables and DNS
1) Environment Variables: *{SVCNAME}_SERVICE_HOST and {SVCNAME}_SERVICE_PORT*
2) DNS: `<svcname>.<namespace>.svc.cluster.local`
Two primary modes::
**environment variables** will be seen in the container of the POD. 
can also use, `dns` names: `<svcname>.<namespace>.svc.cluster.local`

**for env: (if you call the variables, it'll be like)**
*{SVCNAME}_SERVICE_HOST 
{SVCNAME}_SERVICE_PORT*

**for dns names:**
domain may differ for different K8s cluster. most of the time the syntax will be `svc.cluster.local`


NOW WILL GET INTO PRACTICALS, GETTING STARTED WITH `ClusterIP`.

---
# 4. Â Cluster IP service: Introduction

**ClusterIP Service: Introduction**
In services, first and default one covering `ClusterIP`. (is the default type in service.)

![[svc.excalidraw]]

WILL RECAP THE SAME TO REFER `ClusterIP`,
>  Service `svc` type:  ==**ClusterIP**==
- It's a default **Service** `svc` type, if no type value passed while creating **service**
- **Service** create a **VIP (`virtualIP`) address** which will be accessible within the cluster
- Every **Service** `svc` has its own `DNS` and `ClusterIP`
- In K8s, `kube-proxy` is responsible for **VIP mechanism** for **Service** `svc` (Except: `ExternalName` **service** type)
- Modes offered in `kube-proxy`:
1) `iptables(default)`
2) **IPVS** (low latency, higher throughput and better performance)
3) **userspace** (legacy mode, not recommended anymore)
4) Execute cmd:
```
curl http://localhost:10249/{proxyMode, healthz, health, metrics}
```
- `kube-proxy` supports session `affinity` and `stickness timeout`
1) `.spec.sessionAffinty` (`ClusterIP` and none)
2) `.spec.sessionAffinityConfig.clientIP.timeoutSeconds(Default: 10800)` (which is 3hrs)
3) Not supported in **Windows**

as we referred the diagram previously, 
![[svc.excalidraw]]

###### It's a default **Service** `svc` type, if no type value passed while creating **service**
**`ClusterIP` is the default service type in K8s.**  
Even if you didn't declare any service types in the manifest, goes with the default `ClusterIP`. 

###### **Service** create a **VIP (`virtualIP`) address** which will be accessible within the cluster
Since we mention that a service contains of a `VIP` Virtual private address and a `dns name`, which helps to be accessible within the cluster. 

If we create a service, it will have a `dns name` and `VIP` for itself. To be accessible inside the cluster either you are in control plane or in compute plane node. 

THIS CANNOT BE ACCESSIBLE OUTSIDE THE CLUSTER SINCE IT IS A **VIRTUAL PRIVATE IP** can be accessible only in private. 

NOTE TO BE CONSIDERED WHEN WE ARE WORKING WITH **Service** `svc`

###### Every **Service** `svc` has its own `DNS` and `ClusterIP`
Every **Service** `svc` has its own `DNS` and `ClusterIP`
use any one of them to use the `backend` PODs using `LB`. 

###### In K8s, `kube-proxy` is responsible for **VIP mechanism** for **Service** `svc` (Except: `ExternalName` **service** type)
Most important factor. 
In K8s, we have `kube-proxy`. IS THE MAIN CHARACTER.==The actual component who is responsible for sending all the traffic to the `backend` PODs.==

IF CoreDNS takes care of assigning `dns name` to the `svc`, `kube-proxy` does the same for the `VIP`. 

request sent to the `svc`,  `kube-proxy` managed VIP for the `svc` takes care of the rest. Not just the ClusterIP. 

> `kube-proxy` - takes care of distributing and  sending back the traffic. 

*BUT THIS MECHANISM DOESN'T APPLY FOR `ExternalName` service type.  
**that way, we have different modes in `kube-proxy`,**


###### Modes offered in `kube-proxy`:
- iptables
- IPVS
- userspace
**important to understand `kube-proxy` modes as a K8s Administrator.** 
since `kube-proxy` - takes care of the responsibility of distributing and  sending back the traffic from `svc` to `backend` PODs.  

1) **`iptables(default)`**
default mode of `kube-proxy`. less than 1000's of svc's, this mode is enough. 

2) **IPVS** (low latency, higher throughput and better performance)
for specialized use cases shines in HA type of workloads and for large scale applications, such as having more than 1000s of `svc`'s to manage the application,  offering low latency, higher throughput and better performance)

3) **userspace** 
legacy. old and deprecated. not recommended anymore. high latency, poor performance. 

**Execute cmd:**
```
curl http://localhost:10249/{proxyMode, healthz, health, metrics}
```
-> this command talks to `kube-proxy` and that fetches back all the data such as proxyMode, healthz, health, metrics. 


###### `kube-proxy` supports `session affinity` and `stickiness timeout`
1) `.spec.sessionAffinty` (`ClusterIP` and none)
2) `.spec.sessionAffinityConfig.clientIP.timeoutSeconds(Default: 10800)` (which is 3hrs) and its not supported in **Windows**

need to talk about this, the purpose and use of this. `kube-proxy` supports session `affinity` and `stickness timeout`

LETS SAY, 
we have a client (which can be an application running inside a cluster, outside K8s cluster or just a simple user). 

AND You will send requests to the services `svc` an `LB` for in case in order to reach out to the PODs. 

###### 1. session affinity
**`.spec.sessionAffinty` (`ClusterIP` and none)**
Client -- req ->  `svc` -> PODs and here, the client sedning another request,
Client1 -- req1 ->  `svc` -> PODs -> IF I WANT TO SEND THAT SAME ANOTHER REQUEST FROM THE SAME CLIENT TO THE SAME POD. (for particular reasons)
for such cases, we need to use  `Session affinity` as `ClusterIP`.
means if i want to send request to the same POD in the backend of the `svc` - `Session affinity`=`ClusterIP`. 

if not so bothered about such, DISTRIBUTING REQUESTS TO DIFFERENT PODs are okay means, `svc` - `Session affinity`=`none`  - DEFAULT for `svc`'s `session affinity`

###### 2. stickiness timeout 
**`.spec.sessionAffinityConfig.clientIP.timeoutSeconds(Default: 10800)` (which is 3hrs)**
 
in order to enable `stickiness timeout`,  **`.spec.sessionAffinityConfig.clientIP.timeoutSeconds = nInSecs`  (Default: 10800 which is 3hrs)**
 a session timeout of a client to one particular request and following to the PODs. and this feature available only on linux, not on windows. 

THIS PROXY MODES (`iptables`, `ipvs`, `userspace`) APPLIES TO ALL THE `SVC`s. Will get more of `ClusterIP` and cover more of what is `dns name` `clusterIP` **VIP** and all, 


---
# 5.Â Overview on Service type ClusterIP
Overview on **Service** type **`ClusterIP`** - [Doc]

Will do Practical demo of/on `ClusterIP` Service type,

Here in order to create a `svc` to work with a workload, will take the `deployment.yaml` for reference  or create your own by yourself, 

Here in this `deployment.yaml`, which is trying to create 3Replicas in the backend. 
 
```yaml
apiVersion: apps/v1
kind: Deployment
metadata:
Â  name: nginx-deploy
Â  labels:
Â  Â  app: nginx-app
Â  Â  env: prod
Â  Â  release: v1.0

spec:
Â  replicas: 2
Â  selector:
Â  Â  matchLabels:
Â  Â  Â  app: nginx-app
Â  Â  Â  env: prod
Â  Â  Â  release: v1.0

Â  template:
Â  Â  metadata:
Â  Â  Â  name: nginx-deploy
Â  Â  Â  labels:
Â  Â  Â  Â  app: nginx-app
Â  Â  Â  Â  env: prod
Â  Â  Â  Â  release: v1.0

Â  Â  spec:
Â  Â  Â  containers:
Â  Â  Â  - name: write-app
Â  Â  Â  Â  image: alpine
Â  Â  Â  Â  command: ["/bin/sh"]
Â  Â  Â  Â  args: ["-c", "while true; do date >> /var/log/index.html; sleep 10; done"]
Â  Â  Â  Â  resources:
Â  Â  Â  Â  Â  limits:
Â  Â  Â  Â  Â  Â  cpu: 100m
Â  Â  Â  Â  Â  Â  memory: 100Mi
Â  Â  Â  Â  volumeMounts:
Â  Â  Â  Â  - name: deploy-shared-volume
Â  Â  Â  Â  Â  mountPath: /var/log

Â  Â  Â  - name: serve-app
Â  Â  Â  Â  image: nginx:latest
Â  Â  Â  Â  ports:
Â  Â  Â  Â  - containerPort: 80
Â  Â  Â  Â  resources:
Â  Â  Â  Â  Â  limits:
Â  Â  Â  Â  Â  Â  cpu: 100m
Â  Â  Â  Â  Â  Â  memory: 100Mi
Â  Â  Â  Â  volumeMounts:
Â  Â  Â  Â  Â  - name: deploy-shared-volume
Â  Â  Â  Â  Â  Â  mountPath: /usr/share/nginx/html
Â  Â  Â  Â  Â 
Â  Â  Â  volumes: Â 
Â  Â  Â  - name: deploy-shared-volume
Â  Â  Â  Â  emptyDir: {}
```
or you can run your own app. Here,  will work with the existing one. 
recap to this `deployment` object,  creating an `rs`  running `3`replica having one container **writing** data, other **serves** data. 

**Workload Object - Deployment First**!
run the `deploment` object,
```
kubectl create -f /path/to/deploy.yaml
```
verify and check resources status, 
```
kubectl get deploy,rs,po -o wide
```
getting all the 3pods. 

NOW, for the `svc` part, to
Create a **service** `svc` `LB` load balancer in the `ClusterIP` type,
prerequisites before creating a `ClusterIP svc` LB:
1) **check the POD labels to be identical to identify. Verify the labels by printing it using,**
```
kubectl get po -o wide --show-labels
```
prints the labels, 
THESE ARE THE LABELS FOR REFERENCE FOR THE SERVICES TO IDENTIFY THE PODs in the backend to split the traffic using `LB`,

2) **write manifest for the `svc` `clusterIPsvc.yaml`**
```yML
apiVersion: v1 #apiVersion for svc
kind: Service Â #kind of the object
metadata: Â  Â  Â #metadata name and labels
Â  name: nginx-clusterip-svc
Â  labels:
Â  Â  app: nginx-clusterip-svc

spec: Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â #spec of the svc
Â  selector: Â  Â  Â  Â  Â  Â  Â  Â  Â #selector of the svc
Â  Â  app: nginx-app Â  Â  Â  Â  Â  #selector labels of the svc (must be identical/match with pod labels)
Â  ports: Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  #need to pass tow ports (1.port any number of svc to send and receive the traffic and 2.target port which of the same port that send traffic to the destination pod/container)
Â  Â  - port: 80 Â  Â  Â  Â  Â  Â  Â  #ports of the svc (port number that to assign to the svc to send and receive the traffic) (use any ports for the svc)
Â  Â  Â  targetPort: 80 Â  Â  Â  Â  #target port of the svc has to be the same port number of the pod/container (backend port that we send that traffic to) (must use the same port number of the pod/container)
Â  Â  Â  protocol: TCP Â  Â  Â  Â  Â #protocol of the svc
Â  type: ClusterIP Â  Â  Â  Â  Â  Â #svc type (even if we don't specify it, it will be ClusterIP by default) (optional to specify)
```
**labels has to be identical and match with atleast one parameter of label.**  

1) create `svc` and verify resource,
```sh
kubectl create -f /path/to/svc.yaml
kubectl get svc,po -o wide --show-labels
```
```sh
NAME         nginx-clusterip-svc 
TYPE         ClusterIP
CLUSTER-IP   10.107.96.17
EXTERNAL-IP  <none>  
PORT(S)      80/TCP
AGE          23m
SELECTOR     app=nginx-app
```

previously, we have been reaching out to pods with the **PODs Ephemeral IP**. Now, its not needed. `svc` takes care of it as long as its connected to with the labels.

as a client, i will be requesting `ip` of the `svc`. Where the `svc` is a **ClusterIP** type and the `svc` does `LB` in **round robin** fashion. 
```sh
curl svcIP
```
but you will not know from which POD u get the response from. 
rerun the same, maybe u get the response from another POD - **round robin** fashion. 

**viola! ðŸ¤¯ðŸ¤¯ðŸ¤¯**
+++
==ONE BIG TWIST, SO FAR UPTO THIS CONFIG OF THE `svc`. THE `svcIP` of the **ClusterIP`svc`** is ephemeral too. ðŸ’€==

IF WE JUST DELETE `svc` AND TRIED TO RECREATE MEANS, as same the workload objects, the `svc`'s IP changes too. 
verify `svcIP` first, delete and recreate, verify `svcIP` - not same. 
```sh
kubectl get svc,po -o wide --show-labels
kubectl delete path/to/svc.yaml 
kubectl delete svc svcName 
kubectl create path/to/svc.yaml 
kubectl get svc,po -o wide --show-labels
```

WHAT TO DO? 
###### **==DNS NAMES==**
in that case, use **==`dns names.`==** - **PRIORITY AND RECOMMENDED WAY TO `PING` AND COMMUNICATE WITH THE `svc`** 

Since, the IP is ephemeral and too much of hassle and illogic to manage IPs. **Naming resources** would be a wise option. We can use IP but by any chance if gets recreated, IP will be pouf. 


**What is the `dns name` assigned to this `svc`?**
so the name of the `svc` is
`name=svcName`

probably, the `dns name` of `svc` - **`svcName.default`**
THIS WILL BE FAMILIAR FOR THOSE WHO KNOW `NAMESPACES`. 
IF WE DIDN'T PASS ANYTHING, and deployed any application, falls in `default` namespaces.  unless we pass anything or customized regarding namespaces. 

>namespace: `dns-name= svcName.default.svc.cluster.local`

Lets see if this is true or false? in order the check the record of `dns`, will be using
```sh
nslookup
```

If app deployed in a normal machine means, the domain can be resolved with - `nslookup svcIP`, 
```sh
nslookup svcIP 10.96.3.147
**'server cant find 147.3.96.10.in-addr.arpa: NXDOMAIN'
```
would've been resolved but it didn't. 
THIS IS NOT HOW YOU WORK WITH THIS, fetching`svc`'s `dns-names`, since, we cannot run `nslookup`, 

THEN how do we checking the namespaces to get `dns-name`?

Create a pod, having a container in it. From the shell of the container, run `nslookup`.

run, Imperatively
```sh
kubectl get svc,po -o wide --show-labels
kubectl run nslookup --image=dubareddy/utils --rm -i -t -- bash
#'or just a simple shell image with all the utils in it.'
```

if you want to exit and to get back,
```sh
exit
kubectl attach podName -c podName -i -t
```

Check `dns-name`,
1) **nslookup svcIP**
```sh
nslookup svcIP
```
```sh
Server:        k8sServerIP
Address:       k8sServerIP#53
dnsIP.inaddr.arpa   name=svcName.delfault.svc.default.local
```

```sh
Server:        10.96.0.10
Address:       10.96.0.10#53
31.226.102.10.in-addr.arpa   name=clusterip-svc.delfault.svc.default.local
 ```
`name=clusterip-svc.delfault.svc.default.local`
name= 1) the service name, 2) namespace, 3) component + location
> **Note: Cannot run `svc`'s with the same names.**

IN ORDER TO USE THE `dns name` to reach out to the PODs. 
simply, (from that `nslookup` utils shell)
```
nslookup dnsName
```
this `nslookup` svc's `dns name` cannot be resolved blatantly. NOT EVEN FROM THE CLUSTER. 

where a `dns` is configured in a K8s Cluster, this has been performed by the help of **CoreDNS**. Only from the container, from inside the container. 

same goes with 
```
curl dnsName
```
it just works. 

**IF A FRONTEND APP IS TRYING TO REACH OUT TO BACKEND PODs. what that application resolved by the help of dnsName (instead of ephemeral IP's of `svc` and `deployment` obj)**

**==USE ONLY `svc`'s `dns name` or `IP` instead of directIP's of PODs.==** 

if,
```
exit
kubectl get po -o wide
```
`nslookup` POD will not be there. test pod will be removed. 

because of
```
kubectl run nslookup --image=dubareddy/utils --rm -i -t -- bash
```
's `--rm` parameter. 

WILL DIG DEEP INTO MORE OF `ClusterIP`'S `svc`. 


---
# 6. `ClusterIP` svc with Endpoint
service (ClusterIP) with Endpoints - [Doc]

Will do Practical demo of/on `ClusterIP` Service type with Endpoint,
**`ClusterIP's svc endpoint`**

Recap. on `svc` ->  creates ->  **endpoint** -> identifies -> backend **PODs**. 
In order to manage and splitting the traffic. The distribution part of the `svc` is not performed by the `svc` but the **`svc`'s endpoint**

**==The `svc`'s endpoint takes care of adding and removing the PODs svc LB for the distribution part of the traffic.==** 

To do that, **describe** the `svc`: 
```
kubectl describe svc svcName
kubectl describe svc svcName | grep Endpoints
```
and there,
```
Endpoints:     10.244.55.178:80 , 10.244.55.179:80 , 10.244.55.180:80
```









---
# 7. `ClusterIP svc` type creation using Imperative approach
Service ClusterIP type creation using Imperative approach - [Doc]



---
# 8. Service selector and pod labels
Service selector and Pod labels - [Doc]



---
# 9. Advanced: Traffic flow from client (POD) to service to target PODs
Advanced Traffic flow from client (POD) to service to target PODs - [Doc]

![[svc-workflow.excalidraw]]



---
# 10. NodePort service: Introduction
NodePort service Introduction - [Doc]

![[svc-nodeport.excalidraw]]
- In order to expose **service** on `each node` to external traffic using fixed port of nodes in the cluster
- `Workflow`: External traffic -> NodePort -> **Service** (ClusterIP) -> POD
- While creating **service** object, we need to specify type: NodePort
- By default, K8s allocated default **NodePort** from `30000 to 32767`
- we can check the NodePort assigned to the service under .spec.ports[*].nodePort=32170
- In order to set custom IP address for NodePort service, we need to define `--nodeport-address` flag for `kube-proxy` process or `kube-proxy` config file (`nodePortAddress`)
- With this `NodePort` **Service**, we need to open the firewall on the nodes if enabled, unnecessarily even POD is not running on that node. 



###### In order to expose **service** on `each node` to external traffic using fixed port of nodes in the cluster



###### `Workflow`: External traffic -> NodePort -> **Service** (ClusterIP) -> POD




###### While creating **service** object, we need to specify type: NodePort




###### By default, K8s allocated default **NodePort** from `30000 to 32767`



###### we can check the NodePort assigned to the service under *.spec.ports[*]*.nodePort=32170




###### In order to set custom IP address for NodePort service, we need to define `--nodeport-address` flag for `kube-proxy` process or `kube-proxy` config file (`nodePortAddress`)



###### With this `NodePort` **Service**, we need to open the firewall on the nodes if enabled, unnecessarily even POD is not running on that node. 



---
# 11. Overview on Service type NodePort
Overview on Service type NodePort - [Doc]

**Demo: Overview on Service type: `NodePort`,**



---
# 12. Service NodePort type creation using Imperative approach
Service NodePort type creation using Imperative approach - [Doc]

---
# 13. Customize service NodePort range and IP addresses range
Customize service NodePort range and IP addresses range - [Doc]


---
# 14. Â Advanced: Traffic flow from external to node to service to POD
Advanced Traffic flow from external to node to service to POD - [Doc]






---
# 15. Load Balancer service: Introduction
Load Balancer service Introduction - [Doc]


---
# 16. Introduction to MetalLB for On-premises k8s cluster
Introduction to MetalLB for On-premises k8s cluster - [Doc]

---
# 17. Deploying MetalLB on cluster(On-premises)
Deploying MetalLB on cluster - [Doc]



---
# 18. Advanced: Traffic flow while using LoadBalancer service type
Service Load Balancer type creation using Imperative approach - [Doc]




---
# 19. ExternalIP service: Introduction
ExternalIP service Introduction - [Doc]



---
# 20. Overview on Service type ExternalIP


---
# 21. ExternalName service: Introduction
ExternalName service Introduction - [Doc]


---
# 22. Overview on Service type ExternalName



---
# 23. Headless service: Introduction(ClusterIP: None)
Headless service Introduction - [Doc]


---
# 24. Overview on Service type Headless(ClusterIP: None)
Overview on Service type Headless - [Doc]


---




---




