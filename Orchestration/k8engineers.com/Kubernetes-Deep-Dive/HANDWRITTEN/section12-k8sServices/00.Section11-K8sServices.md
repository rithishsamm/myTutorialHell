Section 12: K8S Services
1.  When to learn Stateful set workload resource?
2.  Introduction to k8s Service and Types
		Introduction and Overview to k8 services and types - [Doc]
3.  Overview on need of Service for workload resources
4.  Cluster IP service: Introduction
5. Overview on Service type ClusterIP
		Overview on Service type ClusterIP - [Doc]
1. Service (ClusterIP) with Endpoint
		service (ClusterIP) with Endpoints - [Doc]
2. Service ClusterIP type creation using Imperative approach
		Service ClusterIP type creation using Imperative approach - [Doc]
3. Service selector and pod labels
		Service selector and Pod labels - [Doc]
4. Advanced: Traffic flow from client (POD) to service to target PODs
		Advanced Traffic flow from client (POD) to service to target PODs - [Doc]
5. NodePort service: Introduction
		NodePort service Introduction - [Doc]
6. Overview on Service type NodePort
		Overview on Service type NodePort - [Doc]
7. Service NodePort type creation using Imperative approach
		Service NodePort type creation using Imperative approach - [Doc]
8. Customize service NodePort range and IP addresses range
		 Customize service NodePort range and IP addresses range - [Doc]
9.  Advanced: Traffic flow from external to node to service to POD
		Advanced Traffic flow from external to node to service to POD - [Doc]
10. Load Balancer service: Introduction
		Load Balancer service Introduction - [Doc]
11. Introduction to MetalLB for On-premises k8s cluster
		Introduction to MetalLB for On-premises k8s cluster - [Doc]
12. Deploying MetalLB on cluster(On-premises)
		Deploying MetalLB on cluster - [Doc]
13. Advanced: Traffic flow while using LoadBalancer service type
		Service Load Balancer type creation using Imperative approach - [Doc]
14. ExternalIP service: Introduction
		ExternalIP service Introduction - [Doc]
15. Overview on Service type ExternalIP
16. ExternalName service: Introduction
		ExternalName service Introduction - [Doc]
17. Overview on Service type ExternalName
18. Headless service: Introduction(ClusterIP: None)
		Headless service Introduction - [Doc]
19. Overview on Service type Headless(ClusterIP: None)
		Overview on Service type Headless - [Doc]

---
# Section 12: K8sServices
# 1.  When to learn Stateful set workload resource?

![[Pasted image 20250219185128.png]]

So far, we have covered `rc`, `rs`, `deploy`, `ds`, `cj`, `jobs` and in bonus `auto-cleanup`, 

What is left is, **Statefulset** `sts`

Before, diving into this `sts` Statefulset. We have to/should know the difference between **Stateful** and **Stateless** Resources and a basic understanding about **Services** and **Storage - PV and PVC**,
> K8s Component:  ==**Services**== and ==**Volumes**==
Then will be able understand,
- What is Statefulset
- Why is Statefulset
- and all the advantages of using Statefulset Application Deployment on K8s.

Nothing major but the topics of `Headless` services. Before knowing this, first we need to understand what is `service` in K8s and why + `volumeClaimTemplates` same excuse goes here. Need to understand `volumes - pv, pvc`, difference of **static** and **dynamic** volumes for `sts ` before understanding this. 

Thats why we are jumping into services and storage before diving into **K8s Workload Resource:  ==Statefulset `sts`==**

---
# 2.  Introduction to k8s Service and Types
Introduction and Overview to k8 services and types - [Doc]

**Services**: A way to access a set of PODs managed by the K8s Workload Resources. 
**Storage:** A way to assess and provisions dataset a set of PODs Volumes to be a shared datasets managed by the K8s Workload Resources. 
> K8s Component:  ==**Services**== and ==**Volumes - PV and PVC**==

In these K8s workload Resources, we have seen all these standalone pods, `rc`'s, `rs`'s, `deploy`ments, `ds`'s, `jobs`, `cronjobs` and such. not `sts` yet since we need a brief understanding about **services** and **storage**. 

we have seen all these workloads and stuff but
- HOW DO YOU MANAGE AND DISTRIBUTE THE TRAFFIC? 
- IF A NEED OF A **PROXY** OR **LB**, WHAT ARE ALL THE COMPONENTS THAT WE HAVE?  - **==SERVICES==**

##### Services:
**Services**: A way to access a set of PODs managed by the K8s Workload Resources. 

Will dive into K8s Services and all the services types we have,
1) ClusterIP (default)
2) NodePort 
3) LoadBalancer `LB`
4) ExternalIP
5) ExternalName
6) Headless (ClusterIP: none)

Will be getting started with 
###### ClusterIP(default):
**ClusterIP is the default service type.** Will go in detail about the ClusterIP 
- What is ClusterIP
- Why is it used
- When to use it
- What is DNS
- What is IP Address
- How the DNS assigns and routes
- Who takes care of it
- How `kube-proxy` plays a key roles in distributing the traffic to the workload resources & controlling the network

###### NodePort:
NodePort to accessing the all the applications outside the cluster, 

###### LoadBalancer:
LoadBalancer to distribute the traffic and helps accessing the applications outside the cluster. 

services itself the LoadBalancer `LB`. But if need of an additional Load Balancers for some specified use cases and adding ease of functionality for a complex infrastructure `LB`.  

###### ExternalIP:
Accessing external components from outside the cluster, **ExternalIP** from the/via/through applications running inside the K8s Cluster. 

###### ExternalName:
and the same to communicate to the external resources by using DNS name, **ExternalName**

###### Headless Service (ClusterIP: none):
 important for Statefulset resources. performing all these headlessly. 

WILL DIG DEEPER INTO THIS AND UNDERSTAND WHAT ALL THESE SERVICES MEANS, WHAT TO USE, WHY TO USE AND WHERE TO  USE IT. 

---
# 3.  Overview on need of Service for workload resources

**THE NEED OF `SERVICE` FOR WORKLOAD RESOURCES.**

Say for example that are having a Workflow of a **`Deployment`** Object:
> **Workflow**: Deployment -> ReplicaSet -> POD
> 	`Deploy` -> creates an `rs` of that -> `rs` which have the `replica+Podtemplate` -> creates all the PODs in all the available nodes, based on the **PodTemplate**

The need of a **service**. Why do we are in need of a service to perform such cases. *eg, as  per this diagram,*

Illustratively,
![[svc.excalidraw]] 

Here, We are trying to create a `deployment` object. an `nginx` application for example. As per the nature of the `deploy` object, -> creates an `rs` -> creates `PODs` with the respective labels with all all the `replicas+podTemplateHash`. As the result of the `deployment` object,

> ==**`deploy -nginx` -> `rs: 3` -> POD1 , POD2, POD3**
**have been created.** ==

The thing is, 
say we have a frontend working on somewhere and  this `deployment` object is the backend. If me as userPOD have to access to these PODs 1, 2 and 3 working as the backend.  
**HOW? WILL WE BE REGISTERING ALL THESE IP TO THE FRONTEND DESPITE BEING THE IP's ARE EPHEMERAL?**
> 	Hypothetically, lets say we done the same. configured each of the POD's IP to the frontend app making the backend accessible and working fine.
unfortunately or by any chance, we have deleted or the PODs goes down and gets recreated/self healed. And the IPs gets changed being **ephemeral**. EVEN IF YOU DO SO KEEPING THINGS UPDATED! **What about rollbacks and rollouts?** will we reconfiguring all these over and over again to the Frontend IF IT HAS `LB` then?

**HOW TO APPROACH THIS ?**  **==Services==**
IF IT IS A MANAGED SERVICES, EACH WILL BE PROVIDING THEIR OWN LOAD BALANCER,
- AWS EKS - ELB
- AZURE AKS - ALB
- GCP GKE - LB 

IF IT IS FOR ON-PREM or SUCH, **K8s Providing its own LB via the services.** Here, in the frontend, it is enough to just refer to the services (with VirtualIP or with DNS Name). and that's enough.

`servicesLB.yaml` + `deploy.yaml`. Work is done. Like
![[svc.excalidrawYaml]]
WHAT IS THE KEY COMPONENT HERE FOR THESE INDIVIDUAL ELEMENTS TO WORK WITH EACH OTHER: **==SELECTOR==**

RULE: **SelectorLabels** eg: `app=A` = **PodLabels** `app=A`
In order for the service's selector labels to identify the PODs by matching the PODs Labels, **!IMP - SELECTOR ==Labels==** of the ***services*** even though its just a metadata, plays a key role in identifying the PODs and provisioning it to work with. 
Regardless of the state, the IPs and the nodes. We will be figure and making thing out with these one. **Services** having an LB *distributing the traffic* in round-robin fashion. 

==The way that we get to have a contact with these are with **VIP** - **VIRTUAL IP** Address or **DNS NAME**.== 
Every IP that gets assigned to a service is know as Virtual IP and also these services will have DNS name. 
- who assigns it
- how it get assigned
- how things work in and out

The easiest way to configure a load balancer is of using `service`, these applies to all the workload supports `replication` and voila! -> CORE VALUE AND USE OF THE SERVICE.

since we are using `replica` concept in our cluster by using the `deploy` -> deriving `rs`  / or straight up `rs`, `rc`, `sts` -> PODs out of it.
In order to distribute to traffic to each, instead of loading all into just one pod. For that, LB. To create an LB in K8s - **==`SERVICE`==**
No matter the number of just one or more. 

WILL COVER ALL THINGS ABOUT:
> K8s Component:  ==**Services**==, and types. 
all the interrogative and exclamatory factors of:
- What is
- Why
- Where to use it
- At what conditions and such

###### **tldr** - service:
THE CONCEPT OF **SERVICE** IS SAME. IT IS ALL ABOUT **==DISTRIBUTION==**, where the service identifies PODs using `labels` which are defined in it's **selector** of Service's manifest which is identical to the PODs `matchLabels` and `matchExpressions` and distribute. 


and will cover all the TYPES OF SERVICEs which are,
1) ClusterIP (default)
2) NodePort 
3) LoadBalancer `LB` for service
4) ExternalIP
5) ExternalName
6) Headless (ClusterIP: none)
with these TYPES OF SERVICES we can create services to do distribution in various kinds for different use case of distribution. 

WILL DIG DEEP INTO THE CRITICAL POINTS,

> K8s Component:  ==**Services**==, and types. 
- A way to provider logical abstraction to allow access to application running on set of PODs
- It will make sure the traffic is split to backend PODs managed by workload resources using **LoadBalancer** mechanism (round robin as default)
- We can create different **services** by passing `.spec.type= [CLusterIP(default), LoadBalancer, nodePort]` etc. 
- Backend PODs are integrated to **service** by means of **`Endpoint object`**
- **Service** has a single point of contact with DNS and backend PODs with unique IP addresses (dynamic)
- **Service** will be reachable on IP address as well, which is called as **ClusterIP** (*Virtual IP address* mechanism)
- **Service** *DNS names* are controlled by **CoreDNS** add-on K8s Cluster
- **Service** identifies backend targets (PODs) by `selector` (`.spec.selector`)
- **Service** can be created with `EndPoint` (default) or *without Endpoint (EndpointSlice Object)*
- Default protocol supported by **service**: `TCP (UDP and SCTP)` 
- **Service** can be identified with 2 Primary modes -> Environment variables and DNS
1) Environment Variables: *{SVCNAME}_SERVICE_HOST and {SVCNAME}_SERVICE_PORT*
2) DNS: `<svcname>.<namespace>.svc.cluster.local`


NOW, Will get to the critical points,
##### INTRODUCTION TO SERVICES:
###### A way to provider logical abstraction to allow access to application running on set of PODs
If we need to send traffic to multiple replicas in the backend of our apps in the cluster - **services**  
uhm why?


###### It will make sure the traffic is split to backend PODs managed by workload resources using **LoadBalancer** mechanism (round robin as default)
makes sure the traffic is split to backend PODs managed by workload resources using **LoadBalancer** mechanism (round robin as default) 

as we previously mentioned, 
`userPOD` -- req --> `service` with LB -- redirects to --> pod1, 2, 3 (derived from `deployment` workload resource) - IN A ROUND ROBIN FASHION EQUALLY DISTRIBUTING THE TRAFFIC ONE ON ONE. 


###### We can create different services by passing `.spec.type=[CLusterIP(default), LoadBalancer, nodePort]` etc. 
default service type = **ClusterIP**
We can create different services by passing 
>`.spec.type`=`serviceType`

If in need of specifying different type of port means, 
```
type: serviceType
```

###### Backend PODs are integrated to **service** by means of **`Endpoint object`**
Services uses a component called **`Endpoint object`** in order to manage the resources shattered around the cluster. 
> will be creating a service `svc` -> `endpoint` object -> POD1, 2, 3

**`Endpoint object`** - knows all the pods we have, if anything or any pod gets down and gets recreated with new information, THE RESPONSIBILITY OF UPDATING THE NEW INFO OF THE NEWLY CREATED POD TO `LB`, DISTRIBUTING BY REDIRECTING THE TRAFFIC TO NEW PODs IS TAKEN CARE OF **`ENDPOINT OBJECT`**, plays a key role of adding and removing pods upgrades and deprecations.  

Like in Jenkins controller having agents. 


###### Service has a single point of contact with DNS and backend PODs with unique IP addresses (dynamic)
every service will have a VIP and DNS name
+
###### Service will be reachable on IP address as well, which is called as ClusterIP (*Virtual IP address* mechanism)

IN ORDER TO REACH OUT TO SERVICE, the Single point of contact (SPOC) for these Service `svc` is the 
- **==CLUSTERIP**
- **==DNS NAME==**
and the reason for this is as we mentioned earlier that there are no IP's and Name are persistent but ephemeral. meaning *temporary name and IP*. 

###### Service *DNS names* are controlled by **CoreDNS** add-on K8s Cluster
and these two SPOC of `svc`, 
- **==CLUSTERIP==** - assigned by `svc` itself,
- **==DNS NAME==** - `svc` dns names are resolved and controlled by a component named CoreDNS.
**CoreDNS** is a flexible and fast plugin based add-on DNS server written in Go. It can be used in various environments and integrates with Kubernetes, etcd, and cloud providers.

takes the responsibility of assigning dns names to . 

###### Service identifies backend targets (PODs) by selector (`.spec.selector`)
Service -> identifies Targets with the help of Labels using -> **`Selector`** 

###### Service can be created with `EndPoint` (default) or *without Endpoint (EndpointSlice Object)*
Defining `endpoint` is optional, not so mandatory. works  even if we don't mention it. 

###### Default protocol supported by service: `TCP (UDP and SCTP)` 
Default protocol of service: `TCP (UDP and SCTP)` 

###### Service can be identified with 2 Primary modes -> Environment variables and DNS
1) Environment Variables: *{SVCNAME}_SERVICE_HOST and {SVCNAME}_SERVICE_PORT*
2) DNS: `<svcname>.<namespace>.svc.cluster.local`
Two primary modes::
**environment variables** will be seen in the container of the POD. 
can also use, `dns` names: `<svcname>.<namespace>.svc.cluster.local`

**for env: (if you call the variables, it'll be like)**
*{SVCNAME}_SERVICE_HOST 
{SVCNAME}_SERVICE_PORT*

**for dns names:**
domain may differ for different K8s cluster. most of the time the syntax will be `svc.cluster.local`


NOW WILL GET INTO PRACTICALS, GETTING STARTED WITH `ClusterIP`.

---
# 4.  Cluster IP service: Introduction

**ClusterIP Service: Introduction**
In services, first and default one covering `ClusterIP`. (is the default type in service.)

![[svc.excalidraw]]

WILL RECAP THE SAME TO REFER `ClusterIP`,
>  Service `svc` type:  ==**ClusterIP**==
- It's a default **Service** `svc` type, if no type value passed while creating **service**
- **Service** create a **VIP (`virtualIP`) address** which will be accessible within the cluster
- Every **Service** `svc` has its own `DNS` and `ClusterIP`
- In K8s, `kube-proxy` is responsible for **VIP mechanism** for **Service** `svc` (Except: `ExternalName` **service** type)
- Modes offered in `kube-proxy`:
1) `iptables(default)`
2) **IPVS** (low latency, higher throughput and better performance)
3) **userspace** (legacy mode, not recommended anymore)
4) Execute cmd:
```
curl http://localhost:10249/{proxyMode, healthz, health, metrics}
```
- `kube-proxy` supports session `affinity` and `stickness timeout`
1) `.spec.sessionAffinty` (`ClusterIP` and none)
2) `.spec.sessionAffinityConfig.clientIP.timeoutSeconds(Default: 10800)` (which is 3hrs)
3) Not supported in **Windows**

as we referred the diagram previously, 
![[svc.excalidraw]]

###### It's a default **Service** `svc` type, if no type value passed while creating **service**
**`ClusterIP` is the default service type in K8s.**  
Even if you didn't declare any service types in the manifest, goes with the default `ClusterIP`. 

###### **Service** create a **VIP (`virtualIP`) address** which will be accessible within the cluster
Since we mention that a service contains of a `VIP` Virtual private address and a `dns name`, which helps to be accessible within the cluster. 

If we create a service, it will have a `dns name` and `VIP` for itself. To be accessible inside the cluster either you are in control plane or in compute plane node. 

THIS CANNOT BE ACCESSIBLE OUTSIDE THE CLUSTER SINCE IT IS A **VIRTUAL PRIVATE IP** can be accessible only in private. 

NOTE TO BE CONSIDERED WHEN WE ARE WORKING WITH **Service** `svc`

###### Every **Service** `svc` has its own `DNS` and `ClusterIP`
Every **Service** `svc` has its own `DNS` and `ClusterIP`
use any one of them to use the `backend` PODs using `LB`. 

###### In K8s, `kube-proxy` is responsible for **VIP mechanism** for **Service** `svc` (Except: `ExternalName` **service** type)
Most important factor. 
In K8s, we have `kube-proxy`. IS THE MAIN CHARACTER.==The actual component who is responsible for sending all the traffic to the `backend` PODs.==

IF CoreDNS takes care of assigning `dns name` to the `svc`, `kube-proxy` does the same for the `VIP`. 

request sent to the `svc`,  `kube-proxy` managed VIP for the `svc` takes care of the rest. Not just the ClusterIP. 

> `kube-proxy` - takes care of distributing and  sending back the traffic. 

*BUT THIS MECHANISM DOESN'T APPLY FOR `ExternalName` service type.  
**that way, we have different modes in `kube-proxy`,**


###### Modes offered in `kube-proxy`:
- iptables
- IPVS
- userspace
**important to understand `kube-proxy` modes as a K8s Administrator.** 
since `kube-proxy` - takes care of the responsibility of distributing and  sending back the traffic from `svc` to `backend` PODs.  

1) **`iptables(default)`**
default mode of `kube-proxy`. less than 1000's of svc's, this mode is enough. 

2) **IPVS** (low latency, higher throughput and better performance)
for specialized use cases shines in HA type of workloads and for large scale applications, such as having more than 1000s of `svc`'s to manage the application,  offering low latency, higher throughput and better performance)

3) **userspace** 
legacy. old and deprecated. not recommended anymore. high latency, poor performance. 

**Execute cmd:**
```
curl http://localhost:10249/{proxyMode, healthz, health, metrics}
```
-> this command talks to `kube-proxy` and that fetches back all the data such as proxyMode, healthz, health, metrics. 


###### `kube-proxy` supports `session affinity` and `stickiness timeout`
1) `.spec.sessionAffinty` (`ClusterIP` and none)
2) `.spec.sessionAffinityConfig.clientIP.timeoutSeconds(Default: 10800)` (which is 3hrs) and its not supported in **Windows**

need to talk about this, the purpose and use of this. `kube-proxy` supports session `affinity` and `stickness timeout`

LETS SAY, 
we have a clientPOD (which can be an application running inside a cluster, outside K8s cluster or just a simple user). 

AND You will send requests to the services `svc` an `LB` for in case in order to reach out to the PODs. 

###### 1. session affinity
**`.spec.sessionAffinty` (`ClusterIP` and none)**
clientPOD -- req ->  `svc` -> PODs and here, the clientPOD sedning another request,
clientPOD1 -- req1 ->  `svc` -> PODs -> IF I WANT TO SEND THAT SAME ANOTHER REQUEST FROM THE SAME clientPOD TO THE SAME POD. (for particular reasons)
for such cases, we need to use  `Session affinity` as `ClusterIP`.
means if i want to send request to the same POD in the backend of the `svc` - `Session affinity`=`ClusterIP`. 

if not so bothered about such, DISTRIBUTING REQUESTS TO DIFFERENT PODs are okay means, `svc` - `Session affinity`=`none`  - DEFAULT for `svc`'s `session affinity`

###### 2. stickiness timeout 
**`.spec.sessionAffinityConfig.clientIP.timeoutSeconds(Default: 10800)` (which is 3hrs)**
 
in order to enable `stickiness timeout`,  **`.spec.sessionAffinityConfig.clientIP.timeoutSeconds = nInSecs`  (Default: 10800 which is 3hrs)**
 a session timeout of a clientPOD to one particular request and following to the PODs. and this feature available only on linux, not on windows. 

THIS PROXY MODES (`iptables`, `ipvs`, `userspace`) APPLIES TO ALL THE `SVC`s. Will get more of `ClusterIP` and cover more of what is `dns name` `clusterIP` **VIP** and all, 


---
# 5. Overview on Service type ClusterIP
Overview on **Service** type **`ClusterIP`** - [Doc]

Will do Practical demo of/on `ClusterIP` Service type,

Here in order to create a `svc` to work with a workload, will take the `deployment.yaml` for reference  or create your own by yourself, 

Here in this `deployment.yaml`, which is trying to create 3Replicas in the backend. 
 
```yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: nginx-deploy
  labels:
    app: nginx-app
    env: prod
    release: v1.0

spec:
  replicas: 2
  selector:
    matchLabels:
      app: nginx-app
      env: prod
      release: v1.0

  template:
    metadata:
      name: nginx-deploy
      labels:
        app: nginx-app
        env: prod
        release: v1.0

    spec:
      containers:
      - name: write-app
        image: alpine
        command: ["/bin/sh"]
        args: ["-c", "while true; do date >> /var/log/index.html; sleep 10; done"]
        resources:
          limits:
            cpu: 100m
            memory: 100Mi
        volumeMounts:
        - name: deploy-shared-volume
          mountPath: /var/log

      - name: serve-app
        image: nginx:latest
        ports:
        - containerPort: 80
        resources:
          limits:
            cpu: 100m
            memory: 100Mi
        volumeMounts:
          - name: deploy-shared-volume
            mountPath: /usr/share/nginx/html
         
      volumes:  
      - name: deploy-shared-volume
        emptyDir: {}
```
or you can run your own app. Here,  will work with the existing one. 
recap to this `deployment` object,  creating an `rs`  running `3`replica having one container **writing** data, other **serves** data. 

**Workload Object - Deployment First**!
run the `deploment` object,
```
kubectl create -f /path/to/deploy.yaml
```
verify and check resources status, 
```
kubectl get deploy,rs,po -o wide
```
getting all the 3pods. 

NOW, for the `svc` part, to
Create a **service** `svc` `LB` load balancer in the `ClusterIP` type,
prerequisites before creating a `ClusterIP svc` LB:
1) **check the POD labels to be identical to identify. Verify the labels by printing it using,**
```
kubectl get po -o wide --show-labels
```
prints the labels, 
THESE ARE THE LABELS FOR REFERENCE FOR THE SERVICES TO IDENTIFY THE PODs in the backend to split the traffic using `LB`,

2) **write manifest for the `svc` `clusterIPsvc.yaml`**
```yML
apiVersion: v1 #apiVersion for svc
kind: Service  #kind of the object
metadata:      #metadata name and labels
  name: nginx-clusterip-svc
  labels:
    app: nginx-clusterip-svc

spec:                        #spec of the svc
  selector:                  #selector of the svc
    app: nginx-app           #selector labels of the svc (must be identical/match with pod labels)
  ports:                     #need to pass tow ports (1.port any number of svc to send and receive the traffic and 2.target port which of the same port that send traffic to the destination pod/container)
    - port: 80               #ports of the svc (port number that to assign to the svc to send and receive the traffic) (use any ports for the svc)
      targetPort: 80         #target port of the svc has to be the same port number of the pod/container (backend port that we send that traffic to) (must use the same port number of the pod/container)
      protocol: TCP          #protocol of the svc
  type: ClusterIP            #svc type (even if we don't specify it, it will be ClusterIP by default) (optional to specify)
```
**labels has to be identical and match with atleast one parameter of label.**  

1) create `svc` and verify resource,
```sh
kubectl create -f /path/to/svc.yaml
kubectl get svc,po -o wide --show-labels
```
```sh
NAME         nginx-clusterip-svc 
TYPE         ClusterIP
CLUSTER-IP   10.107.96.17
EXTERNAL-IP  <none>  
PORT(S)      80/TCP
AGE          23m
SELECTOR     app=nginx-app
```

previously, we have been reaching out to pods with the **PODs Ephemeral IP**. Now, its not needed. `svc` takes care of it as long as its connected to with the labels.

as a clientPOD, i will be requesting `ip` of the `svc`. Where the `svc` is a **ClusterIP** type and the `svc` does `LB` in **round robin** fashion. 
```sh
curl svcIP
```
but you will not know from which POD u get the response from. 
rerun the same, maybe u get the response from another POD - **round robin** fashion. 

**viola! 🤯🤯🤯**
+++
==ONE BIG TWIST, SO FAR UPTO THIS CONFIG OF THE `svc`. THE `svcIP` of the **ClusterIP`svc`** is ephemeral too. 💀==

IF WE JUST DELETE `svc` AND TRIED TO RECREATE MEANS, as same the workload objects, the `svc`'s IP changes too. 
verify `svcIP` first, delete and recreate, verify `svcIP` - not same. 
```sh
kubectl get svc,po -o wide --show-labels
kubectl delete path/to/svc.yaml 
kubectl delete svc svcName 
kubectl create path/to/svc.yaml 
kubectl get svc,po -o wide --show-labels
```

WHAT TO DO? 
###### **==DNS NAMES==**
in that case, use **==`dns names.`==** - **PRIORITY AND RECOMMENDED WAY TO `PING` AND COMMUNICATE WITH THE `svc`** 

Since, the IP is ephemeral and too much of hassle and illogic to manage IPs. **Naming resources** would be a wise option. We can use IP but by any chance if gets recreated, IP will be pouf. 


**What is the `dns name` assigned to this `svc`?**
so the name of the `svc` is
`name=svcName`

probably, the `dns name` of `svc` - **`svcName.default`**
THIS WILL BE FAMILIAR FOR THOSE WHO KNOW `NAMESPACES`. 
IF WE DIDN'T PASS ANYTHING, and deployed any application, falls in `default` namespaces.  unless we pass anything or customized regarding namespaces. 

>namespace: `dns-name= svcName.default.svc.cluster.local`

Lets see if this is true or false? in order the check the record of `dns`, will be using
```sh
nslookup
```

If app deployed in a normal machine means, the domain can be resolved with - `nslookup svcIP`, 
```sh
nslookup svcIP 10.96.3.147
**'server cant find 147.3.96.10.in-addr.arpa: NXDOMAIN'
```
would've been resolved but it didn't. 
THIS IS NOT HOW YOU WORK WITH THIS, fetching`svc`'s `dns-names`, since, we cannot run `nslookup`, 

THEN how do we checking the namespaces to get `dns-name`?

Create a pod, having a container in it. From the shell of the container, run `nslookup`.

run, Imperatively
```sh
kubectl get svc,po -o wide --show-labels
kubectl run nslookup --image=dubareddy/utils --rm -i -t -- bash
#'or just a simple shell image with all the utils in it.'
```

if you want to exit and to get back,
```sh
exit
kubectl attach podName -c podName -i -t
```

Check `dns-name`,
1) **nslookup svcIP**
```sh
nslookup svcIP
```
```sh
Server:        k8sServerIP
Address:       k8sServerIP#53
dnsIP.inaddr.arpa   name=svcName.delfault.svc.default.local
```

```sh
Server:        10.96.0.10
Address:       10.96.0.10#53
31.226.102.10.in-addr.arpa   name=clusterip-svc.delfault.svc.default.local
 ```
`name=clusterip-svc.delfault.svc.default.local`
name= 1) the service name, 2) namespace, 3) component + location
> **Note: Cannot run `svc`'s with the same names.**

IN ORDER TO USE THE `dns name` to reach out to the PODs. 
simply, (from that `nslookup` utils shell)
```
nslookup dnsName
```
this `nslookup` svc's `dns name` cannot be resolved blatantly. NOT EVEN FROM THE CLUSTER. 

where a `dns` is configured in a K8s Cluster, this has been performed by the help of **CoreDNS**. Only from the container, from inside the container. 

same goes with 
```
curl dnsName
```
it just works. 

**IF A FRONTEND APP IS TRYING TO REACH OUT TO BACKEND PODs. what that application resolved by the help of dnsName (instead of ephemeral IP's of `svc` and `deployment` obj)**
**==USE ONLY `svc`'s `dns name` or `IP` instead of directIP's of PODs.==** 

if,
```
exit
kubectl get po -o wide
```
`nslookup` POD will not be there. test pod will be removed. 

because of
```
kubectl run nslookup --image=dubareddy/utils --rm -i -t -- bash
```
's `--rm` parameter. 

WILL DIG DEEP INTO MORE OF `ClusterIP`'S `svc`. 

---
# 6. `ClusterIP` svc with Endpoint
service (ClusterIP) with Endpoints - [Doc]

Will do Practical demo of/on `ClusterIP` Service type with Endpoint,**`ClusterIP's svc endpoint.`**

Recap. on `svc` ->  creates ->  **endpoint** -> identifies -> backend **PODs**. 
In order to manage and splitting the traffic. The distribution part of the `svc` is not performed by the `svc` but the **`svc`'s endpoint**
**==The `svc`'s endpoint takes care of adding and removing the PODs svc LB for the distribution part of the traffic.==** 

To do that, **describe** the `svc`: 
```
kubectl describe svc svcName
kubectl describe svc svcName | grep Endpoints
```
and there,
```
Endpoints:     10.244.55.178:80 , 10.244.55.179:80 , 10.244.55.180:80
```
 **endpoints** are nothing but the PODs. These endpoints are the destination points that are the traffic it just ends at. THIS IS AN INDIVIDUAL COMPONENT THAT MAKE THINGS WORK HERE. which is **Endpoint`ep`**
```
kubectl describe ep svcName
```

```sh
Name:         nginx-clusterip-svc
Namespace:    default
Labels:       app=nginx-clusterip-svc
Annotations:  endpoints.kubernetes.io/last-change-trigger-time: 2025-03-06T04:37:32Z
Subsets:
  Addresses:          10.244.55.189,10.244.55.190,10.244.55.191
  NotReadyAddresses:  <none>
  Ports:
    Name     Port  Protocol
    ----     ----  --------
    <unset>  80    TCP

Events:  <none>
```
this thing manages `pods` on behalf of `svc`. here, in the subsets' addresses parameter: you get all the `ip`'s of the PODs. which are managed by endpoints which are endpoints itself. 
shows,
- The PODs IP. which itself are the endpoints (Addresses:          `10.244.55.189,10.244.55.190,10.244.55.191`)
- Ports that the endpoints are listening on (`80`)
- and the protocol (`TCP`)
- the namespace that `ep` is in. 

Only if you create an `svc` with a selector. The **Endpoints** will get created. `svc` without selector, it doesn't. Will see how that rolls out in upcoming days.

tldr: Workflow of the creation of an endpoint: 
`svc+selector` -- creates --> endpoint --> identifies PODs with labels. 

TO UNDERSTAND THE SAME IN DEEP:
list the pods,
```
kubectl get po -o wide
```
if i happened to kill a POD. 
```
kubectl delete po podName
kubectl get po -o wide
```
we all know that the `Deployment` workload resource object will try to create a new POD under the `replica` concept to match the desired and actual state. -> *new POD with a new IP*

REALISTICALLY SPEAKING BASED ON THE CONCEPT AND THE SETUP OF `svc`, the **endpoint** `ep` must be updated. The component must be managing the newly spawn resource. 
```
kubectl describe svc svcName
kubectl describe ep svcName
```
==the *new POD with a new IP* must've been applied and updated to the **Endpoints`ep`**==

**This is possible with the nature of ClusterIP `svc` type with the `selector` written on it, this Endpoint `ep`  gets created and works, which will take the responsibility of finding PODs which gets mapped having with the labels that are identical what we have written in the PODs and in the `svc` manifest and to the `svc` having this `ep` identifying and managing the same** :)

---
# 7. `ClusterIP svc` type creation using Imperative approach
Service ClusterIP type creation using Imperative approach - [Doc]

CREATING THIS `ClusterIP` **svc** USING IMPERATIVE APPROACH:

so far, we have been covering all these by using declarative approach writing manifest for everything and working on the same.

Will see how to work with all these on just from the command line *creating a **ClusterIP`svc`** type of service* (imperatively). NOT JUST `svc` BUT ANY WORKLOAD RESOURCES ALL MATTERS
```
 Possible resources include (case insensitive):

 pod (po), service (svc), replicationcontroller (rc), deployment (deploy), replicaset (rs)
```

For all the imperative practices,
```
kubectl expose --help
```
and here you have all the possible ways to do things imperatively.
all the parameters, args, examples and options. 


**Examples:**
```
  # Create a service for a replicated nginx, which serves on port 80 and connects to the containers on port 8000
  kubectl expose rc nginx --port=80 --target-port=8000

  # Create a service for a replication controller identified by type and name specified in "nginx-controller.yaml", which serves on port 80 and connects to the containers on port 8000
  kubectl expose -f nginx-controller.yaml --port=80 --target-port=8000

  # Create a service for a pod valid-pod, which serves on port 444 with the name "frontend"
  kubectl expose pod valid-pod --port=444 --name=frontend

  # Create a second service based on the above service, exposing the container port 8443 as port 443 with the name "nginx-https"
  kubectl expose service nginx --port=443 --target-port=8443 --name=nginx-https

  # Create a service for a replicated streaming application on port 4100 balancing UDP traffic and named 'video-stream'.
  kubectl expose rc streamer --port=4100 --protocol=UDP --name=video-stream

  # Create a service for a replicated nginx using replica set, which serves on port 80 and connects to the containers on port 8000
  kubectl expose rs nginx --port=80 --target-port=8000

  # Create a service for an nginx deployment, which serves on port 80 and connects to the containers on port 8000
  kubectl expose deployment nginx --port=80 --target-port=8000
```

**Options:**
```
    --allow-missing-template-keys=true:
        If true, ignore any errors in templates when a field or map key is missing in the template. Only applies to golang and jsonpath output formats.

    --cluster-ip='':
        ClusterIP to be assigned to the service. Leave empty to auto-allocate, or set to 'None' to create a headless service.

    --dry-run='none':
        Must be "none", "server", or "clientPOD". If client strategy, only print the object that would be sent, without sending it. If server strategy, submit server-side request without persisting the resource.

    --external-ip='':
        Additional external IP address (not managed by Kubernetes) to accept for the service. If this IP is routed to a node, the service can be accessed by this IP in addition to its generated service IP.

    --field-manager='kubectl-expose':
        Name of the manager used to track field ownership.

    -f, --filename=[]:
        Filename, directory, or URL to files identifying the resource to expose a service

    -k, --kustomize='':
        Process the kustomization directory. This flag can't be used together with -f or -R.

    -l, --labels='':
        Labels to apply to the service created by this call.

    --load-balancer-ip='':
        IP to assign to the LoadBalancer. If empty, an ephemeral IP will be created and used (cloud-provider specific).

    --name='':
        The name for the newly created object.

    -o, --output='':
        Output format. One of: (json, yaml, name, go-template, go-template-file, template, templatefile, jsonpath, jsonpath-as-json, jsonpath-file).

    --override-type='merge':
        The method used to override the generated object: json, merge, or strategic.

    --overrides='':
        An inline JSON override for the generated object. If this is non-empty, it is used to override the generated object. Requires that the object supply a valid apiVersion field.

    --port='':
        The port that the service should serve on. Copied from the resource being exposed, if unspecified

    --protocol='':
        The network protocol for the service to be created. Default is 'TCP'.

    -R, --recursive=false:
        Process the directory used in -f, --filename recursively. Useful when you want to manage related manifests organized within the same directory.

    --save-config=false:
        If true, the configuration of current object will be saved in its annotation. Otherwise, the annotation will be unchanged. This flag is useful when you want to perform kubectl apply on this object in the future.

    --selector='':
        A label selector to use for this service. Only equality-based selector requirements are supported. If empty (the default) infer the selector from the replication controller or replica set.)

    --session-affinity='':
        If non-empty, set the session affinity for the service to this; legal values: 'None', 'ClientIP'

    --show-managed-fields=false:
        If true, keep the managedFields when printing objects in JSON or YAML format.

    --target-port='':
        Name or number for the port on the container that the service should direct traffic to. Optional.

    --template='':
        Template string or path to template file to use when -o=go-template, -o=go-template-file. The template format is golang templates [http://golang.org/pkg/text/template/#pkg-overview].

    --type='':
        Type for this service: ClusterIP, NodePort, LoadBalancer, or ExternalName. Default is 'ClusterIP'.
```

Usage:
```
  kubectl expose (-f FILENAME | TYPE NAME) [--port=port] [--protocol=TCP|UDP|SCTP] [--target-port=number-or-name] [--name=name] [--external-ip=external-ip-of-service] [--type=type] [options]
```

WITH ALL THIS, YOU CAN CREATE RESOURCES IMPERATIVELY, Pick all the essential options and the workload objects to create the same. 

**LETS CREATE A RESOURCE IMPERATIVELY, as we did declaratively:** from the example given here, need to create an **`svc`** with 80 port number with a  running`deployment` object by using a `selector` with the PODs respective labels, (took one from the examples and additionally added `selector`)

to check the labels given below,
```
kubectl get deploy,rs,po -o wide --show-labels
```
```
kubectl expose deployment nginx --port=nOfsvc --target-port=nOfTarget --selector=labels
```

```
kubectl expose deployment deployName --port=8000 --target-port=80 --selector=app=nginx-app --name=nameOfsvc
```
if labels matched, the `svc`'s endpoints will match with the PODs in ease. 
```
kubectl get svc,deploy,rs,po -o wide
curl svcIP:port
```

From this, we have understood that, can create our `svc` of any type using imperative approach. 

*clear and cleanup.* 

---
# 8. Service selector and pod labels
Service selector and Pod labels - [Doc]

So far, we've come through this. WILL COVER THE NEXT SCENARIO OF, 
and WE UNDERSTAND THE WORKINGS OF `svc` that, `svc` -> endpoint -> grabs`selectors` -> matches Labels -> identifies PODs. 

Question is, WHAT IF. I create PODs having the same labels but outside the endpoint targets?
What if like ill be creating a standalone pods with the same labels? (out of our target endpoints.) 

**WILL THE `svc` IDENTIFIES THOSE PODs SINCE IT MATCHES THE LABELS TOO?** 
will try and see that by ourselves, create a service using imperative approach

before that, check all the already available running resources:
```
kubectl get svc,deploy,rs,po -o wide
kubectl get po -o wide --show-labels
kubectl describe po -o wide --show-labels
```

create a pod ` using imperative approach
```
kubectl run podName --image=imageName --labels=identicalLabels 
```
```
kubectl run podName --image=nginx  --labels=app=nginx
```
NOW, WE HAVE A POD that have same identical label as we declared to the `svc`'s to identify the PODs. 

Verify resources. 
```
kubectl get svc,deploy,rs,po -o wide
kubectl get po -o wide --show-labels
```

check the `svc` endpoints: (suspense music)
```
kubectl describe svc svcName
kubectl describe ep svcName
```
YES! THIS **svc** `ep` IDENFIES TH STANDALONE POD TOO. since it has the same labels of `svc`as the `deploy` workloads too.

as you can see the described output. to verify that,
```
curl svcIP
```
SINCE, we simply use `nginx` printing `hello-world` but the rest are the application which prints the date. 

***prints in-between since it follows round-robin fashion.***
**tldr**: yes, `ep` adds if PODs having identical labels whatsoever. 

to remove, simply delete the POD or remove/modify the label.  
to ,modify or remove a PODs label, (enough to mention just the key)
```
kubectl label po podName key-

kubectl label po podName app-
```

verify:
```
kubectl describe ep svcName
kubectl describe svc svcName
```

**Cleanup:**
```
kubectl get svc,deploy,po -o wide
```
```
kubectl delete svc svcName
kubectl delete deploy deployName
```
```
kubectl get svc,deploy,po -o wide
```

:)

---
# 9. Advanced: Traffic flow from client (POD) to service to target PODs
Advanced Traffic flow from client (POD) to service to target PODs - [Doc]

JUST A CONCEPTUAL DIAGRAM OF **THE WORKFLOW OF AN `svc`**:

![[ClusterIP-workflow.excalidraw]]
AFTER UNDERSTANDING IN BRIEF ABOUT `svc` and `ClusterIP`svc type, will see in detail about the workflow of an `svc` that how the traffic flows from client vs `svc` to target PODs. 

HOW IT ALL JUST WORKS? workflow of an `svc` mapping the flow of the traffic from
***ClientPOD -> `svc` -> TargetPODs***

As we have interpreted before, say for example based on this above given diagram, 
IF WE HAVE A Frontend Client POD, user, CDN, container or such for example, taking on all the ingress -> WE SAW ALL THE DRAMA OF BACKEND. 

(tldr: goes to `svc` which distributed to multiple pods from multiple nodes equally and does the same.)

Firstly, (workflow overview)
![[FE2svc2backendPODs.excalidraw]]
what do we have here is,
- a client Frontend POD receiving GETs/Ingress/Traffic address registered to an `svc` in a K8s environment.  
- sends to that K8s Cluster environment, there we have an `svc`
- that `svc` is sending that traffic to PODs having identical labels of each other. 
- this `svc` distributes traffic to the relevant PODs based on the labels.
- where those PODs are managed under a workload resource (here, it is a `deployment` object for managing versions and rolling it out and back which also creates `rs`for the `replica`s )
we have already seen all these in practice previously. 

Here, the context is that - the frontend POD wants to talk to an another POD, which is its supposed backend. ( a POD to POD Conversation)
1) We will be giving the frontend POD, the `svc`'s IP address which is `ClusterIP` or `dns name` 
2) **Frontend `POD` -- req will be sent to --> `svc`**
3) `svc` as `LB` -- distributing traffic evenly (round robin) --> to backend `PODs`.

THIS IS THE MODEL, HOW DOES THIS WORKFLOW WORKS?
**Understanding in brief from the Networking Perspective:** (how does the traffic flows travelling from Client POD via `svc` to those relevant Backend PODs.)

HOW THE TRAFFIC WILL FLOW IN THIS?

Secondly, brief UNDER THE HOOD of the workflow:
![[ClusterIP-FE2BEUnderTheHood.excalidraw]]
Lets say that 
- the **Frontend POD** is running on **node1** (which has its client IP address,
- here, lets take the same as **Client1**, wants to talk to **Backend** service pod.
- Under the hood, the **Frontend** POD first talks to --> `kube-proxy` on its node(**node1**)
>`kube-proxy` plays a key role when its coming to K8s `svc`'s
So, that's why, `kube-proxy` is obsolete on every available node (both control plane or worker node)

**GET:**  FE ---> BE
- That way, the ingress/traffic out of the **Client1** -- sent to --> `kube-proxy` first. (why?)
- coz, `kube-proxy` knows where the relevant `svc` is available (where it has the **src** and **dest**-`ClusterIP` in its packet)
- that uses **DNAT** - Destination Network Address Translation
- in the DNAT, it acknowledges and resolves the **src** and redirects **dest** to the **Backend Service POD** of the request using the packet. (this what DNAT does)
- after resolving out of `kube-proxy`, packet travels from Node1 -> Node2's Pod2
POST: BE ---> FE
- packet will be travel back as src from PodB and the destination is simply PodA straight away.
- Here, no DNAT but Reverse DNAT.  
- simply goes to `kube-proxy` to get resolved and 
- There, by **Reverse DNAT** ting , the **src** changes back to the appropriate Client PodA which is the frontend
- POSTs to Frontend client PodA.
This how the packets will be travelling back and forth between two PODs across **Nodes** - using **ClusterIP** Address.

Here, we understand `kube-proxy` plays a key role on resolving traffic for GET and POST between nodes using DNAT and Reverse DNAT ting. 

---
# 10. NodePort service: Introduction
**NodePort** `svc` Introduction - [Doc]

We have seen the **ClusterIP** svc type. Now will cover another `svc` type which is **NodePort** `svc` type.

Will dive in deep of **NodePort** `svc` type:

EXPOSING THE APPLICATION OUTSIDE K8s CLUSTER by opening up the PORTs, If external traffic wants to talk to the resources inside K8s Cluster via PORTs with the one called **NodeIP Address** = NodeIP address + NodePort (both control plane and worker nodes) - **NodePort**

Traffic -> NodeIP+Port -> `kube-proxy` backed `svc` -> resolves the **src** and **dst** target PODs and does back the same. 

>**Within the Cluster, would've used `ClusterIP`.** (can't access from outside since it is private) 
**Outside Cluster, `NodePort`**

Will dig deep into each key points of `NodePort`
**NodePort svc Architecture:** (nodeService:enp0s3)
![[svc-nodeport.excalidraw]]
- In order to expose **service** on `each node` to external traffic using fixed port of nodes in the cluster
- `Workflow`: External traffic -> NodePort -> **Service** (ClusterIP) -> POD
- While creating **service** object, we need to specify type: NodePort
- By default, K8s allocated default **NodePort** from `30000 to 32767`
- we can check the NodePort assigned to the service under .spec.ports[*].nodePort=32170
- In order to set custom IP address for NodePort service, we need to define `--nodeport-address` flag for `kube-proxy` process or `kube-proxy` config file (`nodePortAddress`)
- With this `NodePort` **Service**, we need to open the firewall on the nodes if enabled, unnecessarily even POD is not running on that node. 

###### In order to expose **service** on `each node` to external traffic using fixed port of nodes in the cluster.
accessing K8s Cluster from outside - NodePort

previously we have seen about `hostname and IP` in order to identify or assess PODs. Here, we have something known as **Node Group**. 

Even if there are no PODs running on, the **NodeIP** and the **NodePort** will be open and available still even without resources. Will see how it gets assigned and see both the pros and cons of the same. 

###### `Workflow`: External traffic -> NodePort -> **Service** (ClusterIP) -> POD

External Traffic with the PORT number -> NodePort -> **Service** (ClusterIP) -> Backend dst Target POD.
> Node -> svc -> POD. 

###### While creating **service** object, we need to specify `type: NodePort`
 
While creating **service** object, we need to specify `type: NodePort` whether Imperatively or declaratively, should be defining `svc`:
```
type: NodePort
```
without defining, it'll proceed with the default `clusterIP`.


###### By default, K8s allocated default **NodePort** from `30000 to 32767`
NodePort's Port range series: 30000 to 32767 for a K8s Cluster. Also we can extend the same if required by specific methods. Will cover the same. 
If you are defining and assigning a Port Number to a resource means, 0


###### we can check the NodePort assigned to the service under *.spec.ports[*]*.nodePort=32170*
we can check the same NodePort assigned to the `svc` under, in order to specify port,
`.spec.ports[*].nodePort= nPortNumber`

###### In order to set custom IP address for NodePort service, we need to define `--nodeport-address` flag for `kube-proxy` process or `kube-proxy` config file (`nodePortAddress`)

In order to set custom IP address which is **NodeIP** address  for **NodePort** service(`enp0s3`), need to define 
```
--nodeport-address
```
flag for `kube-proxy` process or `kube-proxy` config file.

eg: the interface of Node`svc` via multiple ways to be such as to be exposed to see the Interface to interact with the application(any node), such as 
- Loopback Interface - **localhost**
- enp0s3
- or other interface
can listen to any of these. 

Now we have to bind this `NodeIP` interface to the `kube-proxy`.  to define that: `--node-port-address`. 
define `--nodeport-address` flag for `kube-proxy` process or `kube-proxy` config file.
>Simply binding the **NodeIP** to an interface. 

eg: if the nodePort is some `NodeIP` and the Loopback localhost is `127.0.0.1`, 
*Will be binding the `kube-proxy` only to the `loopback localhost` or `other Interface`*. This gets resolved within the cluster but exposed just the application out to the public. 
`svcNodeIP+Port`


###### With this `NodePort` **Service**, we need to open the firewall on the nodes if enabled, unnecessarily even POD is not running on that node. 
Basic and obvious point. Should open up the port in firewall if restricted. 

With this `NodePort` **Service**, we need to open the firewall on the nodes if enabled, unnecessarily even POD is not running on that node. 

Will understand the same in deep by practicing the same. 


---
# 11. Overview on Service type NodePort
Overview on Service type NodePort - [Doc]

**Demo: Overview on Service type: `NodePort`,**

To experiment with the same, will use the same application as part of our practice. This is a simple `svc` and this deployment object is more than enough. 

Just to understand services, not the workload resources. 


`deploy.yaml`
```yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: nginx-deploy
  labels:
    app: nginx-app
    env: prod
    release: v1.0
  
spec:
  replicas: 3
  selector:
    matchLabels:
      app: nginx-app
      env: prod
      release: v1.0

  template:
    metadata:
      name: nginx-deploy
      labels:
        app: nginx-app
        env: prod
        release: v1.0

    spec:
      containers:
      - name: write-app
        image: alpine
        command: ["/bin/sh"]
        args: ["-c", "while true; do date >> /var/log/index.html; sleep 10; done"]
        resources:
          limits:
            cpu: 100m
            memory: 100Mi
        volumeMounts:
        - name: deploy-shared-volume
          mountPath: /var/log

      - name: serve-app
        image: nginx:latest
        ports:
        - containerPort: 80
        resources:
          limits:
            cpu: 100m
            memory: 100Mi
        volumeMounts:
          - name: deploy-shared-volume
            mountPath: /usr/share/nginx/html
          
      volumes:  
      - name: deploy-shared-volume
        emptyDir: {}
```
this is the `deployment` workload resource object to experiment with **ExternalIP** `svc`. Same stuff. 
>`deploy` object -- creates `rs` --> creating `3replica`  --> `PODs` with 2containers on each available node .

Will apply and verify the `deploy.yaml` first,
```
kubectl create -f /path/to/deploy.yaml
kubectl get deploy,po -o wide
```
app is created. back to `svc`,

So, the **NodePort** `svc`,
if we lookback to the manifest of **ClusterIP** `svc`, here it is nothing much different other than **NodePort** specific parameters,
```
type: NodePort
```

manifest of **NodePort**`svc`:
```
apiVersion: v1
kind: Service
metadata:
  name: nginx-nodeportsvc
  labels:
    app: nginx-nodeportsvc
  
spec:
  selector:
    app: nginx-app
    env: prod
    release: v1.0
  ports:
    - port: 80
      targetPort: 80
      protocol: TCP
    
  # type: ClusterIP
  type: NodePort #30000-32767
```
ykwim!, that simple. just declare the `type` param and specify `svc` type.

As we saw about **NodePort**'s overview, it is primarily **NodeIP** and **NodePort** specific of the Node. So obviously the Port specification is needed. 

About the Port, the Series of Ports ranges from: 30000 - 32767
```
kubectl create -f /path/to/nodeportsvc.yaml
```

Since, we are doing **NodePort** `svc`, doesn't mean it creates **ClusterIP**. It creates **ClusterIP** and yes, it it possible to use the same to expose application out to the internet but additionally **NodePort**. 

what does it mean? if you verify the resource
```
kubectl get svc -o wide
```
verify,
```
NAME          nodeportsvc
TYPE          NodePort
CLUSTER-IP    10.98.166.208
EXTERNAL-IP   <none>
PORT(S)       80:31119/TCP
AGE           95s
SELECTOR      app=nginx-app,env=prod,release=v1.0
```

here, and the same in the manifest. The port section is
1) port is the port of the service
2) **TargetPort** is the port that the application is exposed to.

What **NodePort** means by here, in the output's port section,
`PORT(S) - 80:31119/TCP`, Picked the **NodePort**'s range by itself that the application is just got exposed to. 

**NodePort got binded to the application port.**
just do the 
```
curl svcIP
```
and just works as same as ClusterIP but here within the cluster but what we have been using over here is NodePort and it is supposed to work **outside cluster by accessing and opening up the Node machine's IP and Port**. 

So, to proceed with that,
```
ip a
```
either pick the **systems's** IP address or the **localhost**.

```
curl nodeMachine/VMsIP:bindedNodePort
curl 192.168.0.123:31119
```
just works! **Voila**! port80 for the `svc`. within the NodePort's range of 30000-32767. OK THIS IS INSIDE CLUSTER. HOW ABOUT EXTERNAL TRAFFIC. 

GO TO BROWSER and,
```
nodeMachine/VMsIP:bindedNodePort
192.168.0.123:31119
```
:) out in the public to access via browser. 

If anything is conflicting in this setup, please do mind to release and expose all the essential port in the firewall.

>**WORFLOW**:
External Traffic -- PUT --> server/VM - Node --> `svc`(NodePort) --> `Deploy` -> `rs` -> `backendPODs`.

We have seen that the **NodePort** `svc` picks randomly within its range. But,

Instead of getting random port number, how do i get a specific port number within the **NodePort**'s range, so to do that
```
kubectl delete svc svcName
```

You can specify the port number within its range. 
```Yaml
apiVersion: v1
kind: Service
metadata:
  name: nodeportsvc
  labels:
    app:  nodeportsvc

spec:
  selector: 
    app: nginx-app
    env: prod
    release: v1.0
  ports:
    - port: 80
      targetPort: 80
      nodePort: 31000
      protocol: TCP
  # type: ClusterIP
  type: NodePort
```
THIS HOW ITS DONE. APPLY THE MANIFEST AND IT JUST WORKS!

```
kubectl create -f path/to/nodeportsvc.yaml
kubectl get svc,deploy,po -o wide
nodeMachine/VM-IP:nodeport
```

WHAT IF YOU GIVE A **NodePort** NUMBER NOT WITHIN THE RANGE? Lets day, you use a port that are available in every node *not conflicting with any other*. NOTHING BUT THROWS ERROR of,
```
The Service "nodeportsvc" is invalid: spec.ports[0].nodePort: Invalid value: 3900: provided port is not in the valid range. The range of valid ports is 30000-32767
```
Schema validation error. So, provide appropriate range within 30000-32767.

fin

---
# 12. Service NodePort type creation using Imperative approach
Service NodePort type creation using Imperative approach - [Doc]

Doing everything we have seen above but in imperative approach, we cleanup and redeploy all to work with the same. 

apply the `deploy` object manifest and will create an `svc` for it imperatively,
```
kubectl create -f path/to/deploy.yaml
```
```
kubectl get deploy,rs,po -o wide --show-labels
```

based on these resources labels, we will be creating an `svc` to all the target PODs imperatively,
```
kubectl expose deployment nginx-deploy --port=8090 --target-port=80 --selector=app=nginx --type=NodePort --name=nginx-svc
```
```
kubectl get svc -o wide
```

check if it's working,
```
curl nodemachine/VMip:nodeport
```
catch is, **cannot allocate specific port for it to be exposed.**  like we do it declaratively in `nodePort` parameter.

cleanup
```
kubectl delete svcName
```

---
# 13. Customize service NodePort range and IP addresses range
Customize service NodePort range and IP addresses range - [Doc]

How else we can customize the **NodePort** `svc` and the IP range, (that we mentioned previously while referring the NodePort's Overview. Defying the laws of breaking the range beyond 30000-32767)

eg: if your admin or as per organizational demands that they are not convinced with the excuse of having the range only within 30000-32767. How to crack this.

1) Login to the **control plane** node (coz it have all its K8s Component's manifest)
2) located in
```
sudo ls -al /etc/kubernetes/
sudo ls -al /etc/kubernetes/manifest 
```
3) open a `kube-apiserver`component to customize,
```
sudo vi /etc/kubernetes/manifests/kube-apiserver.yaml
```
4) there, under the `container` section. find the parameter of `--service-node-port-range`, below that, add the parameter, (any range as per convenience)
```
--service-node-port-range=25000-28000
```
5) after that, cant run `kubectl` commands. since it is restarting the entire cluster to apply the changes. so wait for it.
6) to check the changes applied, 
```
sudo ps aux | grep kube-apiserver
```
7) here, find the flag of `--service-nodeport-range`
8) and check if the range is applied. 
9) if so, create an `svc` to conclude. 

```
apiVersion: v1
kind: Service
metadata:
  name: nodeportsvc
  labels:
    app:  nodeportsvc

spec:
  selector: 
    app: nginx-app
    env: prod
    release: v1.0
  ports:
    - port: 80
      targetPort: 80
      nodePort: 27000 
      protocol: TCP
  # type: ClusterIP
  type: NodePort
```
```
kubectl apply -f /path/to/nodeportsvc.yaml
```

if the range is out of scope/not correct, will throw
```
The Service "nodeportsvc" is invalid: spec.ports[0].nodePort: Invalid value: 39000: provided port is not in the valid range. The range of valid ports is 25000-28000
```

same goes for **ClusterIP** range by changing classes. Should not conflict with the POD or with **nodeIP** address. !IMP: DO NOT MESS WITH THESE. 


---
# 14.  Advanced: Traffic flow from external to node to service to POD
Advanced Traffic flow from external to node to service to POD - [Doc]

![[Nodeport-workflow.excalidraw]]

Nothing new, just as same as the one we saw earlier. 

Now, will understand how the traffic flows from getting that 
**`External Traffic`** all the way back to the **`backend PODs`** using this **NodePort** `svc` type. Will see all that by explaining each.

![[Nodeport-workflow-brief.excalidraw]]

Here: 
**GET:**
- External Client  -> make requests to `192.168.0.123:nodePort` (to **node1**)
- The **Node1**'s IP is **192.168.0.123**, 
- The **Node2's IP is 192.168.0.124**, 
- To access, we can use any one of the IP addresses though.
- External Traffic/`src` -- GET req --> `kube-proxy` ----->  `dest`/TargetPODs
- GET action contains `src` as client, `dest` as target -> goes to `kube-proxy` 
- `kube-proxy` resolves it  by applying NAT. 
- `kube-proxy`-> forwards the GET action request to `src` as **Node1**, `dest` as **TargetPODs**.
**POST:**
- now in response POST action, TargetPODs responses
- That POST packet contains `src` as **TargetPODs**, `dest` as **Node1**
- After that POST gets to Node1's `kube-proxy` 
- `kube-proxy` resolves it by reversing the NAT. 
- `kube-proxy`-> forwards the POST action request to `src` as back to **Node1**, `dest` back to **client**.
But in a direct overview, Client -> Node1's `kube-proxy` -> DNAT(for GET)/reverseDNAT(for POST) -> TargetPods/


---
# 15. Load Balancer service: Introduction
Load Balancer service Introduction - [Doc]

We have seen the `svc` types as ClusterIP(default), NodePort and now, 
IF YOU WANT TO ACCESS APPLICATIONS ON K8s CLUSTERS FROM OUTSIDE, - **Load Balancer**

`LB` as an `svc` type, debunked! 
- will see how it works
- how it differs than other `svc` types,
- how it differs between On-Prem to Cloud

If we notice, these `svc` are themselves an `LB` right? distributing all these traffic to all the relevant PODs. when `svc` itself an `LB`

**Then why `LB` separately as an `svc` type itself.** 
- The one we have saw are all works within the clusters
- This one is which are going to be placed/configure **outside** the K8s Clusters in order to reach to our apps. 
an **==LB outside the K8s cluster==** --> sending the traffic to the nodes -> that to the `svc`s  - and that to the all the respective PODs.
==`svc` Load Balancer==

**Key points of LB `svc` type:**
1) In order to implement external traffic load balancing, we need to create Service of` type: LoadBalancer `
2) Workflow: 
External Traffic -> Load Balancer -> NodePort (optional) -> Service ClusterIP -> PODs
3) Service LoadBalancer works under *Layer4 load balancing* as per the OSI Model
4) When we create a service, declare `type: LoadBalancer` under spec(`.spec.type=LoadBalancer)
5) We can use imperative approach as well to create a LoadBalancer service. 
```
kubectl expose workloadObject workloadObjName --port=8080 --target-port=80 --name=svcName --type=LoadBalancer
```
6) While using `EKS`, `AKS`, `GKE` etc. cloud K8s services, it uses Cloud Load Balancer `svc` which redirects traffic from the LB to backend Pod's based on configuration
7) In order to disable **NodePort** creation for LoadBalancer Service, set `spec.allocteLoadBalancerNodePort=false`,
8) If we need to set internal LoadBalancer for service on cloud, declare annotations (eg: AWS EKS)
```
metadata:
  name: my-service
  annotations:
    service.beta.kubernetes.io/aws-load-balancer-internal: "true"
```

###### 1) In order to implement external traffic load balancing, we need to create Service of` type: LoadBalancer `
as we just mentioned, If we want to drive an external traffic to balance the load to the cluster and then distribute it inside into the applications - **LoadBalancer** `svc` type.


###### 2) Workflow: 
External Traffic -> Load Balancer -> NodePort (optional) -> Service ClusterIP -> PODs

***how does it flow?*** 
External Traffic (from the client or whatnot) -> Reaches to the **LB** `svc` type (which we configure outside the cluster) --> reaches `nodeport` (varies based on different cloud platforms, if on-prem it exists) if we configure which assess the node Machine's/VM's IP and port (**NodePort** is totally optional) --> **ClusterIP** `svc` --> **Pods**

this way the traffic flows from outside the K8s to the applications inside the cluster. 



###### 3) Service LoadBalancer works under *Layer4 load balancing* as per the OSI Model
**LB** `svc` works on L4 **Transport Layer**4 level Load Balancing (to the core). the layer which are next to just going digital to physical - **Network level load balancing**. not *application level lb*


###### 4) When we create a service, declare `type: LoadBalancer` under spec (`.spec.type=LoadBalancer)
```
  type: LoadBalancer
```
```
spec:
  selector: #labels
	...
  ports:#ports
    ...

  type: NodePort
```
and the `svc` type will be `LoadBalancer`.


###### 5) We can use imperative approach as well to create a LoadBalancer service. 
```
kubectl expose workloadObject workloadObjName --port=8080 --target-port=80 --name=svcName --type=LoadBalancer
```
also, can use Imperative approach to create a **LoadBalancer** `svc` type.


###### 6) While using `EKS`, `AKS`, `GKE` etc. cloud K8s services, it uses *Cloud Load Balancer* `svc` which redirects traffic from the LB to backend Pod's based on configuration
if/when we use cloud or other managed services for K8s such as AWS EKS, Azure AKS, GCP GKE, Rancher, IBM Cloud, Oracle Cloud, Mirantis and such, 

Creating a K8s cluster inside these cloud or other managed services, No need of worrying about managing Load Balancing since it is a managed service for K8s. LB has been taken care. 

> **Need of `LB` is essential to create on to manage and distribute the flow of traffic in ease when we deploy things on-premise since we are the pilot** 
> For cloud or other services, managed services will take care of managing the rest on L4. 


###### 7) In order to disable **NodePort** creation for LoadBalancer Service, set `spec.allocteLoadBalancerNodePort=false`,
as we saw the **NodePort** `svc` is optional which gets created, to enable or disable it, set the `spec.allocteLoadBalancerNodePort`parameter to either **true** or **false**. 

If LB on-premise, create NodePort in order to redirect the traffic to the node. in some cases, need to disable it. To disable it
```
spec.allocteLoadBalancerNodePort=false
```
traffic reaches directly **LB** `svc` to the **ClusterIP** and to the PODs

###### 8) If we need to set internal LoadBalancer for service on cloud, declare annotations (eg: AWS EKS)
```
metadata:
  name: my-service
  annotations:
    service.beta.kubernetes.io/aws-load-balancer-internal: "true"
```
In order to set or create Internal or external LoadBalancer for service on cloud, need to declare some parameters/annotations which is nothing but set of parameters to the object, kind of label but does tasks to change the function or behavior.  (eg: AWS EKS)
```
metadata:
  name: my-service
  annotations:
    service.beta.kubernetes.io/aws-load-balancer-internal: "true"
```
the **LoadBalancer** type that we want to create. If you don't, it creates an internal phasing LoadBalancer as default (which is essential). 

On cloud in the area of load balancing, the load balancing are of two types,
- Internal Load balancing
- Internet phasing
==->tldr: The LoadBalancer svc type is of that we place it outside the cluster to manage traffic and distribute it to the application inside the cluster.  configuring this one varies for different platforms.==
 
Will see both of the same and observe how it behaves based on the platforms. simply on-prem and for cloud. 

In order to implement a L4-LoadBalancer Solution for K8s services on both on-prem or cloud, what are all the ways?
**Onprem (oss):**
1) MetalLB (*popular*)
2) OpenELB
3) PureELB
for on-prem, to deploy L4 LB top our clusters. 
**Cloud(managed services):** 
4) AWS - AWS NLB (Network LB)
5) Azure - ALB - Azure Load Balancer
6) GCP - GCP Cloud Load Balancing
for cloud, to deploy L4 LB top our clusters. 

WILL SEE HOW BOTH OF THESE WORKS!

---
# 16. Introduction to MetalLB for On-premises K8s cluster
Introduction to MetalLB for On-premises k8s cluster - [Doc]

**Will dig deep into the MetalLB and see how it works on-prem.**
- Definition
- how it works
- why MetalLB

MetalLB for On-Premises Kubernetes Cluster (Service type: LoadBalancer)
1) Kubernetes doesn't offer `network load balancer` for `svc` Service Object of type `LB` LoadBalancer
2) For bare-metal cluster, using Service of type `NodePort` and `ExternalIP` is not recommended for Production grade infrastructure.
3) So, MetalLB provides a solution to implemet *Layer-4 LoadBalancer* for Kubernetes `svc` type **LoadBalancer**
**Components of MetalLB:**
- `metallb-system/controller`: It will be running as a **`Deployment`** object in K8s cluster that handles IP address assignments
- `metallb-system/speaker`: It will be installed as **`DaemonSet`** to make sure that the `svc` is reachable of protocol type enabled. 
**Requirements:**
- K8s Cluster version >= 1.13.0
- CNI that supports MetalLB *(**calico**, antrea, canal, cilium, flannel, kube-router, wave net, Kube-ovn)*
- Free IP address pool to be allocated to **MetalLB**
- L2 operating mode, ensure `7946` port is allowed between nodes (if firewall is enabled)


##### **MetalLB for On-Premises Kubernetes Cluster (Service type: LoadBalancer)**
###### 1) Kubernetes doesn't offer `network load balancer` for `svc` Service Object of type `LB` LoadBalancer
So , K8s does not offer **L4 Network Load Balancer** for `svc` object of LB by itself inertly. 
doesn't do LB by itself. It offers LoadBalancer `svc` type as a service type but it doesn't do the actual work of Load balancing which has been done by other one which is **MetalLB** to do the actual load balancing for your application and that too is on **Transport layer L4 Load Balancer**. aka Network load balancer.  

###### 2) For bare-metal cluster, using Service of type `NodePort` and `ExternalIP` is not recommended for Production grade infrastructure.

For a bare-metal production grade K8s Cluster setup `NodePort` and `ExternalIP` are not recommended for the `svc` type. 
Suits for `dev` environments but not for production. We should not expose our application using `NodePort` and `ExternalIP` which is on an on-prem BareMetal production setup.

Due to security concerns, such as 
- Directs access to firewall
- Opens ports with broader access
- Unnecessary clutter of opening random ports

Alternate to these are we are looking at `LoadBalancer` using **MetalLB** for on-prem and other relative `svc`'s as well. Will dig deep into all of that. 

###### 3) So, MetalLB provides a solution to implement *Layer-4 LoadBalancer* for Kubernetes `svc` type **LoadBalancer**
For On-prem bare metal server, had to do like these. An external Load Balancer component to do the load balancing which is **MetalLB**. No need for cloud or other managed services, since those are self managed. 

For these MetalLB, there are two components inherited to it,
- controller
- speaker
These two components of MetalLB will be running on its separate namespace - `metallb-system` namespace, not will be in default or K8s namespace. 

Will be created when you deploy applications On-prem bare metal cluster. 
In these `metallb-system` namespace for each, each of these component of `metallb`,
- the `metallb`'s `controller`  will be running as a `Deployment` object - managing the IP addresses of the node.
-  the `metallb`'s `speaker`  will be running as a `daemonset` object - running on all the available node to handle the protocol of the `svc` that uses

###### **Components of MetalLB:**
- `metallb-system/controller`: It will be running as a **`Deployment`** object in K8s cluster that handles IP address assignments - ==as an `svc`, it handles IP address of the cluster==
- `metallb-system/speaker`: It will be installed as **`DaemonSet`** to make sure that the `svc` is reachable of protocol type enabled. - ==runs on all the available nodes, handles/manages protocol of the svc that it uses.== these are handled by the `speaker`. 

###### **Hardware Requirements:** for`MetalLB`
- K8s Cluster version >= 1.13.0 (more than)
- CNI that supports MetalLB *( supported cni's: **calico**, antrea, canal, cilium, flannel, kube-router, wave net, Kube-ovn)*
- Free IP address pool to be allocated to **MetalLB** in order to make it work it as an `LB`. will see how to allocate one. 
- L2 operating mode (all nodes can communicate with each other), ensure `7946` port is allowed between nodes (if firewall is enabled)

These are the crucial point to consider before moving on with MetalLB. Will see how to implement one. 

---
# 17. Deploying MetalLB on cluster(On-premises)
Deploying MetalLB on cluster - [Doc]

How to setup **MetalLB** on-prem K8s Cluster practically in order to implement the **LoadBalancer**`svc` type. 

Refer: [MetalLB's Official Documentation](https://metallb.io/installation/)

There are three ways/methods to deploy **MetalLB** Load balancer on our K8s Cluster. 
1) K8's manifest based (declarative)
2) using Kustomize
3) using Helm charts

Will go with 1) writing a manifest, nothing but writing a manifest and apply to create the object. (as we are creating so far)

Prerequisites before deploying the **MetalLB** Load Balancer, check `kube-proxy`'s status and the ***mode*** that we are using

If we revise about `kube-proxy`, which have two modes. 
- `ipvs`
- `iptables` (default) 
For **MetalLB** - `ipvs`. How do you verify that if im using `ipvs` or `iptables`? by checking the *logs* of the `kube-proxy`:
```
kubectl get po -o wide -n kube-system 
```
here, we'll be filtering out all the **PODs** running under `kube-system` namespace, which are literally all the K8s components. There lies the `kube-proxy`.

There seems to be **two** `kube-proxies`. which are nothing but on for `controlplane` and other for `workernode`'s. 
Pick the `workernode`'s `kube-proxy` which can be identified under the NODE column. 

Now will check the logs. 
```
kubectl logs kubeProxyPodName -n kube-system
```
 
```bash
kubectl edit configmap -n kube-system kube-proxy
```

---
# 18. Advanced: Traffic flow while using LoadBalancer service type
Service Load Balancer type creation using Imperative approach - [Doc]



---
# 19. ExternalIP service: Introduction
ExternalIP service Introduction - [Doc]



---
# 20. Overview on Service type ExternalIP


---
# 21. ExternalName service: Introduction
ExternalName service Introduction - [Doc]


---
# 22. Overview on Service type ExternalName



---
# 23. Headless service: Introduction(ClusterIP: None)
Headless service Introduction - [Doc]


---
# 24. Overview on Service type Headless(ClusterIP: None)
Overview on Service type Headless - [Doc]


---




---




