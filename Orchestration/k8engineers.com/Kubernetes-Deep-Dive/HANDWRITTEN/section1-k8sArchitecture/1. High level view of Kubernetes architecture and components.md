Architecture and Components - [Doc](obsidian://open?vault=tutorialHell&file=Orchestration%2Fk8engineers.com%2Fofficial%2FMaster%20Docker%20and%20Kubernetes%2FCommon%20components%20for%20Control%20plane%20and%20Compute%20plane%20nodes)

**Kubernetes Architecture in brief:** For
1) Single Node &
2) Multi Node

Here, the **single node** is that
	1. control plane & components + 1 compute/worker components = runs on the ***same** One single node.*
the same works like this (in the below given image)
![[Pasted image 20240624111946.png]]
> Good for practicing and experimentation purposes when you lack in resource

**multi node** (irl, we use two on more than that)
	2.  all of those nodes runs on its own node. 
	Control plane works on control plane node, compute plane works on compute plane node.
the same works like this (in the below given image)
![[Screenshot 2024-06-24 110726.png]]
Control Plane Node:
- etcd
- Kube-API-server
- Kube-Scheduler
- Kube/Cloud-Controller manager
> Production purposes, In real time workloads.   

Compute/Worker Plane Node:
- Kubelet
- Kube-proxy
- CRI-containerd -> 
- + Pods

> [!NOTE: when you refer K8's Architecture]
> Theorotically, we differentiate the nodes technically as its own, but
> Practically speaking, them components works on the same node 
![[Pasted image 20240624111946.png]]
Plus, you can refer the same here
.
Also, you can see in the diagram that, there are three nodes -
called - **HA (High Availability) SETUP.**

Multi nodes configured in a manner name HA Setup.
**HIGH AVAILABILTY = 3 OR 5 OR MORE CONTROL Plane gets running**. This get managed by LOAD BALANCER.
###### WHY HA Setup?
if there is one control particular control plane is running and the work or a particular job is down, the rest (the application) would still exist and will keep on working.

if IRL, Prod Env with multi-node
**Kubectl**   -- *req* --> **LoadBalancer** (gets distributed)-- *req* -->  **apiserver**
> **HIGH AVAILABLITY SETUP**
>will the more on the same in practice.

> THESE ARE ALL THE HIGH LEVEL OVEREVIEW OF THE POINTS THAT WE'VE COVERED.
So far, covered the architectures of -
1) Single node setup
2) Multi node setup
3) High Availability (HA) Setup

How to communicate or setup and configure such environment:
###### **Kubectl** - the way admins can interact with K8s clusters - Admins 👨‍🏭
-- deploy applications
-- administrate them
-- develop on the same

when the users try to access the application -> they get redirected to 
###### Kube-proxy - users gets here 👤
takes the responsibility of sending the traffic to the relevant/appropriate application.
running as a container inside the kubernetes cluster. **This also runs as a container inside the k8s cluster.**

this is the high-level overview so far. In detail, refer the diagram given below.
![[highlevel-k8sArchitecture.excalidraw]]
**USECASE**:


**Objective**: As an administrator, you have three pods which are created logically. As an admin, **you want to spin up three pods** like the same as the diagram.

> [!NOTE] ***POD***
> >POD: one or a group of containers

**Action**:
Where the administrator's request will be sent?

#### CONTROL PLANE
👤(spin three pods) **kubectl** 🐚  --- *req* ---> LB -- *req* --> **kube-Apiserver** ☸️

> [!NOTE] ***API-SERVER***
>  ***Apiserver*** - > the first component who will receive the information or command from the administrator via kubectl.
>Communicates with each of the kube-components but the rest are all isolated and have no business to do with the rest of the components. 
>
>	All Components --- only through ---> **kube-apiserver**☸️
>	**kube-apiserver**☸️ --- can talk to --->  All components
>	
>	`there will be all the authentication and authorization in-order to communicate with each other`

**kube-apiserver**☸️--*req*--> **etcd** 🫙 (checks the request  exist or not) --*res*-> **kube-apiserver** ☸️

> [!NOTE] ***etcd***
>***etcd*** - a distributed key-value kube datastore component
-- if exist -> throws the existing response
-- if not -> stores it and send back the response
response = a.k.a **Acknowledgement**

**kube-apiserver**☸️(decide where to launch the app) --*pass Info* --> **kube-scheduler** ⌛ (validates and schedules job) -- *reply* --> **kube-apiserver**☸️
> [!NOTE] ***kube-scheduler***
> validates each worker node, pods in each node, picks the available & suitable pod (for relevant config). schedules deployment)

**kube-apiserver**☸️(datastores the config)-->*sends*--> **etcd**🫙-->*acknowledges* -->**kube-apiserver**☸️

**kube-apiserver**☸️--> *launches the pod* --> (available) **Kubelet**🛞 [on Worker node] (which was decided by the **scheduler**)

then, what is the purpose of Control manager?
**control-manager** 🛂--> updates and commands all the info of the cluster to  -->  **kube-apiserver**☸️ (--> that stores the same to etcd🫙)
> [!NOTE] ***kube-controller manager***
> it will watch/ overlook the entire kubernetes cluster. Gather all the information about the cluster such as availability, load, status, errors, faults, downtimes, mismatch, self-heal and more.

#### WORKER NODE / COMPUTE PLANE
who receive all these commands and information of all these configuration get defined within these nodes and clusters.

**kube-apiserver**☸️ -- (*config*) --> **kubelet**🛞 (receives info)
> [!NOTE] ***Kubelet***
>  Who should be the one who takes input from **kube-apiserver**☸️ to create container,  **kubelet**🛞 not to the CRI itself
>  
> >  **kube-apiserver**☸️ -> input --> **kubelet**🛞  --> **CRI** ⚙️ --> creates -->  POD 🪣

**kubelet**🛞-- *talks to* --> **CRI** ⚙️(container runtime interface) --> POD 🪣 (creates them in the backend)

> [!NOTE] ***CRI***
> CRI - Container Runtime Interface. K8s doesnt create containers in the backend. its is a platform. Container Runtime does! 
These Container runtime interfaces talks to the CONTAINER ENGINE WHICHEVER THAT WORKS ON THE SAME. such as Containerd, Docker-Engine, CRI-O

> WILL SEE MORE ON THE SAME CONTROL PLANE NODE AND COMPUTE PLANE WORKER NODE AS SEPERATE IN DETAIL and see what are all the responsibility of each of these components briefly.

---