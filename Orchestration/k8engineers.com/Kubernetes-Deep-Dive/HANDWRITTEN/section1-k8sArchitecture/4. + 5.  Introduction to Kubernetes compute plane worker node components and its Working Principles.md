###### Common components on COMPUTE WORKER PLANE 
just for all the recap, copying and referring the same.=
![[Pasted image 20240625165735.png]]
#### Kubelet🛞
> [!NOTE] **1) Kubelet 🛞**
> is the one who is responsible to create all the PODS and the containers in the backend not by itself but who that gets the input from the **apiserver**!
>  > **kube-apiserver**☸️ -> input --> **kubelet**🛞  --> **CRI** ⚙️ --> creates -->  POD 🪣
> 
> note: Kubelet will be running on both **CONTROL PLANE** AND **WORKER NODE** AS WELL. (client -server model)
>********
Who should be the one who takes input from **kube-apiserver**☸️ to create container,  **kubelet**🛞 not to the CRI itself

**kubelet** - will be running on both control and compute plane as well. 
> ***logically speaking***. At the end of the day, all of the components are simply a static pod, which is obviously created, managed and controlled directly by the kubelet. SIMPLE!

This component will be running as a **daemon service** on all the nodes in the cluster not a static pod. obviously!
> **logically speaking** kubelet is to be the one who is supposed to be create PODS, how a POD creator can be end up as a static POD who creates it by themselves?

All the control plane nodes are STATIC PODS but not the Worker node Pods which are 
**daemon-service** -> is a service running on the node in the backend. 

OKAY! BIG QUESTION?
How will i be able to run if the **kubelet**🛞is a **daemon-service? ==Systemctl==**

>Once the decision has been made and the POD gets scheduled on a node where the input of this came from **kube-apiserver**☸️,
> > **kubelet**🛞 service will makes sure all the PODS and Containers gets launched for the POD by 
> > `pulling images --> create/build the same with the image + the app -->  start/spin up/launch the containers.
On the node, where the POD has been scheduled.

Here, the daemon service of it (a binary or a daemon system itself) has to have a **workdir** or **rootdir** to make this thing all work? How that works? Here, the naming convention of it named as

> 	**DATA DIRECTORY**:
> the kubelet config files will be stored under `/var/lib/kubelet`
even all the POD + Kubelet related information within itself will get stored under this **data directory**. a.k.a ==**kubelet data directory**==

**==BACKUPS FOR THESE ARE MUST AS WELL.==** If needed.

  
> [!NOTE] !IMPORTANT
> **Static PODS** are controlled by Kubelet service which is a **daemon** by itself
> (apart from PODS scheduled by **apiserver**) 
> -> that includes the Control Plane components too!
> 
Instead of using REST API calls, to launch pods with the help of **kube-apiserver**☸️, we use Who should be the one who takes input from **kube-apiserver**☸️ we use **kubelet**🛞 service itself not by **apiserver**
.
PODS like such which was created and controlled by the **kubectl** but not with **apiserver** which are static and you can do nothing on it. -> THOSE ARE CALLED AS 
> > **STATIC PODS**
>  
what i mean by nothing means?
  even if you ***try to delete the POD*** means, The Static PODs will get **auto-spined up**
> > BECAUSE IT IS CONTROLLED BY **KUBELET🛞**

When the POD gets failed? what happens BTS?
**KUBELET🛞** will restart the POD. THE CONTAINER GETS RECREATED/RESTART `SELF HEALING`
BTS -> Kubelet will take care of it. RESTART = RECREATING THE CONTAINER IN THE BACKEND!

 >even the storage of the POD Workload is controlled/managed by **KUBELET🛞**
 eg: volumes with NFS, hostpath, volume binding and more, THERE **KUBELET🛞** takes care of it too creating the particular volume/storage for your workload/POD.

& default port of **KUBELET🛞** will be `10250`.
IF SOMEONE WANTS TO COMMUNICATE WITH MY **KUBELET🛞** SERVICE. LISTENS TO THIS PORT. 

> ==**PORT HAS TO BE OPENED ON BOTH OF THE NODES==** = CONTROL PLANE + WORKER NODE too.

---
#### CRI⚙️
> [!NOTE] **2) CRI ⚙️**
> CONTAINER RUNTIME INTERFACES? -> What are all that?
> 1) Containerd
> 2) docker
> 3) cri-o and more.
> > will be using **containerd** for the same to stay relevant
>********
> CRI - Container Runtime Interface. K8s doesn't create containers in the backend. its is a platform. Container Runtime does! 
These Container runtime interfaces talks to the CONTAINER ENGINE WHICHEVER THAT WORKS ON THE SAME. such as Containerd, Docker-Engine, CRI-O
> > -- + IT EXISTS ALL THE NODES BOTH CONTROL PLANE AND COMPUTE PLANE NODES INSIDE THE CLUSTER.

**CRI - CONTAINER RUNTIME INTERFACE:**
is **plugin** who will takes care of the responsibility of  Kubernetes to create containers/PODS in the backend by using another Container runtime (containerization) tools. such as Docker, Containerd, CRI, Merantis CR and more,
the tools that you can integrate. 
> Kubernetes alone cannot able to create PODS/ Containers in the backend. Thats why the plugin created to integrate compatible container tools to create all these PODS and Containers.

	SUPPORTS MULTIPLE CONTAINERIZATION TOOLS THAT CAN BE INTEGRATED TO K8s CLUSTER GIVING US THE FLEXIBILTY TO SPIN UP WHATEVER CONTAINERS IN EASE.

some of the CRI tools:
1) Docker Engine (deprecated)
2) Containerd (default, standardized and recommended) -> CNCF project
3) CRI-O 

*why docker engine got deprecated + will analyze the* **workflow of CRI:**
#### containerd v1.0
![[Pasted image 20240625184914.png]]
  
###### Diagram 1:
previously when Docker Engine was the one being the default one. as you can see, 
  ***Docker version of CRI***
  =>  **Kubelet🛞** < - *CRI* - >  **dockershim** < - - - > **Docker Engine** < - - - >  **containerd** < - > >> PODS/CONTAINERS

why docker got deprecated and the value of efficiency and latency,  -> => *lets take a scenario of launching number of containers a pod*
1) earlier, **Kubelet🛞**(*takes the information from **apiserver** of what to create*)  
2) --> talks to **CRI**⚙️'s (*takes* help from it to create the Pods)
3) ==Inner workings of the previous versions  CRI==  -> **CRI** ⚙️takes **dockershim** (previous docker's runtime which was default at that *time*) ---> this **dockershim** talks to **Docker Engine** (*basically **docker daemon** + its **CLI***) ---> (***daemon** the talks* to) **containerd** => that creates all the container. 
![[Pasted image 20240626110358.png]]
> THE PROBLEM HERE IS:
> 1) Too many gates, Too many obstacles, ==LOTS OF LATENCY== 

###### Diagram 2:
SEE HOW THE CURRENT VERSION OF IT **RESOLVES THE PROBLEM**:
  ***containerd v1.0*** 
  =>  **Kubelet🛞** < - *CRI* - >  **cri-containerd** < - - - > **containerd** < - > >> PODS/CONTAINERS

same here, 
1) **Kubelet🛞** -> talks to **CRI**⚙️(*passes the input of what to create*)
2) **CRI**⚙️-> **CRI-containerd** the client side of the tool (*the CRI itself*) (*having the information*)
3)  **CRI-containerd** -> (*communicates directly to*) **containerd** (*creating the containers on the POD right away*)
4) **containerd** ->>> CREATES ALL THE CONTAINERS
![[Pasted image 20240626110430.png]]
> Here, THE SOLUTION is that:
> 1) We have avoided the latency of 
> => SKIPPING the **CRI⚙️**  -> taking the dockershim (the docker engine client) --> to **DockerEngine** (the daemon + CLI) --> and then to **containerd**  -- > to create the CONTAINERS on a POD
==AVOIDING THE LATENCY WORKING *WITH THE WORKFLOW OF DOCKER*, Instead the **Kubelet🛞** directly making communication with **CRI**⚙️PLUGIN TO TAKE **cri-containerd** to spin up containers on the PODS== .
	they created containerd integrated with kubelet -> creates the containers on the PODS. 
**VIOLA!**
*****
> WHY **containerd** when we have **DockerEngine**?
>  > There has to be something that got to be under K8's Worker node's **Kubelet🛞**'s Control -> To define such container's as PODS to working with it in ease. 
>  > 
>  > If isn't the case with this practice, the CONTAINER WILL END UP HOUSED UNDER ***DOCKER ENGINE*** which is such a pain to administrate.
 => so that daemon talks to containerd giving all the containers and Making it work as a POD

>THESE THINGS DOESN'T MATTER WHEN WE WORK WITH IT. 
>BUT TO HAVE AN INSIGHT OF WHAT IS WORKING UNDER THE HOOD/BACKEND
>
>THE VALUE OF LATENCY  AND EFFICIENCY WILL MEAN NOTHING FOR A SMALLER WORKLOADS. 
>BUT IF THE CLUSTERS ARE LARGER IN SIZE AND WIDER TO SCALE ->  PLAYS A MAJOR ROLE -> GETS SHIT DONE QUCIKER AND SHIP THINGS FASTER WITHOUT ANY HASSLE.
>> due to latency and for efficiency, **dcokerengine** got DEPRECATED. STOPPED SUPPORT from v1.24 and the **containerd** got standardized, default and recommended from the same v1.24.

***
--> LEAVE THIS ALL <--
we got **containerd v2.0**, we see what is it about in brief.
#### containerd v2.0
![[Pasted image 20240626113111.png]]
WHAT THIS HAS TO OFFER/ WHAT SPECIAL ABOUT IT?
> NOTHING NEW. AS SAME AS BEFORE BUT EVERYTHING GOT STREAMLINED!

here in the diagram in brief, you can see the 
1) the highlighted crossed out section is **containerd v1.0**, where
-> the **Kubelet🛞** -> **DockerEngine** -> **dockerd**(daemon + CLI) -> **containerd** -> containers on the POD
(instead of kubelet talks to docker -> talks to dockerd -. talsk to containerd)

2) the pointed out green on is **containerd v2.0**, where
-> the **Kubelet🛞** -> (previously the **CRI** ⚙️plugin was standalone to work with whatever containers)
here the plugin coupled with **containerd** itself. -> (now **Kubelet🛞** directly talks to **containerd**) --> creates all the containers as Pods. ( ==**v2.0** (*which reduces more latency and gains efficiency*)== > *v1.0*)

> THIS DOCUMENTATION IS RECORDED IN **v2.4**
---
#### **kube-proxy**🔀

> [!NOTE] **3) kube-proxy 🔀**
> network proxy. If you'd like to access the application that is sitting inside the K8s Cluster. The request will comes here in this **kube-proxy**
> 
> In the backend, it'll check all the entries in all the **Iptables** or IPV's and takes decision to send the traffic to the respective application in the backend. 
> > User 👤 -->  Node  --> **kube-proxy**🔀 (redirects) -- PODS (in theory)
> > 
> > technically in overview:
> > frontend CLIENT  ---> Service (LB with RR Algo) ---> **kube-proxy**🔀(with IPTables) ---> PODS x3🪣
> > 
> > technically in brief:
> >   ---> (*takes input from **kube-apiserver***☸️) 
> >   ---> **kubelet**🛞  --> **CRI** ⚙️ --> containerd  ->  creates -->   Service (LB) ---> **kube-proxy**🔀 ---> PODs 🪣
> > 
>********
> a proxy server which takes care handling the network managing things such as communication between the Pods and traffic. Kube-proxy knows how to send/redirect the traffic where it has to point to.

1.
> ==NOTE: THAT IT IS RUNNING ON BOTH THE NODES, **CONTROL PLANE** NODE + WORKER **COMPUTE PLANE** NODE==
> EVEN THOUGH, THIS IS A COMPUTE PLANE COMPONENT

**kube-proxy**🔀runs on each node of the Kubernetes cluster.

2.
Handles <b><u>network-proxy</u></b> on each node, where it watches control plane for the **Addition or Removal of Service and EndpointSlice Objects**. (if it didn't make sense),
eg: ***TRYING TO CREATE A POD:***

**CLIENT** [FRONTEND APPLICATION for eg] ---> (the requests will be sent only to the service) **Load Balancer** (=is a **Service**)  (with Round Robin algorithm) - - - > Backend Apps (endpoints = )
|endpoint1  |  endpoint2 | endpoint3|  

| POD1       | POD2       | POD3       |
| ---------- | ---------- | ---------- |
| containers | containers | containers |
![[Pasted image 20240626141922.png]]
Here, if the Client as frontend getting all the traffic, requests and such,
To whom does it connect to? Since you have 3PODS as Backend which each of has IP address as its own .
	Will we be changing each of the PODS one on one in the backend? NO! 
We should not and not supposed to do any manual task. To tackle and distribute the traffic/requests to each of the POD, should integrate **LOAD BALANCER**. 
> The implementation of a LOAD BALANCER  in any of the K8s Environment => **==Service==**  
 
These **Services** will have **EndpointSlices**. which means that each destinations of the requests/ traffic that ends up name **Endpoint**. So the LB is integrated with the **backend endpoints**.
>NO NEED TO WORRY ABOUT THE PODS AT ALL, THE REQUEST WILL NOT BE GOING TO THE BACKEND PODS but to the **Services** which is **LB** here. 
>SO THE SERVICE WILL RUNNING INTERNALLY TO LOAD BALANCE THE TRAFFIC.

*WHO WILL BE THE ONE SENDING TRAFFIC FROM THE ==CLIENT==  --> TO THE ==SERVICE== (which is a Load Balancer in this case)  --> to the ==Endpoints== (which are Backend PODS)?* => **kube-proxy**🔀. How?
> **kube-proxy**🔀 have **Iptables** in the backend of it. (a linux kernel feature). With that Iptables concept, it will understand that IF THIS REQUEST COMING TO THIS  IP of this **Service** (which is either a LB, Proxy or DNS).
> ***Determines (how it has to be sent)***, that request has to be redirected to the backend PODS. 
> This will be controlled by the ==**kube-proxy's**🔀Iptables== 

3. 
**kube-proxy**🔀: Supports different proxy-modes like:
1) user space (old and deprecated)
2) iptables or IPVS (linux-kernel feat uses netlink) //if you fail to mention anything, it will proceed with the default.
-> used to stored the network details. IF THIS PACKET SENT TO THIS PARTICULAR IP, TO WHOM I NEED TO SEND IN THE BACKEND. 
		These type of information will be stored in all the **kube-proxy**🔀iptables. 
4.
Handles all the traffic inside and outside of the cluster to the respective PODS. 
> Ultimately, the responsibility of the **kube-proxy**🔀is nothing but IF YOUR REQUEST COMING FROM INSIDE or OUTSIDE the cluster? According to the request, redirects the traffic to the relevant POD. in the backend behind the service.

5.
When the traffic reaches the service (ClusterIP or DNS), Kube-proxy redirects the traffic based on the rules (round robin algo) to the Backend PODS. [the picture for reference we saw previously]. Request -> Service => by either a ClusterIP or DNS name. After reaching there, the traffic has to go the backend PODs. (who does that job?) -> **kube-proxy**🔀
![[Pasted image 20240626145730.png]]
For better understanding:
can see the **kube-proxy**🔀 here, 
--> **apiserver** --> WHATEVER --> **kube-proxy**🔀or **Client** -> vIP OR dns -> Iptables --> redirects --> POD