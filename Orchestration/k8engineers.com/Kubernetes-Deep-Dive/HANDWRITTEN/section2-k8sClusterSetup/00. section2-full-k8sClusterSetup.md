### SectionÂ 2:Â K8S Setup
#### 1. Â Tools to setup kubernetes cluster and Cloud services
###### Cluster setup - [Doc](obsidian://open?vault=tutorialHell&file=Orchestration%2Fk8engineers.com%2Fofficial%2FMaster%20Docker%20and%20Kubernetes%2Fsection2%2FTools%20to%20setup%20kubernetes%20cluster%20and%20cloud%20service)
#### 2. Â Overview on single/multi node setup using kubeadm tool
###### Overview of setup - [Doc](obsidian://open?vault=tutorialHell&file=Orchestration%2Fk8engineers.com%2Fofficial%2FMaster%20Docker%20and%20Kubernetes%2Fsection2%2FOverview%20of%20Single%20or%20Multi%20node%20kubernetes%20setup%20using%20kubeadm%20tool%20and%20containerd%20CRI)
#### 3. Â Why Containerd not Docker? (k8s dropped support for Docker)
###### Containerd over Docker - [Doc](obsidian://open?vault=tutorialHell&file=Orchestration%2Fk8engineers.com%2Fofficial%2FMaster%20Docker%20and%20Kubernetes%2Fsection2%2FWhy%20are%20we%20using%20containerd%20over%20docker%20in%20kubernetes)
#### 4. Â Kubernetes single node setup using kubeadm tool(cri-containerd)
#### 5. Â Kubernetes multi-node setup using kubeadm tool(cri-containerd)
###### Multi-Node Setup - [Doc](obsidian://open?vault=tutorialHell&file=Orchestration%2Fk8engineers.com%2Fofficial%2FMaster%20Docker%20and%20Kubernetes%2Fsection2%2FMulti-Node%20Setup)
#### 6. Â Part1: Kubernetes HA setup using kubeadm tool(cri-containerd)
#### 7. Â Part2: Kubernetes HA setup using kubeadm tool(cri-containerd)
#### 8. Â Part3: Kubernetes HA setup using kubeadm tool(cri-containerd)
#### 9. Â Part4: Kubernetes HA setup using kubeadm tool(cri-containerd)
#### 10. Â Part5: Kubernetes HA setup using kubeadm tool(cri-containerd)
***
#### Kubernetes Setup Tools and Services

need to know what are all the tools and services to setup Kubernetes on both on-premise and on the cloud.

## 1. Tools to setup kubernetes cluster and Cloud services
### **on-prem:**
###### 1) Kubeadm 
 is a tool which is used for a secure production grade-level deployment. 
if you want to create a Kubernetes cluster on-premise on production level, Kubeadm is the g-to solution.

According to Redhat, `Kubeadm`Â provides knowledge of the life-cycle management of Kubernetes clusters, including self-hosted layouts, dynamic discovery services, etc. Had it belonged to the newÂ [operators world](https://www.redhat.com/en/topics/containers/what-is-a-kubernetes-operator?intcmp=701f20000012ngPAAQ), it may have been named a "Kubernetes cluster operator."

If to practice Kubernetes, setting up a kubernetes cluster where it is a single node, multi-node or HA Setup. Kubeadm will work just fine! 
> Moreover, all the Kubernetes certifications like  CKA, CKS, CKAD, They will be deploying kubernetes on **Kubeadm**. Applies to the examination as well.

Reason why? the series of course covering the same in order to setup K8s setup for single node,, multi-node and HA Setup. 
***
###### 2) minikube
for single node (one compute plane + one worker node). Practicing K8s here deploying and setting up the same in the local system and deploy applications, minikube would be a wise solution. easy to manage.
> having some in-built addons enable and disable on fly in ease just using commands making API calls.

According to docs,
minikube quickly sets up a local Kubernetes cluster on macOS, Linux, and Windows. We proudly focus on helping application developers and new Kubernetes users.
Highlights
- Supports the latest Kubernetes release (+6 previous minor versions)
- Cross-platform (Linux, macOS, Windows)
- Deploy as a VM, a container, or on bare-metal
- Multiple container runtimes (CRI-O, containerd, docker)
- Direct API endpoint for blazing fastÂ [image load and build](https://minikube.sigs.k8s.io/docs/handbook/pushing/)
- Advanced features such asÂ [LoadBalancer](https://minikube.sigs.k8s.io/docs/handbook/accessing/#loadbalancer-access), filesystem mounts, FeatureGates, andÂ [network policy](https://minikube.sigs.k8s.io/docs/handbook/network_policy/)
- [Addons](https://minikube.sigs.k8s.io/docs/handbook/deploying/#addons)Â for easily installed Kubernetes applications
- Supports commonÂ [CI environments](https://github.com/minikube-ci/examples)
***
###### 3) kubespray
if kubespray, i can deploy this particular Kubernetes Cluster on-premise and on cloud as well. AWS, Azure, GCP whatever. Can spin up kubernetes cluster over there by using kubespray. On-premise data center too. No problem. Works just fine.

According to its docs, with the help of various automation tools. Kubespray is a combination of Kubernetes andÂ [Ansible](https://www.ansible.com/?intcmp=701f20000012ngPAAQ). That means we can install Kubernetes using Ansible. We can also deploy clusters usingÂ `kubespray`Â on cloud compute services like EC2 (AWS). 

- Can be deployed onÂ **[AWS](https://kubespray.io/#/docs/cloud_providers/aws), GCE,Â [Azure](https://kubespray.io/#/docs/cloud_providers/azure),Â [OpenStack](https://kubespray.io/#/docs/cloud_providers/openstack),Â [vSphere](https://kubespray.io/#/docs/cloud_providers/vsphere),Â [Equinix Metal](https://kubespray.io/#/docs/cloud_providers/equinix-metal)Â (bare metal), Oracle Cloud Infrastructure (Experimental), or BareMetal**
- **Highly available**Â cluster
- **Composable**Â (Choice of the network plugin for instance)
- Supports most popularÂ **Linux distributions**
- **Continuous integration tests**
- Kubespray provides deployment flexibility. It allows you to deploy a cluster quickly and customize all aspects of the implementation.
- Kubespray strikes a balance between flexibility and ease of use.
- You only need to run one Ansible playbook and your cluster ready to serve.
> a moderate k8s setup tool. does generic configuration management tasks from the "OS operators" Ansible world, with some initial K8s clustering (with networking plugins included) and control plane bootstrapping. consume life-cycle management domain knowledge and offload generic OS configuration tasks from it.
> 
>  nothing but generic but kubeadm providing certs and secure setup in ease.

***
###### 4) KOPS - Kubernetes Operations
kOPS by RedHat inc. Kubernetes Operations. `kubectl`Â for clusters. 

According to docs, `kops`Â will not only help you create, destroy, upgrade and maintain production-grade, highly available, Kubernetes cluster, but it will also provision the necessary cloud infrastructure. [AWS](https://kops.sigs.k8s.io/getting_started/aws/)Â (Amazon Web Services) andÂ [GCE](https://kops.sigs.k8s.io/getting_started/gce/)Â (Google Cloud Platform) are currently officially supported, withÂ [DigitalOcean](https://kops.sigs.k8s.io/getting_started/digitalocean/),Â [Hetzner](https://kops.sigs.k8s.io/getting_started/hetzner/)Â andÂ [OpenStack](https://kops.sigs.k8s.io/getting_started/openstack/)Â in beta support, andÂ [Azure](https://kops.sigs.k8s.io/getting_started/azure/)Â in alpha.

**Features**
- Automates the provisioning of Highly Available Kubernetes clusters
- Built on a state-sync model forÂ **dry-runs**Â and automaticÂ **idempotency**
- Ability to generateÂ [Terraform](https://kops.sigs.k8s.io/terraform/)
- SupportsÂ **zero-config**Â managed kubernetesÂ [add-ons](https://kops.sigs.k8s.io/addons/)
- Command lineÂ [autocompletion](https://kops.sigs.k8s.io/cli/kops_completion/)
- YAML Manifest Based APIÂ [Configuration](https://kops.sigs.k8s.io/manifests_and_customizing_via_api/)
- [Templating](https://kops.sigs.k8s.io/operations/cluster_template/)Â and dry-run modes for creating Manifests
- Choose from most popular CNIÂ [Networking](https://kops.sigs.k8s.io/networking/)Â providers out-of-the-box
- Multi-architecture ready with ARM64 support
- Capability to add containers, as hooks, and files to nodes via aÂ [cluster manifest](https://kops.sigs.k8s.io/cluster_spec/)

> But to break, despite all the gimmicks, kOPS are very much friendly with Cloud. Especially AWS and GCP. Yet to support on other cloud platforms too.

***
###### Kind - Kubernetes in the Docker
a simple tool. low level tool which help you setup the same single node single cluster as containers. Instead of VM or BareMetal, Things work on a Docker Container. means, the control plane and Worker node = will be as a Container. Inside, we will be running al of our PODS. -

According to docs, [kind](https://sigs.k8s.io/kind)Â is a tool for running local Kubernetes clusters using Docker container â€œnodesâ€.  
kind was primarily designed for testing Kubernetes itself, but may be used for local development or CI.
kind consists of:
- GoÂ [packages](https://github.com/kubernetes-sigs/kind/tree/main/pkg)Â implementingÂ [cluster creation](https://github.com/kubernetes-sigs/kind/tree/main/pkg/cluster),Â [image build](https://github.com/kubernetes-sigs/kind/tree/main/pkg/build), etc.
- A command line interface ([`kind`](https://github.com/kubernetes-sigs/kind/tree/main/main.go)) built on these packages.
- DockerÂ [image(s)](https://github.com/kubernetes-sigs/kind/tree/main/images)Â written to run systemd, Kubernetes, etc.
- [`kubetest`](https://github.com/kubernetes/test-infra/tree/master/kubetest)Â integration also built on these packages (WIP)
kind bootstraps each â€œnodeâ€ withÂ [kubeadm](https://kubernetes.io/docs/reference/setup-tools/kubeadm/kubeadm/). For more details seeÂ [the design documentation](https://kind.sigs.k8s.io/docs/design/initial).
**NOTE**: kind is still a work in progress, see theÂ [1.0 roadmap](https://kind.sigs.k8s.io/docs/contributing/1.0-roadmap).
***
###### MicroK8s
by Ubuntu. MicroK8s is a tool especially designed for Ubuntu. v20-22.04 and all. Good for practice.

According to the docs, MicroK8s - Zero-ops Pure Upstream HA Kubernetes for developers, edge and IoT from dev workstations to Production. **MicroK8s**Â is the simplest production-grade conformant K8s. Lightweight and focused. Single command install on Linux, Windows and macOS. Made for devOps, great for edge, appliances and IoT. Full high availability Kubernetes with autonomous clusters and distributed storage.

MicroK8s is an open-source system for automating deployment, scaling, and management of containerised applications. It provides the functionality of core Kubernetes components, in a small footprint, scalable from a single node to a high-availability production cluster.

By reducing the resource commitments required in order to run Kubernetes, MicroK8s makes it possible to take Kubernetes into new environments, for example:

- turning Kubernetes into lightweight development tool
- making Kubernetes available for use in minimal environments such as GitHub CI
- adapting Kubernetes for small-appliance IoT applications

Developers use MicroK8s as an inexpensive proving ground for new ideas. In production, ISVs benefit from lower overheads and resource demands and shorter development cycles, enabling them to ship appliances faster than ever.

The MicroK8s ecosystem includes dozens of usefulÂ **Addons**Â - extensions that provide additional functionality and features.
***
###### K3s - Lightweight Kubernetes 

According to the docs, **K3s**Â is a Kubernetes distribution that is easy to install, lightweight, and secure. It runs on edge, homelab, IoT, CI, development, and embedded environments with a single binary or minimal container image. Also,  a certified Kubernetes distribution that simplifies and secures the installation and management of production workloads in remote or constrained environments. Learn how to download, run and updateÂ **K3s**Â with a single binary, and explore its architecture and features

![[how-it-works-k3s-revised.svg]]
> HERE, WE ARE PRIMARILY GOING TO USE **KUBEADM**. 

***
---
### Cloud - managed services 
###### EKS (Elastic Kubernetes Service) - AWS
 If you think of Kubernetes Cluster on the AWS Cloud. EKS is the solution. 
###### GKE (GCP Kubernetes Engine) - Google GCP
 If you think of Kubernetes Cluster on the Google Cloud GCP. GKE is the solution. 
###### AKS (Azure Kubernetes Service)- Microsoft Azure
 If you think of Kubernetes Cluster on Microsoft Azure. AKS is the solution. 
###### OpenShift
think of Red hat, use OpenShift. Openshift Container Engine. Especially designed for Red hat where a lot of integrations with most of the Redhat products and Services. which is called as OC - Openshift CLI Bundle. 
###### Rancher
is one of the solution that most of the companies are using if you want to manage the Authentication and Authorization on a centralized location having your Kubernetes Clusters sitting on cloud like EKS, GKE or AKS, this is the solution. Plus, if you want to setup K8s cluster on production grade level. THIS WORKS TOO! 

there are more other options to do the same. such as,
CIVO and all.

> sure that you're gonna work with all or any of these tools in near future but no matter the tools or managed services, after the kubernetes cluster gets created, THE TECH IS SAME, THE APPLICATIONS AND THE DEPLOYMENT ARE THE SAME, 
> Where you are running your Kubernetes might vary. Listed all for your convenience.
> Plus, https://www.techwithkunal.com/blog/10-years-of-kubernetes?utm_source=hashnode_blog_newsletter&utm_medium=email&utm_campaign=10-years-of-kubernetes
THESE MIGHT HELP TOO!
***
## 2. Overview on single/multi node setup using kubeadm tool

###### Setting up K8s Clusters using **Kubeadm** Tool (setting up K8s Clusters on Single or Multi-node) + using containerd and CRI tool for Container creation.

**Pre-requisite:**
1)  system - Bare-metal/ Physical Local or remote, VM and Cloud Instance/VM (here we use a remote VM - Document scripted on Oracle VirtualBox, Practiced on Windows Hyper-V)
 2) OS - ubuntu22..04 LTS (amd64/x84_64) - easy to use and OSS
 3) Spec - 2 CPU Cores for each instance
4) 2GB> RAM or more as it scale by deploying more apps
5) Disable Swap memory -  K8s !IMPORTANT recommendation - Apps gets slowed down, lacks efficiency when,
> Memory that get stored on the disc not on RAM. 
6)  Ensure network availability (no NAT, Full Ping Internet) Should be able to talk on VM to another without any issue. ! Must have unique IP and Hostname.
WELL COMPATIBLE TO RUN **KUBEADM** 

**containerd and K8s setup and all the points that has to be taken to consideration**
going to use **containerd** as container runtime interface for K8s (since K8s cannot create all the PODS, but form and orchestrate clusters) 
--  If you want to create containers as PODS on K8s -> has to pass information to **containerd** (which creates all the containers in the backend)
1) install **containerd** (or whatever as per your convenience) as CRI for Kubernetes, since K8s v1.24+
2) Install packages:
	-**kubeadm** -to setup all the K8s cluster, 
	-**kubelet** - daemon service for all the nodes, *(takes responsibility of how/ what to be created -> gets pass to **containerd**)* and 
	-**kubectl** - cli tool -> path or way to communicate/talk to K8s and share information.
3) Setup Project **Calico** as CNI - Container Network Interface for K8s Networking. (every pod will gets its own particular IP Address from this Calico(uses Overlay Network with **VXLanCrossSubnet** tag to enable communication between one container to another sitting on two different nodes))  Adapting all the same here too.
4) Disable Taints on control plane node for single node setup. (*no need if you are using multi-node setup*)

#### See all the same in practical session
*****
## 3. Why Containerd not Docker? (k8s dropped support for Docker)

WILL DISCUSS What are all the container runtime that we have?
WHAT K8s has supported?
Why **containerd** and why **Docker** got deprecated v1.24+?
>Note: K8s supports lot of container runtime interfaces. CRI. Just as listed below. 
>1) Containerd
>2) CRI-O
>3) Docker-Engine (using cri-dockerd) 
>4) Rocket Container Engine (rkt)
>5) and more
>6) also we have hypervisor container runtime interfaces as well

Here, we're primarily using **containerd**
**container runtimes**

| Runtime                              | Path to UNIX Domain socket               |
| ------------------------------------ | ---------------------------------------- |
| ==container==                        | unix://var/run/container/containerd.sock |
| crio                                 | unix://var/run/crio/crio.sock            |
| Docker Engine (using cri-containerd) | unix://var/run/cri-dockerd.sock          |

> [!NOTE] Why **containerd**?
> > **containerd** allows the kubelet to talk to the containerd with low latency and high efficiency compared to **Docker-Engine**.  *major reason why Docker-Engine got deprecated* and now the containerd is the default Container Runtime Interface for Kubernetes.
any other CRI can be used too as per ops convenience. such as we seen above. Rocket contanier runtime(rkt)
   
**HOW THIS WORKS IN UNDER THE HOOD?**
![[Pasted image 20240627145820.png]]
> **KubeletðŸ›ž** (a grpc client) < - -*CRI protocol*- - > **CRI shim** (grpc server) < - - > (*container runtime*) **cri-containerd** -- >>> containers on PODS

As given in the above diagram, (say creating containers as POD)
1) **KubeletðŸ›ž** (a part of K8s) -> this, a **daemon-service** WILL BE SEEN RUNNING ON ALL THE NODES (*both control plane or worker node*) => this will get in act as a ==**CLIENT**== and in this context, **cri-containerd**, a **==SERVER==**
>this **KubeletðŸ›ž**client having an objective is to create PODS.:
>In order to do that, **KubeletðŸ›ž** client -> has to talk to CR -> **cri-containerd** server => to create PODS.
`CLIENT HAS TO TALK TO THE SERVER`
	In-between, we got something called **cri-shim** or whatever tool. this is called as a => ***Mediator or Middleware.*** => THIS IS ULTIMATELY, THE **==CRI==** **CONTAINER RUNTIME INTERFACE**.

> [!NOTE] **CRI - CONTAINER RUNTIME INTERFACE** high level overview
> CRI, a Container Runtime Interface is a mediator between the **Kubelet**(**Client**) and the **container runtime (Server).**
 
 > This Container Runtime will receive the information the **KubeletðŸ›ž** via/with the help of **CRI** => using **==gRPC==** protocol (an RPC Framework)  => MODE OF COMMUNICATION TO PASS THE INFORMATION FROM THE **KubeletðŸ›ž** to the **Container Runtime** which is **containerd**(or) can be others.
 
 ==container-runtime takes information and create containers accordingly.== once all the containers get created and deployed.

THIS HOW IT WORKS UNDER THE HOOD.
###### How Docker works in parallel, all the working principles and the inner workflow behind the magic of CREATING CONTAINERS:
![[Pasted image 20240627173518.png]]

> [!NOTE] **DOCKER CONTAINER CREATION WORKFLOW** Magic of containerization
> **KubeletðŸ›ž**(**client**) --> passes info using [Rest API] --> **docker (container runtime)** ***CLI*** --> takes the information in and to docker(d) **docker daemon(server)** -->  talks to using [gRPC] **containerd** --> passes by [OCI bundle] to --> **runc** --> assigns [namespaces, cgroups] --> **libcontainer** --> *packager completes containerizing by isolating with namespaces and cgroups* , [runc exits] *from here* --> **container status** -- [console output] --> PODS gets CREATED --> this output get passed back to --> **KubeletðŸ›ž**

this is how it works in backend when you try to created containers using Docker in the K8s cluster. 

**SEEMS GOOD! WHY IT GOT DEPRECATED and Should we thrash Docker!? No!** explain why?
eg:
I need to create a `Dockerfile` with all the custom instructions and scripts. I need to build a docker image and store in a container registry as artifacts. CAN KUBERNETES DO THIS? **NO**!

Need Containerization tool for sure. In that, you got the popular tool Docker. With this, we can able to build images and store it in container registries. And to test your application in the local system, Kube have no use doing the same on that scale for simple use cases. 

Simply, you can build the same by writing the essential `Dockerfile`and test the apps. Obviously **Docker!** it just got deprecated the support for Kubernetes to create containers. 

HOW IT GOT DEPRECATED?  
> **docker (container runtime)** ***CLI*** --> takes the information in and to docker(d) **docker daemon(server)** -->
> THESE STEPS GOT SKIPPED!

==and rolling over that update as -> **KubeletðŸ›ž**(**client**) -> directs talks to **cri-plugin** as interface (which is) cri-containerd ->  having **containerd (the server)** --> creates CONTAINERS AS PODS.==
DIRECTLY TALK TO **containerd**. less latency.
> this why **containerd** became the default option. v1.24+

**WILL SEE ALL THESE WITH MORE DETAILS-**
###### Why kubernetes dropped support for Docker as CRI from v1.24+
we have seen these already in [section1](obsidian://open?vault=tutorialHell&file=Orchestration%2Fk8engineers.com%2FKubernetes-Deep-Dive%2FHANDWRITTEN%2Fsection1-k8sArchitecture) dropping support for Docker. A short diagram for your reference. 
![[Pasted image 20240628101805.png]]

All the architectural differences between all three of these. +Remember, 
> after all these pods gets created, the information has to sent back to **KubeletðŸ›ž**)

1) All these Docker clunks
2) **Containerd** v1.0 --> **KubeletðŸ›ž**(**client**) talks to -> **CRI interface** -> receives and then pass it to **containerd** -> and left it to create **containers**
 3) Container v2.0 -> **KubeletðŸ›ž**(**client**) --> cri became a part of containerd itself as **cri-containerd** (like a plugin) which directly creates --> **containers**.

THIS HOW THE LATENCY HAS BEEN AVOIDED AND IMPROVISED by cutting down inbetween clunks of docker!  
***

## 4. Â Kubernetes single node setup using kubeadm tool(cri-containerd)

Setting up Single-node Kube setup to create a cluster infra using Kubeadm tool:
In this demo, we use:
1) **Kubeadm** - to create cluster
2) **containerd v1.25** - Container runtime (plugin plugged in)

for system reference:
```bash
ipconfig/all
```
###### CREATE A VM
1) Any VM Engine as per your convenience. Ill be using Hyper-V. 
2) Create VM
	1) Name - vmName (kubeadm-Single-Node)
	2) ISO folder -Ubuntu 22.04 - live server jammy jelly
	3) Image
	4) Type
	5) Version

Hyper-V steps:
1) Open Hyper-V
2) New -> Create VM -> next
3) Name, Gen2, RAM (4096MB/2GB), Bridge Connection (for static IP) - > 4x Next
4) Name, Location, Storage Size (50gb) -> all default -> Next
5) OS (ISO location) 
6) Preview 
7) Finish
Right Click -> Settings
1) Security -> Disable Secure Boot
2) Apply convenient configurations
3) Apply and Ok
4) Open VM

Virtual Box Setup:
1) New
2) Name, ISO, Skip Unattended Installation
3) Spec: 4CPU, 4GB RAM
4) Storage
5) Pre-allocate : false

Ubuntu 24.0 Boot Settings:
1) Try Install
2) Language
3) Continue with or without (I'm continuing without updating)
4) Keyboard Layout (with default)
5) base OS Version (Ubuntu Server Standard)
6) Network Interface config 
`Note: We can see an IP allocated by default without config. This happened because of bridge Network where it automatically allocates an IP cause of DHCP given by router. Which isn't static or permanent. 
**We'd like to have a static IP to get the image be persistent.**
> Will configure the same.

2) (Tab) to eth0 => Edit IPv4 -> (say that it is Automatic(DHCP)(ip gets allocated just like your mobile gets connected to the wifi giving an IP) **These IPs comes from router**)
	`Need a fixed IP address, have to change automatic to static to assign a fixed IP to the system, in this case, VM`

3)  Enter -> Manual
` Open cmd -> ipconfig -> Ethernet adapter - Bridged, Have this aside for this reference)`

```shell
   Connection-specific DNS Suffix  . :
   Link-local IPv6 Address . . . . . : fe80::612f:24d5:4f01:4e62%17
   IPv4 Address. . . . . . . . . . . : 192.168.0.158
   Subnet Mask . . . . . . . . . . . : 255.255.255.0
   Default Gateway . . . . . . . . . : 192.168.0.1
```

4)  Edit eth0 IPv4 Configuration
	IPv4 Method: Manual
**-> Subnet is nothing but base IP Address. In cmd prompt, shows the IPv4 address - 192.168.0.0/24 based on the subnet mask which is 255.255.255.0. Based on the subnet mask, its /24.**  
If any 
	Subnet: 192.168.0.0 (subnet rule IP/Mask)
**-> Address is what Static IP that you'd wanted to assign to this persistent VM. (in this case, in my network, i've assigned what hasn't been used before)**
	Address: 192.168.0.==143==
**-> Gateway from base machine will not be much of a problem.**
	Gateway: 192.168.0.1
**-> here, you can use google, Cloudflare's, cisco name servers or the ISP Provided name servers.** 
	Name Servers: 8.8.8.8,8.8.4.4 //Google's
	Search Domains: -blank-

5) Now, Network Config Interface:
	eth0 - 
	static: 792.168.0.222/24

6) Configure Proxy, if required
	Proxy Address: 
7) Mirror Default
8) Storage Config (default) - OS to take care of it. 
	Option1
PREVIEW -> Done + Continue!

Profile Setup:
1) Name
2) Server (computerName)
3) username
4) password
5) confirm

Ubuntu Pro: Apply if required 
install Essentials - such as openSSH and more.
Done!

Checks:
1) Login
2) Ping
3) Check network
4) IP and more
5) Connect remote - ssh username@IP and password, yes and password  
6) check ip -a
all done.
> -- So far we have performed setting the VM for Kubernetes. Now will set-up kubernetes single node setup. --

prerequisite for kubernetes kubeadm single node setup,
COMPLETED SINGLE NODE KUBEADM VM SETUP WITH SSH COMPATIBILITY.
Now in this particular machine, will see how to 
#### ==Setup Single Node having One Control plane + worker node:==
Steps and Procedures to follow the same - 
USING KUBEADM TOOL TO SETUP KUBERNETES CLUSTER +  Containerd as Container runtime interface in the backend -> For kubernetes to launch the application.

**Prepared a document which** will have all the manifest and refer the same.
Reference: https://github.com/devopsyuva/kubernetes_latest_manifest
-> if docker, 2) k8s-setup-kubeadm-containerd.md
-> if containerd, 3) k8s-setup-kubeadm-containerd.md
All the documentation for the same have delivered here.

**Using K8s Setup using kubeadm *containerd*** [Doc](obsidian://open?vault=tutorialHell&file=Orchestration%2Fk8engineers.com%2Fkubernetes_latest_manifest%2FKubernetes%2F01-kubernetes-architecture-Installation%2F01-k8s-components)
How and why we're using containerd:
![[Pasted image 20240711151147.png]]
1) all the system specs 
> This documents includes both ==**Single and Two node setup.**==

**PREREQUISITES TO CONFIGURE CONTAINERD:**
1) sudo -i and apt updates
2)  verify if reboot required checking by -> `ls -l /vat/run/reboot-required/` 
-> if it doesn't exist, no reboot is required
4) Follow:
-> BEFORE INSTALLING KUBEADM + CONTAINERD -> We have to ==install some packages== + ==enable and modify some modules== + ==network configurations==:
enabling  
1) overlay
2) br_netfilter -> Bridge network Filter
with the cmd 
`modprobe overlay` and `modeprobe br_netfilter` 
> Problem with executing these commands standalone is that these command has to be executed after every reboot. Instead, will make all of those permanent. for that, save it in a file as script named `containerd.conf`. ,**MODULES ARE ENABLED SUCCESSFULLY.**

After that, we should to do some systemctl parameters. Will execute the third para comment to do the save it a file named `kubernetes-cri.conf`.  

After applying these, you need to enable these changes, for that can't reboot the system instead, `systemctl --system` 
```shell
modprobe overlay
modprobe br_netfilter
```
```sh
cat > /etc/modules-load.d/containerd.conf <<EOF
overlay
br_netfilter
EOF  
```
Setup required sysctl params, these persist across reboots.
```sh
cat > /etc/sysctl.d/99-kubernetes-cri.conf <<EOF
net.bridge.bridge-nf-call-iptables  = 1
net.ipv4.ip_forward                 = 1
net.bridge.bridge-nf-call-ip6tables = 1
EOF
```
```sh
sysctl --system
```
> to check and enable all of these,
>`sysctl --system`

**NOW ALL THE MINIMAL REQUIREMENTS ARE DONE! NEXT, WILL CONTINUE WITH THE REST INSTALLING OTHER COMPONENTS**

## `containerd` INSTALLATION
Will follow all the steps to install containerd package on all nodes. REFER THE SAME OR REFER OFFICIAL DOCKER DOCS. -> since we are installing containerd not docker packages.
- Set up the repository
- Install packages to allow apt to use a repository over HTTPS
```Shell
sudo apt-get install ca-certificates curl gnupg lsb-releases
```
 
or else, refer: 
[Reference:](https://github.com/rithishsamm/myTutorialHell/blob/main/Orchestration/k8engineers.com/kubernetes_latest_manifest/Kubernetes/01-kubernetes-architecture-Installation/03-k8s-setup-kubeadm-containerd.md#reference)
- [Containerd](https://docs.docker.com/engine/install/ubuntu/)
- [kubeadm install](https://kubernetes.io/docs/setup/production-environment/tools/kubeadm/install-kubeadm/#installing-kubeadm-kubelet-and-kubectl)
++
- [crictl](https://github.com/kubernetes-sigs/cri-tools/blob/master/docs/crictl.md)
- [containerd](https://kubernetes.io/blog/2018/05/24/kubernetes-containerd-integration-goes-ga/)
- [Docker and Containerd](https://kubernetes.io/docs/tasks/administer-cluster/migrating-from-dockershim/check-if-dockershim-deprecation-affects-you/)
or search [**==Docker install ubuntu==**](https://docs.docker.com/engine/install/ubuntu/). to check accessing ubuntu package index pages:
since we're using docker official docs. WE GOTTA IMPORT THE `GPG key` and store it in the local system. first to create a `dir` and store the same.
1) and 2) Set up repo + Add Docker's official GPG key:
```Shell
sudo mkdir -p /etc/apt/keyrings
sudo install -m 0755 -d /etc/apt/keyrings
sudo curl -fsSL https://download.docker.com/linux/ubuntu/gpg -o /etc/apt/keyrings/docker.asc
sudo chmod a+r /etc/apt/keyrings/docker.asc
```
basic packages to proceed further.  
AND, 
3) after that, need to create a repo.
```bash
# Add the repository to Apt sources:
echo \
  "deb [arch=$(dpkg --print-architecture) signed-by=/etc/apt/keyrings/docker.asc] https://download.docker.com/linux/ubuntu \
  $(. /etc/os-release && echo "$VERSION_CODENAME") stable" | \
  sudo tee /etc/apt/sources.list.d/docker.list > /dev/null
sudo apt-get update
```
whenever you create a new repo especially in the host ubuntu machine, run `apt update`. eg: 
```
ls -l /etc/apt/
```
run `apt update
> NOW, The `apt` can fetch the docker as well. 

After then, will install all the essential packages with the `containerd`
here, in order to install: 
	in docker docs, we have all the commands which we can install relevant run **Docker**,  but we need only `containerd` , to filter and install that, skip all except `containerd.io`:
```
sudo apt update && sudo apt upgrade && install -y containerd.io 
```

  Next,
  We are all done with the installation. NOW WE HAVE TO CONFIGURE THE `containerd`. which means, we need to save the configuration in a file as same as we did before - with minimum required parameters in the configuration. Create the same by,  Configure containerd,
  1) mkdir - Create directory
  2) load config - execute the given command below for saving the output in this file.
  3) changing a Cgroup from **false** to **true**. Can be performed both manually or by via the script.
```bash
mkdir -p /etc/containerd

containerd config default > /etc/containerd/config.toml

sed -i 's/SystemdCgroup = false/SystemdCgroup = true/g' /etc/containerd/config.toml
```
and Restart and check status containerd
```
systemctl restart containerd
systemctl status containerd
```
CONFIGURATIONS IS SUCCESSFULLY COMPLETED AND ITS UP AND RUNNING. 
Other steps to follow:
-- Since we're doing all container things with containerd but not docker, We need something to make communication and interact with containerd. (if docker, we could've executed it's client but not doing that here)

To interact with containerd, `crictl` is a solution where we can perform all the container creation, troubleshooting and the CRI stuff. -> It is available by 
Back to rest and continue following all the step installing crictl:
## `crictl` INSTALLATION
**INORDER TO INTERACT WITH THE `CONTAINERD` TO PERFORM ALL THE ACTIONS LIKE TROUBLESHOOTING, CONTAINER CREATIONS AND ALL THE CRI STUFF, SINCE WEREN'T USING DOCKER HERE TO USE ITS CLI COMMANDS. WE SHOULD HAVE `crictl` tool.** which is all available by default.
*To execute crictl CLI commands, ensure we create a configuration file as mentioned below*. 
Here,  
1) Creating config file for `crictl.yaml` 
2) Means that we're telling `crictl`, to follow this yaml config file to communicate with `containerd`
3) Create and save the data by running the command.
```SHELL
cat > /etc/crictl.yaml <<EOF
runtime-endpoint: unix:///run/containerd/containerd.sock
image-endpoint: unix:///run/containerd/containerd.sock
timeout: 2
EOF
```
now these `crictl` commands will communicate with the `containerd` to fetch kube information.
## `kubadm` `kubernetes`  INSTALLATION
Follow the same documentation or follow official docs. Following the official docs. to follow, search [==**kubeadm install**==](https://kubernetes.io/docs/setup/production-environment/tools/kubeadm/install-kubeadm/), scroll till - [Debian-based distributions](https://kubernetes.io/docs/setup/production-environment/tools/kubeadm/install-kubeadm/#k8s-install-0) section. Now, we have to install all the packages named,
- Kubeadm
- Kubectl
- Kubelet
1) before all of that, will do setup all the prerequisite to setup all of these. To do that,
do run, # apt-transport-https may be a dummy package; if so, you can skip that package. we're installing it.
```shell
sudo apt-get update
sudo apt-get install -y apt-transport-https ca-certificates curl gpg
```
2) Download `gpg keys` for the same:
```shell
curl -fsSL https://pkgs.k8s.io/core:/stable:/v1.30/deb/Release.key | sudo gpg --dearmor -o /etc/apt/keyrings/kubernetes-apt-keyring.gpg
```
3) Add K8s `apt` repo:
```shell
echo 'deb [signed-by=/etc/apt/keyrings/kubernetes-apt-keyring.gpg] https://pkgs.k8s.io/core:/stable:/v1.30/deb/ /' | sudo tee /etc/apt/sources.list.d/kubernetes.list
```

4) Update theÂ `apt`Â package index, install kubelet, kubeadm and kubectl
> to check available versions of the same, check it by 
> `apt-cache policy kubelet` it'll install the latest if no version specified while installation. **all three of these packages shares and maintains the same version**. if specific version needed, you need to do an `=versionNO`
> ++
> 	WHY HOLD PACKAGES? - **FOR NO AUTO-UPGRADE**. If not, it will do an auto-update  where it is prone to mess the cluster.
```shell
sudo apt-get update
sudo apt-get install -y kubelet kubeadm kubectl
sudo apt-mark hold kubelet kubeadm kubectl
```
5) After installation, Enable the kubelet service before running kubeadm:
```shell
sudo systemctl enable --now kubelet
```
 > Now, WILL CREATE ALL THE KUBERNETES CLUSTERS BY **BOOTSTRAPPING THE SINGLE  NODE both Control Plane and Worker Node.** 

To do that,
SYNTAX:
> kubeadm init 
> - `[mapping IPAddr to apiserver]`do not use localhost or 127.0.0.1IP
> - `[referring config file for cri ]`
> - `[declaring destination network cidr block to deploy pods at the endpoint]`which is the server'S itself. POD NETWORK CIDR.
Every POD inside the Kubernetes Cluster will get an IPAddr from this series - 10.244.0.0/24. This is a class A Network. Execute the same and modify the parameters.

```sh
kubeadm init --apiserver-advertise-address=IPADDR --cri-socket=/run/containerd/containerd.sock --pod-network-cidr=10.244.0.0/16
```

```sh
kubeadm init --apiserver-advertise-address=192.168.0.143 --cri-socket=/run/containerd/containerd.sock --pod-network-cidr=10.244.0.0/16
```

1) Runs all the pre-flight checks => THIS PROCESS IS CALLED AS **BOOTSTRAPPING** the node.
Troubleshoot all of them :(, my encounters are,
-> Swap enabled, to turn off -> 
```sh
free -h
swapoff -a
```
but, it only disables temporarily it. for a permanent turnoff, do comment out the swap section. i gave this #'ed for your reference.
```sh
free -h
nano /etc/fstab 
#/swap.img none swap sw 0 0
```
comment out the hashed one.

**Re-run kubeadm. If any errors still persists. Troubleshoot it.
Since i cancelled the process in-between which already bootstrapped half way and not re-running next other steps-> Steps that i followed is,**
```
kubeadm reset 
iptables
ipvsadm --clear
#if not, sudo apt install ipvsadm
done!
```
or If you know what you are doing, you can `ignore-preflight-checks= whatever to skip or all`**
```sh
kubeadm init --apiserver-advertise-address=192.168.0.143 --cri-socket=/run/containerd/containerd.sock --pod-network-cidr=10.244.0.0/16 --ignore-preflight-checks=all
```
voila! Successfully initialized control-plane. must give the output of,
> [!NOTE] output
> ```
> sudo kubeadm init --apiserver-advertise-address=192.168.0.222 --cri-socket=/run/containerd/containerd.sock --pod-network-cidr=192.168.0.0/24 --ignore-preflight-errors=all
W0712 10:21:46.616343   14940 initconfiguration.go:125] Usage of CRI endpoints without URL scheme is deprecated and can cause kubelet errors in the future. Automatically prepending scheme "unix" to the "criSocket" with value "/run/containerd/containerd.sock". Please update your configuration!
[init] Using Kubernetes version: v1.30.2
[preflight] Running pre-flight checks
[preflight] Pulling images required for setting up a Kubernetes cluster
[preflight] This might take a minute or two, depending on the speed of your internet connection
[preflight] You can also perform this action in beforehand using 'kubeadm config images pull'
W0712 10:21:47.877835   14940 checks.go:844] detected that the sandbox image "registry.k8s.io/pause:3.8" of the container runtime is inconsistent with that used by kubeadm.It is recommended to use "registry.k8s.io/pause:3.9" as the CRI sandbox image.
[certs] Using certificateDir folder "/etc/kubernetes/pki"
[certs] Generating "ca" certificate and key
[certs] Generating "apiserver" certificate and key
[certs] apiserver serving cert is signed for DNS names [kubeadm1n kubernetes kubernetes.default kubernetes.default.svc kubernetes.default.svc.cluster.local] and IPs [10.96.0.1 192.168.0.222]
[certs] Generating "apiserver-kubelet-client" certificate and key
[certs] Generating "front-proxy-ca" certificate and key
[certs] Generating "front-proxy-client" certificate and key
[certs] Generating "etcd/ca" certificate and key
[certs] Generating "etcd/server" certificate and key
[certs] etcd/server serving cert is signed for DNS names [kubeadm1n localhost] and IPs [192.168.0.222 127.0.0.1 ::1]
[certs] Generating "etcd/peer" certificate and key
[certs] etcd/peer serving cert is signed for DNS names [kubeadm1n localhost] and IPs [192.168.0.222 127.0.0.1 ::1]
[certs] Generating "etcd/healthcheck-client" certificate and key
[certs] Generating "apiserver-etcd-client" certificate and key
[certs] Generating "sa" key and public key
[kubeconfig] Using kubeconfig folder "/etc/kubernetes"
[kubeconfig] Writing "admin.conf" kubeconfig file
[kubeconfig] Writing "super-admin.conf" kubeconfig file
[kubeconfig] Writing "kubelet.conf" kubeconfig file
[kubeconfig] Writing "controller-manager.conf" kubeconfig file
[kubeconfig] Writing "scheduler.conf" kubeconfig file
[etcd] Creating static Pod manifest for local etcd in "/etc/kubernetes/manifests"
[control-plane] Using manifest folder "/etc/kubernetes/manifests"
[control-plane] Creating static Pod manifest for "kube-apiserver"
[control-plane] Creating static Pod manifest for "kube-controller-manager"
[control-plane] Creating static Pod manifest for "kube-scheduler"
[kubelet-start] Writing kubelet environment file with flags to file "/var/lib/kubelet/kubeadm-flags.env"
[kubelet-start] Writing kubelet configuration to file "/var/lib/kubelet/config.yaml"
[kubelet-start] Starting the kubelet
[wait-control-plane] Waiting for the kubelet to boot up the control plane as static Pods from directory "/etc/kubernetes/manifests"
[kubelet-check] Waiting for a healthy kubelet. This can take up to 4m0s
[kubelet-check] The kubelet is healthy after 503.07131ms
[api-check] Waiting for a healthy API server. This can take up to 4m0s
[api-check] The API server is healthy after 9.504487982s
[upload-config] Storing the configuration used in ConfigMap "kubeadm-config" in the "kube-system" Namespace
[kubelet] Creating a ConfigMap "kubelet-config" in namespace kube-system with the configuration for the kubelets in the cluster
[upload-certs] Skipping phase. Please see --upload-certs
[mark-control-plane] Marking the node kubeadm1n as control-plane by adding the labels: [node-role.kubernetes.io/control-plane node.kubernetes.io/exclude-from-external-load-balancers]
[mark-control-plane] Marking the node kubeadm1n as control-plane by adding the taints [node-role.kubernetes.io/control-plane:NoSchedule]
[bootstrap-token] Using token: bqflzv.nbb753ji2xawnm6q
[bootstrap-token] Configuring bootstrap tokens, cluster-info ConfigMap, RBAC Roles
[bootstrap-token] Configured RBAC rules to allow Node Bootstrap tokens to get nodes
[bootstrap-token] Configured RBAC rules to allow Node Bootstrap tokens to post CSRs in order for nodes to get long term certificate credentials
[bootstrap-token] Configured RBAC rules to allow the csrapprover controller automatically approve CSRs from a Node Bootstrap Token
[bootstrap-token] Configured RBAC rules to allow certificate rotation for all node client certificates in the cluster
[bootstrap-token] Creating the "cluster-info" ConfigMap in the "kube-public" namespace
[kubelet-finalize] Updating "/etc/kubernetes/kubelet.conf" to point to a rotatable kubelet client certificate and key
[addons] Applied essential addon: CoreDNS
[addons] Applied essential addon: kube-proxy
HERE in bootstrapping these node means **BOOOTSRAPPING THE CONTROL PLANE NODE**.
>IN THE CLUSTER, makes it up one node first and joins with the rest.
all the 
1) The command gets initiated
2) Images been pulled
3) Files and configurations got created
4) Setups and addons has been applied
 YET TO COMPLETE CREATING THE KUBERNETES CLUSTER SETUP. THIS IS THE OUTPUT THAT WE SUPPOSED TO END UP WITH which is: Your Kubernetes control-plane has initialized successfully!
```sh
#Your Kubernetes control-plane has initialized successfully!
#To start using your cluster, you need to run the following as a regular user:

  mkdir -p $HOME/.kube
  sudo cp -i /etc/kubernetes/admin.conf $HOME/.kube/config
  sudo chown $(id -u):$(id -g) $HOME/.kube/config

#Alternatively, if you are the root user, you can run:
   export KUBECONFIG=/etc/kubernetes/admin.conf

#You should now deploy a pod network to the cluster.Run 
kubectl apply -f [podnetwork].yaml #with one of the options listed at:
  https://kubernetes.io/docs/concepts/cluster-administration/addons/

#Then you can join any number of worker nodes by running the following on each as root:

kubeadm join 192.168.0.143:6443 --token bqflzv.nbb753ji2xawnm6q \ --discovery-token-ca-cert-hash sha256:7b897f575d86f96ac4e3571bd19  0407253184e717bdeaf80b34e6f95c794f96b
```

>You can perform the following instructions, if
  -> **Single node setup**
```sh
###To start using your cluster, you need to run the following as a regular user:
  mkdir -p $HOME/.kube
  sudo cp -i /etc/kubernetes/admin.conf $HOME/.kube/config
  sudo chown $(id -u):$(id -g) $HOME/.kube/config

#Alternatively, if you are the root user, you can run:
#skip this:   export KUBECONFIG=/etc/kubernetes/admin.conf

#You should now deploy a pod network to the cluster. For that you need to install a CNI first. We'll be using calico. After that, run 
kubectl apply -f [podnetwork].yaml #with one of the options listed at:
https://kubernetes.io/docs/concepts/cluster-administration/addons/
```
> ->  If it is a **multi-node setup**, rest of it should be completed, to join whatever Compute plane worker node to the control plane node. Have to execute the command till that. which is simply, joining the control plane with `worker nodes`
> 
> you continue with the rest of it by,
```sh
#Then you can join any number of worker nodes by running the following on each as root:
kubeadm join 192.168.0.143:6443 --token bqflzv.nbb753ji2xawnm6q \ --discovery-token-ca-cert-hash sha256:7b897f575d86f96ac4e3571bd190407253184e717bdeaf80b34e6f95c794f96b
```
and the rest of these should be followed in order to start using the cluster. till at least `kubectl apply` if it is a single node setup.
>**ignoring the last since we're setting up a single node cluster setup.**
**BOOTSTRAPPING DONE SUCCESSFULLY!**
#### To start using your cluster, you need to run the following as a regular user:
```sh
mkdir -p $HOME/.kube
sudo cp -i /etc/kubernetes/admin.conf $HOME/.kube/config
sudo chown $(id -u):$(id -g) $HOME/.kube/config
```
here, 
1) creating `.kube` directory 
> `mkdir -p $HOME/.kube`
2) under that `.kube`, gotta store a **config** file. Why? If someone wants to communicate with the kubernetes cluster, must have a **kube-config** file. Without it, can't able to communicate with the Kubernetes Cluster regardless of platforms both on-prem or cloud. 
By this time, the kubeconfig presented in default and have to move it to home directory.  the file entries will be saved under the configured file location.
> `sudo cp -i /etc/kubernetes/admin.conf $HOME/.kube/config`
3) and, change permissions to the `config` files
> `sudo chown $(id -u):$(id -g) $HOME/.kube/config`

After that, can skip `export KUBECONFIG=/etc/kubernetes/admin.conf` since we already ran our thing before in prior.

NOW, WILL INSTALL AND ENABLE ADDONS WHICH IS VITAL RELATED TO ORCHESTRATING THE CLUSTERS: -> **A CNI - Container Network Interface**. -> We are using `Calicio` - network for kubernetes CNI for our use case. Alternatively, there are cilium Istio and more does exists.

**BEFORE RUNNING THIS,** should pull - calico network for kubernetes CNI. Then will run that `kubectl apply -f podnetwork.yaml https://kubernetes.io/docs/concepts/cluster-administration/addons/`
with the CNI + Addons to complete the same.

##### INSTALLING CALICO NETWORK IN OUR KUBERNETES CLUSTER:
SEARCH **Calico Kubernetes**. -> Official Docs -> Install Calico -> Kubernetes -> QuickStart -> [QuickStart for Calico on Kubernetes](https://docs.tigera.io/calico/latest/getting-started/kubernetes/quickstart#big-picture)
Following the docs, Installing Calico CNI
Install Calico[â€‹](https://docs.tigera.io/calico/latest/getting-started/kubernetes/quickstart#install-calico "Direct link to Install Calico")
1) Install the Tigera Calico operator and custom resource definitions.
```sh
kubectl create -f https://raw.githubusercontent.com/projectcalico/calico/v3.28.0/manifests/tigera-operator.yaml
```
make you don't expose ports on the system for prior services.
kubectl **Output:**
```sh
kubectl create -f https://raw.githubusercontent.com/projectcalico/calico/v3.28.0/manifests/tigera-operator.yaml
namespace/tigera-operator created
customresourcedefinition.apiextensions.k8s.io/bgpconfigurations.crd.projectcalico.org created
customresourcedefinition.apiextensions.k8s.io/bgpfilters.crd.projectcalico.org created
customresourcedefinition.apiextensions.k8s.io/bgppeers.crd.projectcalico.org created
customresourcedefinition.apiextensions.k8s.io/blockaffinities.crd.projectcalico.org created
customresourcedefinition.apiextensions.k8s.io/caliconodestatuses.crd.projectcalico.org created
customresourcedefinition.apiextensions.k8s.io/clusterinformations.crd.projectcalico.org created
customresourcedefinition.apiextensions.k8s.io/felixconfigurations.crd.projectcalico.org created
customresourcedefinition.apiextensions.k8s.io/globalnetworkpolicies.crd.projectcalico.org created
customresourcedefinition.apiextensions.k8s.io/globalnetworksets.crd.projectcalico.org created
customresourcedefinition.apiextensions.k8s.io/hostendpoints.crd.projectcalico.org created
customresourcedefinition.apiextensions.k8s.io/ipamblocks.crd.projectcalico.org created
customresourcedefinition.apiextensions.k8s.io/ipamconfigs.crd.projectcalico.org created
customresourcedefinition.apiextensions.k8s.io/ipamhandles.crd.projectcalico.org created
customresourcedefinition.apiextensions.k8s.io/ippools.crd.projectcalico.org created
customresourcedefinition.apiextensions.k8s.io/ipreservations.crd.projectcalico.org created
customresourcedefinition.apiextensions.k8s.io/kubecontrollersconfigurations.crd.projectcalico.org created
customresourcedefinition.apiextensions.k8s.io/networkpolicies.crd.projectcalico.org created
customresourcedefinition.apiextensions.k8s.io/networksets.crd.projectcalico.org created
customresourcedefinition.apiextensions.k8s.io/apiservers.operator.tigera.io created
customresourcedefinition.apiextensions.k8s.io/imagesets.operator.tigera.io created
customresourcedefinition.apiextensions.k8s.io/installations.operator.tigera.io created
customresourcedefinition.apiextensions.k8s.io/tigerastatuses.operator.tigera.io created
serviceaccount/tigera-operator created
clusterrole.rbac.authorization.k8s.io/tigera-operator created
clusterrolebinding.rbac.authorization.k8s.io/tigera-operator created
deployment.apps/tigera-operator created
```

Done! Secondly, this. 
>Note: but I don't want to run the same directly. Because, We made few modifications in the pod CIDR = 192.168.0.0`/16` but we change it to `/24`  . So, Cant use this since we're using different CIDR. To apply the relevant config for the same.
2) Install Calico by creating the necessary custom resource. For more information on configuration options available in this manifest, seeÂ [the installation reference](https://docs.tigera.io/calico/latest/reference/installation/api).
-- Get binaries using `wget` command. 
```sh
wget https://raw.githubusercontent.com/projectcalico/calico/v3.28.0/manifests/custom-resources.yaml 
```

-- edit `custom-resource.yaml` using `vi` or `nano`
```sh
nano custom-resource.yaml
```
and modify the CIDR by finding it in the spec section, CIDR.
	cidr:  //in docs,  `10.244.0.0/16` 
since, we've ran `kubectl init` with all the ip, container config and CIDR

Run, To apply the settings:
```kubectl
kubectl apply -f custom-resources.yaml 
```
>**Output: Tigera.io's Operator of Installation and apiserver/default got created.**

To check, Run this by watching the running nodes and pods:
```kubectl
kubectl get no #for node
kubectl get po -A #or --all-namespaces for pods
```
ALL OF THESE PODS HAS TO BE UP AND RUNNING. Few might've initiated, others might've in pending. Wait till completion. Only after completion, you'll see the node's status is ready.. ALL GOOD!

For more info about the node, run 
```kubectl
kubectl get no -o wide
```
telling the role which is obviously `control plane`, time, version, IP for the control-plane, OS, Kernel and Container runtime.

### *==**KUBERENETES ON A SINGLE NODE SETUP HAS BEEN COMPLETED SUCCESSFULLY!!!**==*
==**USING KUBEADM WITH CONTAINERD AS A CONTAINER RUNTIME AND CALICO AS CNI HAS BEEN COMPLETED SUCCESSFULLY!!!***==

**NOW, We can deploy our applications**. If you want to deploy and launch applications on these nodes, first we have to test it. Will test it by launching a pod running our applications on it.

No experimentation. such testing a real world application on a POD. 
To do that, follow
```kubectl
kubectl run test-pod --image=nginx -t -i 
```
or whatever image as per your convenience. if you run this, throws
> error: timed out waiting for the condition
```kubectl
kubectl get po
kubectl get po -o wide
```
shows: **test-pod** status is **Pending** and nothing in all ways.
**WHY?, Why waiting for a condition? How to validate to resolve the same** 
LETS TROUBLESHOOT.
```kubectl
kubeclt get pod #will be in pending status
kubectl describe po test-pod #pod-name
```
Shows the description,
in the events section, if notification shows a 
>Type: Warning, Reason: Failed Scheduling, ,Message: untolerated taint. 

The Reason behind that is **Taints**. 
>We have only one node which is control plane node. So, the control plane by default will not allow me to run the pod/application.  Our objective is to practice kubernetes deploying applications on the single node.

Here, the single node is a control plane node. Now, we need to add few metrics to make compute plane work too. So we need to make the control plane node work as compute plane in order to launch pods on the same. 

In order to perform such actions.    We need to remove taints. To remove taints, remove taints. By doing,
```kubectl
kubectl get no -o wide #kubeadm1n in my case, to get the nodename to edit
```
```kubectl
kubectl edit no kubeadm1n #nodename
```
scroll down and go to a block named **TAINTS** in spec section. 
It will have a parameter called `effect: NoSchedule` This particular metric is the reason that we couldn't able to launch the pods. Means, for trying to deploy an application it is not possible. So
>Remove the **TAINTS** block. 

> [!What is TAINTS]
> Taints and tolerations work together to ensure that pods are not scheduled onto inappropriate nodes. One or more taints are applied to a node; this marks that the node should not accept any pods that do not tolerate the taints.
> 
>  Since our Single node is a control plane node, which also have compute plane components. Now we want our control plane node to act as a compute plane node in-order run our pods in ease without any taints and tolerations. For that, we simply have to disable it.

NOW, re-check. 
```kubectl
kubectl get po -o wide
```
Will be resulting in `ContainerCreating` or `Running`. How come now? Because, we have removed the taints to launch the application.

If not, you can't be able to practice doing both compute and control on a same one single node. Since it `kubectl` is tainted.

##### Conclusion
The reason behind this is that, By default, you can't launch your application, container or an instance in a control plane node, because it is a control plane node.  How to know what node consist what containers. Here, we made it by
```kubectl
kubectl get no #check the role of it, finding the node
kubectl edit no nodename #node-name + edited the TAINT
kubectl get po -o wide #made it work by removing the taints condition parameter 
```
no, we can able to launch the applications. to get the app status, copy the IP by getting info of that pod, 
```bash
curl 10.244.101.71
```

shows, welcome to nginx homepage output:
```html
<!DOCTYPE html>
<html>
<head>
<title>Welcome to nginx!</title>
<style>
html { color-scheme: light dark; }
body { width: 35em; margin: 0 auto;
font-family: Tahoma, Verdana, Arial, sans-serif; }
</style>
</head>
<body>
<h1>Welcome to nginx!</h1>
<p>If you see this page, the nginx web server is successfully installed and
working. Further configuration is required.</p>

<p>For online documentation and support please refer to
<a href="http://nginx.org/">nginx.org</a>.<br/>
Commercial support is available at
<a href="http://nginx.com/">nginx.com</a>.</p>

<p><em>Thank you for using nginx.</em></p>
</body>
</html>
```

Based on this, i can confirm that i can able to launch my application on the same SETTING UP AND CONFIGURING SINGLE NODE KUBERNETES CLUSTER SETUP WITH KUBEADM WITH CONTAINERD AND CALICO.

#### CONNECTING BACK TO A K8s Cluster after restart.
No need of anything. Just run it all right away after boot. Make sure you done all the config correct as per instructed.

--- FIN ---
*****
## Kubernetes multi-node setup using kubeadm tool(CRI:Â containerd)
Setting up multi-node Kube setup to create a cluster infra using Kubeadm tool:
MEANS, **TWO NODE**! = ONE CONTROL PLANE NODE + ONE WORKER NODE! ðŸ¤¯
Previously we ran both control and compute plane on one single node itself combined.

**Now we're covering multi-node setup. Same kubeadm tool, same containerd as container runtime interface, and same Calico as CNI.**

Will look into specs and configurations. 

##### **==EVERYTHING AS SAME CONFIG AS BEFORE. Ditto Copy -> Control Plane Node + Clone the same and tone down 2GB and 2vCPU for worker node==**

Nothing much. Just simply clone. or Start over the same drill. No problem if you start over. If it is a clone, you need to perform some extra mods. such as,

###### Configuring Control Plane and worker node:
**Control Plane**:
1) login as root
2) `ip a` to check the IP. must be same. 
3) Configure and modify the static ip address.\
4) EVERYTHING ALL AS SAME AS WE DID BELOW TO THE PREVIOUS MACHINE
**Worker Node:**
1) Rename
2) Keep NAT MAC, next
3) Full Clone, Finish
Further mods after clone: such as changing hostname, IP and more.
1) su root
2) changing hostname
```sh
hostnamectl set-hostname kubadm2nctrlplane
```
3) changing ip address, this is an automatic DHCP IP. I need a **static** one. To do that,
```sh
ip a
nano /etc/netplan/00-installer-config.yaml
```
```yaml
# This is the network config written by 'subiquity'
network:
  ethernets:
    enp0s3:
      dhcp4: false
      addresses:
      - 192.168.0.153/24
      nameservers:
        addresses:
        - 8.8.8.8
        - 8.8.4.4
        search: []
      routes:
      - to: default
        via: 192.168.0.1
  version: 2
```
4) Save changes. and Apply,
```sh
netplan apply
```
6) and Reboot 
7) `ip a`  recheck and ensure the ip got changed
8) ensure swap memory got disabled
```sh
swapoff -a
nano /etc/fstab
#comment out the swp and the whole header
```
9) All done, reboot
```sh
init 6
```
for `6. Automount units provide automount capabilities, for on-demand mounting of file systems as well as parallelized boot-up. See systemd.automount(5).`

All good! -> Preflight checks before running kube: 
- control plane config
- worker node config
- swap off
- hostname change
- setup config
- done!!
##### RECALLING THE PREVIOUS SETUP TO APPLY THE SAME:
Referring to the [](obsidian://open?vault=tutorialHell&file=Orchestration%2Fk8engineers.com%2Fkubernetes_latest_manifest%2FKubernetes%2F01-kubernetes-architecture-Installation%2F03-k8s-setup-kubeadm-containerd) Document.
Here, the setup says that
- Setup: ` Single/Two nodes: VirtualBox was used to launch both nodes(Master Hostname: controlplane and Worker Hostname: computeplaneone)`
	- Network: `Bridge` for Oracle Virtual Box, since the IP can assigned from the router itself.
APPLYING THE SAME!
###### Lets Configure: EVERYSAME THING AS WE DID THE SAME PREVIOUSLY FOR THE SINGLE NODE SETUP.
recalling the kubernetes manifests file setup as we spoke for setting the the single node on the same. [](obsidian://open?vault=tutorialHell&file=Orchestration%2Fk8engineers.com%2Fkubernetes_latest_manifest%2FKubernetes%2F01-kubernetes-architecture-Installation%2F03-k8s-setup-kubeadm-containerd)
1) Make both the VM readily available
2) Open Mobaxterm
3) Open and SSH into both of the VMs as separate sessions
4) Split Screen and separate both of the nodes
5) Select Multi-exec + make use of multi-paste option for copy-paste
>==**CHEF'S KISS SETUP ACCESSING BOTH THE M ACHINES AT THE TIME SAVING HOURS  CONFIGURING AND SETTING THINGS UP IN EASE!==**

####### **PREREQUISITES TO CONFIGURE CONTAINERD [](obsidian://open?vault=tutorialHell&file=Orchestration%2Fk8engineers.com%2FKubernetes-Deep-Dive%2FHANDWRITTEN%2Fsection2-k8sClusterSetup######**PREREQUISITES TO CONFIGURE CONTAINERD:**):** + everything that we did for the same in single node setup. 
Will see further of the common steps that we have for both the nodes. al are common until we execute `kubeadm init`

apply:
1) setup config for `containerd`
```sh
cat > /etc/modules-load.d/containerd.conf <<EOF
overlay
br_netfilter
EOF
```
```sh
cat > /etc/sysctl.d/99-kubernetes-cri.conf <<EOF
net.bridge.bridge-nf-call-iptables  = 1
net.ipv4.ip_forward                 = 1
net.bridge.bridge-nf-call-ip6tables = 1
EOF
```
```sh
sysctl --system
```

2) installing essentials, `containerd` + adding gpg keys and repo to apt resources [**docker official docs - search: docker engine install**]
```sh
sudo apt update
sudo apt install -y ca-certificates curl gnupg lsb-release
```
 ```bash
sudo install -m 0755 -d /etc/apt/keyrings
sudo curl -fsSL https://download.docker.com/linux/ubuntu/gpg -o /etc/apt/keyrings/docker.asc
sudo chmod a+r /etc/apt/keyrings/docker.asc
```
```bash
echo \
  "deb [arch=$(dpkg --print-architecture) signed-by=/etc/apt/keyrings/docker.asc] https://download.docker.com/linux/ubuntu \
  $(. /etc/os-release && echo "$VERSION_CODENAME") stable" | \
  sudo tee /etc/apt/sources.list.d/docker.list > /dev/null
sudo apt update
```
```sh
sudo apt-get install containerd.io 
```
  
3) Configuring `containerd`
back to our docs: creating essentials and folders +default containerd config +reboot -> enabling and running systemd cgroup.

```sh
mkdir -p /etc/containerd
```
```containerd
containerd config default > /etc/containerd/config.toml
sed -i 's/SystemdCgroup = false/SystemdCgroup = true/g' /etc/containerd/config.toml
```
```
systemctl restart containerd
systemctl status containerd
```

4) `crictl`, ensure we create a configuration file as mentioned below
```
cat > /etc/crictl.yaml <<EOF
runtime-endpoint: unix:///run/containerd/containerd.sock
image-endpoint: unix:///run/containerd/containerd.sock
timeout: 2
EOF
```

5) installing `k8s` packages + adding `gpg` key + repo + components [**kubeadm official docs**]
```sh
apt update 
sudo apt-get install -y apt-transport-https ca-certificates curl gpg
```
```sh
sudo mkdir -p -m 755 /etc/apt/keyrings
curl -fsSL https://pkgs.k8s.io/core:/stable:/v1.30/deb/Release.key | sudo gpg --dearmor -o /etc/apt/keyrings/kubernetes-apt-keyring.gpg
```
```shell
echo 'deb [signed-by=/etc/apt/keyrings/kubernetes-apt-keyring.gpg] https://pkgs.k8s.io/core:/stable:/v1.30/deb/ /' | sudo tee /etc/apt/sources.list.d/kubernetes.list
```
```sh
sudo apt update
sudo apt install -y kubelet kubeadm kubectl
sudo apt-mark hold kubelet kubeadm kubectl
```
```
sudo systemctl enable --now kubelet
```

6) Bootstrapping control plane to create `k8s` clusters
## RUN CONTROL PLANE ON ONE AND WORKER ON OTHER SEPERATELY.
-> On control nodes (k8s master node): [](https://github.com/rithishsamm/myTutorialHell/blob/main/Orchestration/k8engineers.com/kubernetes_latest_manifest/Kubernetes/01-kubernetes-architecture-Installation/03-k8s-setup-kubeadm-containerd.md#on-control-nodes-k8s-master-node)
- "--pod-network-cidr" was changes here on local setup. Since my VMs are using same series network.
- So instead of applying the calico YAML files, first download them and update the CIDR of pod to be used cidr which ever "For example: 10.244.0.0/16 private class".
```
kubeadm init --apiserver-advertise-address=IP --cri-socket=/run/containerd/containerd.sock --pod-network-cidr=CLASS-A-pvt-network
```

```
kubeadm init --apiserver-advertise-address=192.168.0.153 --cri-socket=/run/containerd/containerd.sock --pod-network-cidr=10.244.0.0/16
```

*Output: Your Kubernetes control-plane has initialized successfully!*
> ==**BOOTSTRAPPING CONTROL PLANE IS DONE SUCCESSFULLY ON KUBEADM**==

```sh
mkdir -p $HOME/.kube
sudo cp -i /etc/kubernetes/admin.conf $HOME/.kube/config
sudo chown $(id -u):$(id -g) $HOME/.kube/config
```
k8s deployed successfully done everything as a super user.. 

## ==!IMPORTANT==
re-init or keep the session without clear +  there will be `kubeadm join` -> KEEP THIS ASIDE
- control plane
```kubeadm
kubeadm join 192.168.0.153:6443 --token kcouvm.8c1qftekbt92anm0 \
        --discovery-token-ca-cert-hash sha256:77230416fa024c6eecb42c612bb574098d92af586812d56fc26f7889cb3ae3cd

```

###### Configuring the CRI container network interface: - CALICO CNI -  official docs [](https://docs.tigera.io/calico/latest/getting-started/kubernetes/quickstart#how-to)
***ONLY ONE THE CONTROL PLANE NODE***
1) Install the Tigera Calico operator and custom resource definitions.
```kubectl
kubectl create -f https://raw.githubusercontent.com/projectcalico/calico/v3.28.0/manifests/tigera-operator.yaml
```
```sh
wget https://raw.githubusercontent.com/projectcalico/calico/v3.28.0/manifests/custom-resources.yaml
```

==CHECK IF THE APPROPRIATE **CIDR** APPLIED==
`cidr: 10.244.0.0/16`
```kubectl
kubectl apply -f custom-resources.yaml
```

## NOW, in the worker node server,
join the control with kubeadm,
```kubeadm
kubeadm init --apiserver-advertise-address=192.168.0.153 --cri-socket=/run/containerd/containerd.sock --pod-network-cidr=10.244.0.0/16
```
>CIDR MATTERS!
+
**SUCCESSFULLY CONNECTED KUBEADM WORKER NODE WITH CONTROL PLANE NODE**

Output:
> [!NOTE]
> ```
This node has joined the cluster: 1)  Certificate signing request was sent to apiserver and a response was received. 2) The Kubelet was informed of the new secure connection details.
Run 'kubectl get nodes' on the control-plane to see this node join the cluster.

CHECK BACK ON **CONTROL PLANE NODE**:
```kubectl
kubectl get nodes
```

| NAME               | STATUS | ROLES | AGE    | VERSION |
| ------------------ | ------ | ----- | ------ | ------- |
| kubadm2nworkernode | Ready  | none  | 6m52s  | v1.30.3 |
| kubadm2nctrlplane  | Ready  | none  | 12m52s | v1.30.3 |

not ready, since the container is getting created. To check more about whats happening in the node:
```
kubectl get po -A -o wide
```

==**Multi-node SETUP HAS BEEN DONE SUCCESSFULLY! voila!**==
*****
# 6. Part1- Kubernetes HA setup using kubeadm tool(cri-containerd)
### Kubernetes HA setup and configuration usingÂ **kubeadm**Â tool [](https://github.com/rithishsamm/myTutorialHell/blob/main/Orchestration/k8engineers.com/kubernetes_latest_manifest/Kubernetes/01-kubernetes-architecture-Installation/05-k8s-ha-kubeadm-containerd.md#kubernetes-ha-setup-and-configuration-using-kubeadm-tool)

Previously, we have configured single node having both the nodes on one single node under one single VM, multi-node using Kubeadm. Similarly, will deploy a K8s HA Setup using the same.

### k8s Tools used
- kubeadm
- kubectl
- kubelet

**OBJECTIVE**: 
	Deploying more than one control plane node. List of nodes used for Kubernetes HA setup. In order to manage traffic to a control plane node, need of LOAD BALANCER is a necessary.
- 1 node for HAproxy 
- 3 nodes for Control plane
- 1 or more nodes for Compute plane (as per our need)

In our case, will be using for the case for **Control plane load balancing** using **HA Proxy.** Other proxies can be applicable too such as Load Balancer's from Nginx, Apache and more as per Ops convenience. We will be using HA Proxy Load balancer for our case. 
 
Means, the `kubectl` or all of my API calls will be made and gets transmitted to this **HA Proxy Load Balance**r to all the relevant **control plane node**s. 

This setup will be well fit in AWS - Amazon Web Services placing all these nodes on the cloud.
#### Cloud Setup:
Reason for such setup in the cloud is that, in on-prem means on my or your local machine takes up a *lot of resources* for such use cases. Cloud will be a much wiser and convenient option for both the local setup and to the production grade deployment. 

**Setup Environment:**
- AWS account (Free trail)
- **3 control planes have been used as part of HA setup using AWS EC2 service** (Since we might have less resource on-prem)
- Compute plane nodes consist of few Kubernetes workloads like PODs, Replication Controllers, ReplicaSets, Deployments, DaemonSet, Statefulset, Jobs, CronJobs and etc.,
> How many if these components that we will be make use of is depend on how much of a complex application, or many of the application or microservices that you'd seek to run on the same. 

If it is a simple application, one or two compute plane is more than enough for such workloads. List of nodes used to setup Kubernetes HA.
- **1** node for **HAproxy**
- **3** nodes for **Control plane**
- **2** or more nodes for **Compute plane** (as per our need)
Totally six nodes will be running here as nodes on each EC2 Instances.

**Nodes System Specs and Prerequisite:**
- Ubuntu 22.04 or more (LTS Standard) 
- Disable swap
free -h
/etc/fstab/ -> # comment out swp
- Spec: 15GB, 2vCPU, 4GB RAM - t2medium++
> Will deploy all these nodes.

###### EC2 Installation Steps:
**AWS** -> Region (**Mumbai**) -> 

==**EC2**== -> Name -> Application or OS Image: **Ubuntu 22.04 LTS** -> Arch: **64bit x86** -> Instance Type: **2vCPU, 4GB RAM - t2micro** -> Key pair: **RSA** or **ED25519**-> Network -> Select **VPC** and **Subnets** -> Security Group: Select **SG** -> Storage: **No of GB** volume->  Number of Instances to launch: **6** (HAProxy -**1**, Control Plane node- **3**, Compute Worker Node- **2**). -> ==**Launch instance**== -> 
Name All the instances:
1) HA Proxy
2) Control Plane 1
3) Control Plane 2
4) Control Plane 3
5) Worker Node 1
6) Worker Node 2

## Now applying some Common operations on all nodes - except HAproxy node
 1) Become a root user and Update ubuntu
```sh
sudo apt update
```
2) Upgrade
```sh
sudo apt full-upgrade
```
3) install K8s and CRI Packages
```sh
sudo apt-get install ca-certificates curl gnupg
```
and further which will cover further on the following docs.
*****
# 7. Part2- Kubernetes HA setup using kubeadm tool (cri-containerd)

Will log into all these nodes and configure the same + installing K8 Components such as container runtime interface, kubeadm, kubelet and kubectl to end up having kubeadm join command.

1) SSH into a worker node 1 & 2:
Mobaxterm -> Create Session -> SSH -> IP + Username + Key ->Login
2) All all the same for all the EC2 nodes + Split screen with Multi-Exec mode
3) Exclude all the multi-exec except two Worker Nodes
4) Root into those worker nodes
Worker Nodes setup:
1) root
```sh
su root
```
 2) apt update and upgrade
```sh
 apt update && apt upgrade -y
```
3)  **installing `containerd`,** 
- Add Docker's official GPG key:
```sh
sudo apt-get update
sudo apt-get install ca-certificates curl
sudo install -m 0755 -d /etc/apt/keyrings
sudo curl -fsSL https://download.docker.com/linux/ubuntu/gpg -o /etc/apt/keyrings/docker.asc
sudo chmod a+r /etc/apt/keyrings/docker.asc
```
- Add the repository to Apt sources:
```sh
echo \
  "deb [arch=$(dpkg --print-architecture) signed-by=/etc/apt/keyrings/docker.asc] https://download.docker.com/linux/ubuntu \
  $(. /etc/os-release && echo "$VERSION_CODENAME") stable" | \
  sudo tee /etc/apt/sources.list.d/docker.list > /dev/null
sudo apt update
```
- installing containerd component
```sh
sudo apt-get install containerd
```
4) Configuring `containerd` and `kube-components` 
- `containerd` conf
```sh
cat > /etc/modules-load.d/containerd.conf <<EOF
overlay
br_netfilter
EOF
```
- enabling conf mods
```sh
modprobe overlay
modprobe br_netfilter
```
- `systemctl` conf, setup required sysctl params, these persist across reboots. 
```
```sh
cat > /etc/sysctl.d/99-kubernetes-cri.conf <<EOF
net.bridge.bridge-nf-call-iptables  = 1
net.ipv4.ip_forward                 = 1
net.bridge.bridge-nf-call-ip6tables = 1
EOF
```
- enabling `systemctl` params/applying changes
```sh
sysctl --system
```
 - **Install `containerd`**
```sh
apt-get update && apt-get install -y containerd.io
```
- Configure `containerd`, create dir + conf file + pass params 
```
mkdir -p /etc/containerd
containerd config default > /etc/containerd/config.toml
sed -i 's/SystemdCgroup = false/SystemdCgroup = true/g' /etc/containerd/config.toml
```
- Restart containerd
```sh
systemctl restart containerd
```
- To execute crictl CLI commands, ensure we create a configuration file as mentioned below
```sh
cat > /etc/crictl.yaml <<EOF
runtime-endpoint: unix:///run/containerd/containerd.sock
image-endpoint: unix:///run/containerd/containerd.sock
timeout: 2
EOF
```
5) Installing K8s components such as `Kubectl`, `kubelet` and `kubeadm`(as per official kubeadm docs)
```shell
sudo apt-get update
# apt-transport-https may be a dummy package; if so, you can skip that package
sudo apt-get install -y apt-transport-https ca-certificates curl gpg
```
6) Adding `gpg-key`
```shell
sudo mkdir -p -m 755 /etc/apt/keyrings
curl -fsSL https://pkgs.k8s.io/core:/stable:/v1.30/deb/Release.key | sudo gpg --dearmor -o /etc/apt/keyrings/kubernetes-apt-keyring.gpg
```
7)  This overwrites any existing configuration in ```
```sh
/etc/apt/sources.list.d/kubernetes.list
echo 'deb [signed-by=/etc/apt/keyrings/kubernetes-apt-keyring.gpg] https://pkgs.k8s.io/core:/stable:/v1.30/deb/ /' | sudo tee /etc/apt/sources.list.d/kubernetes.list
```
8) update and hold
```shell
sudo apt-get update
sudo apt-get install -y kubelet kubeadm kubectl
sudo apt-mark hold kubelet kubeadm kubectl
```
9) enabling services
```sh
sudo systemctl enable --now kubelet kubectl kubeadm
```
10) Reboot and login back as root
```
init 6
su root
sudo apt-mark hold kubelet kubeadm kubectl
```
then `kubeadm` `join` with your control plane! Further setup yet to be configured.

***
### Part3- Kubernetes HA setup using kubeadm tool(cri-containerd)
Here, we will be covering the remaining and stranded **HAProxy Instance** in our AWS EC2.

```
sudo apt update && apt fulll-upgrade -y
sudo reboot
```
or **Reboot Instance** button in AWS.
>**Note:** Rebooting instance doesn't change our IP but starting and stopping. 
==Reconnect and Login again.==

```
sudo -i
```

**HAProxy Setup Configuration:**
Installing HAProxy Package and do minimal config working with the K8s Virtual Network with mapping IPs of control plane node and more to compliantly work with our Kubeadm Setup.

1) Will see all of that below here. Now, Install **HAProxy**
```
sudo apt install -y haproxy
```
2) Update all the network config of it.
```
 Â nano /etc/haproxyhaproxy.cfg
```
```
global
        log /dev/log    local0
        log /dev/log    local1 notice
        chroot /var/lib/haproxy
        stats socket /run/haproxy/admin.sock mode 660 level admin expose-fd listeners
        stats timeout 30s
        user haproxy
        group haproxy
        daemon

        # Default SSL material locations
        ca-base /etc/ssl/certs
        crt-base /etc/ssl/private

        # See: https://ssl-config.mozilla.org/#server=haproxy&server-version=2.0.3&config=intermediate
        ssl-default-bind-ciphers ECDHE-ECDSA-AES128-GCM-SHA256:ECDHE-RSA-AES128-GCM-SHA256:ECDHE-ECDSA-AES256-GCM-SHA384:ECDHE-RSA-AES256-GCM-SHA384:ECDHE-ECDSA-CHACHA20-POLY1305:ECDHE-RSA-CHACHA20-POLY1305:DHE-RSA-AES128-GCM-SHA256:DHE-RSA-AES256-GCM-SHA384
        ssl-default-bind-ciphersuites TLS_AES_128_GCM_SHA256:TLS_AES_256_GCM_SHA384:TLS_CHACHA20_POLY1305_SHA256
        ssl-default-bind-options ssl-min-ver TLSv1.2 no-tls-tickets

defaults
        log     global
        mode    http
        option  httplog
        option  dontlognull
        timeout connect 5000
        timeout client  50000
        timeout server  50000
        errorfile 400 /etc/haproxy/errors/400.http
        errorfile 403 /etc/haproxy/errors/403.http
        errorfile 408 /etc/haproxy/errors/408.http
        errorfile 500 /etc/haproxy/errors/500.http
        errorfile 502 /etc/haproxy/errors/502.http
        errorfile 503 /etc/haproxy/errors/503.http
        errorfile 504 /etc/haproxy/errors/504.http

frontend kube-apiserver
        bind *:6443
        mode tcp
        option tcplog
        default_backend kube-apiserver

backend kube-apiserver
        mode tcp
        option tcplog
        option tcp-check
        balance roundrobin
        default-server inter 10s downinter 5s rise 2 fall 2 slowstart 60s maxconn 20 maxqueue 256 weight 100
        server kube-apiserver-1 <controplanode-ip>:6443 check
        server kube-apiserver-2 <controplanode-ip>:6443 check
        server kube-apiserver-3 <controplanode-ip>:6443 check        
```
here, replace all of the existing Content + Control Plane's IP.
3) enable `haproxy`,
```
sudo systemctl enable haproxy
sudo systemctl restart haproxy
sudo systemctl status haproxy
```
Enable port `6443` on the subnet.

4) Test the `HAProxy` server is reachable from any of these control plane nodes.
```
ping #haproxy's IP
```
>==HAProxy== Load Balancer is successfully configured! done!

***
### Part4-Kubernetes HA setup using kubeadm tool(cri-containerd)
This part's context is mostly based out of this documentation: https://kubernetes.io/docs/setup/production-environment/tools/kubeadm/high-availability/

Here, it briefly explains about
1) What is HASetup
2) How to configure
3) Prerequisites 
4) and measures to be considered
#### Before we begin[](https://kubernetes.io/docs/setup/production-environment/tools/kubeadm/high-availability/#before-you-begin)
The prerequisites depend on which topology that we have selected for our cluster's control plane, there are two:
1) Stacked etcd
2) External etcd
the names says itself. Here, we will be using stacked vanilla etcd. `etcd` will be a part of our control plane nodes.

Ensure that our HAProxy LoadBalancer instance is reachable or not from the Control Plane EC2. 
```
ping #HAProxy's IP
```
```
nc -v #HAProxy's IP 6443
```

  IT SHOULD THROW 
```
connection to <HAPorxy'sIP> 6443 port[tcp/*] successded!  
```
>==DO THE SAME FOR ALL THE CONTROL PLANE NODES TOO!==
>IF it throws error in anyway, There must be an error configuring HAProxy node or on the AWS Security Group Rules.

Out of these three control plane nodes, i need to bootstrap one node. *Applies to any cluster whether it is a DB, Webserver, Cache, Big Data Haddop and more* ANY CLUSTER BY ANY MEANS ANY FORM. Th concept of ==**BOOTSTRAPPING**== APPLIES TO ALL. 
> ==**BOOTSTRAPPING**== IS A TERM THAT IN A CLUSTER LIKE ENVIRONMENT, TAKING ALL THESE COMPONENTS IN ORDER TO LAUNCH FOR PREFLIGHT CHECK AND DEPLOY YOUR WORKLOADS.
> 
Here, will bring these clusters for both control plane node and compute plane node. In control plane node, we'll be bringing up only one node. The process of bringing up and getting things together in prior on the frontier for a launch is called **BOOTSTRAPPING**

Other nodes will be joining in the cluster such as other control planes, Load balancers or worker nodes to the frontier **bootstrapping control plane node**.
##### Stacked control plane and etcd nodes[](https://kubernetes.io/docs/setup/production-environment/tools/kubeadm/high-availability/#stacked-control-plane-and-etcd-nodes)
Steps for the first control plane node[](https://kubernetes.io/docs/setup/production-environment/tools/kubeadm/high-availability/#steps-for-the-first-control-plane-node)
1. Initialize the control plane:
syntax:
```sh
sudo kubeadm init --control-plane-endpoint "LOAD_BALANCER_DNS:LOAD_BALANCER_PORT" --upload-certs --pod-network-cidr 192.168.0.0/16
```
- `--control-plane-endpoint`Â flag should be set to the address or DNS and port of the load balancer.
- `--upload-certs`Â flag is used to upload the certificates that should be shared across all the control-plane instances to the cluster.
- `--pod-network-cidr` 192.168.0.0/16 applies CIDR to the network. This must not conflict with the network that the kubernetes nodes uses. If this series has been using CIDR already, Do not overlap by adding the same on top of it. Nodes and pods having same series. Another one will conflict with the same. Thats why we're using a different one. if not, it will ping either the pod or the node.
Ensure that HAProxy EC2 node is up and running to get align with the same. 

```sh
sudo kubeadm init --control-plane-endpoint "HAPROXY pvt IP":6443 --upload-certs --pod-network-cidr 192.168.0.0/16
```

 This will start to **bootstrapping** one of the node.  
the same output after bootstrapping will pop up telling that `The Kubernetes Control plane has initialized successfully.
To start using the clusters, configs will be given and `kubeadm apply` for **deploying pod network to the cluster** and `kubeadm join` to **join to any number of control plane nodes.**

Next, we gotta configure CNI. **CONTAINER NETWORK INTERFACE** FOR KUBERNETES to manage K8s Pods IP and networking stuff. Using **Calico CNI** for that. 

Ref: [](https://docs.tigera.io/calico/latest/getting-started/kubernetes/quickstart)
Since we all gave our pre configs such as CIDR and all, we can proceed straight to, [](https://docs.tigera.io/calico/latest/getting-started/kubernetes/quickstart#install-calico) + configure the custom CIDR while executing kubeadm command. Since we configured CIDR, Will get straight into installation.
###### Install Calico[â€‹](https://docs.tigera.io/calico/latest/getting-started/kubernetes/quickstart#install-calico "Direct link to Install Calico") as per official documentation
1. Install the Tigera Calico operator and custom resource definitions.
```
kubectl create -f https://raw.githubusercontent.com/projectcalico/calico/v3.28.1/manifests/tigera-operator.yaml
```
2. Install Calico by creating the necessary custom resource. For more information on configuration options available in this manifest, seeÂ [the installation reference](https://docs.tigera.io/calico/latest/reference/installation/api).
```
kubectl create -f https://raw.githubusercontent.com/projectcalico/calico/v3.28.1/manifests/custom-resources.yaml
```

Now, do give `kubeadm join` command. Objective is to join with the control plane nodes. so from,
> compute plane -> kubeadm join -> control plane

```
kubeadm join ip --token token \ --discovery-ca-cert-hash sha256hasg \ --control-plane --cerficate-key key
```
> ON BOTH THE NODES.
> 
> does all the preflight checks and runs things. (static etcd. etcd will be running on the same control plane node not externally since it is node external etcd.)

After all these completed setup. We just need to check all the nodes available and running from the **BOOTSTRAPPED FRONTIER NODE**. Check that by doing,
```
kubectl get no -o wide
```

> All the nodes will be in ready status. 

**NOW, ALL OUR CONTROL PLANE NODES HAS BEEN SET UP TO MEET HIGH AVAILABILITY KUBERNETES CLUSTER**. IT'S ALL DONE!

Now, will complete the setup by connecting the same to the compute nodes to the control plane nodes, and NOW ALL THE CONTROL PLANE NODES ARE READY TO ACCEPT THE WORKER NODES join requests. `kubeadm join` for connecting it at the last.

Whatever request (even from worker nodes via kubectl) that goes to the cluster,
will be getting through 
**HAProxy** -> One of the backend **Control plane node** 1, 2, 3 -> Worker
```
kubectl get no
watch kubectl get no
```

To watch of the telemetry of the request, we can check that by tapping the logs.
```
tail -f /var/log/haproxy.log
```
USES THE ROUND ROBIN WAY OF DISTRIBUTING REQUESTS TO EACH NODE.

Next, we will be configuring and joining all the same with compute plane nodes.

***
### Part5- Kubernetes HA setup using kubeadm tool(cri-containerd)

Log into all the nodes except HAProxy.  (Initial configuration tasks before running it on full flow).

Tasks:
1) Tasks
2) Updating Index page
3) Updating packages
4) Installing Kubernetes components, such as CRI, 
5) and more..

Login into all the same as a root user! 
First in all three control plane nodes as root + Multi-exec mode.
```
sudo -i
```

Now,
Docker installation, not to install docker on kubernetes but `containerd` as K8s CRI.

 