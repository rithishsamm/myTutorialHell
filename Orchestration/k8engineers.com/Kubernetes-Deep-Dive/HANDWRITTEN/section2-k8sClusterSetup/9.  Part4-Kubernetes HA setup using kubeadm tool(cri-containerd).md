This part's context is mostly based out of this documentation: https://kubernetes.io/docs/setup/production-environment/tools/kubeadm/high-availability/

Here, it briefly explains about
1) What is HASetup
2) How to configure
3) Prerequisites 
4) and measures to be considered
#### Before we begin[](https://kubernetes.io/docs/setup/production-environment/tools/kubeadm/high-availability/#before-you-begin)
The prerequisites depend on which topology that we have selected for our cluster's control plane, there are two:
1) Stacked etcd
2) External etcd
the names says itself. Here, we will be using stacked vanilla etcd. `etcd` will be a part of our control plane nodes.

Ensure that our HAProxy LoadBalancer instance is reachable or not from the Control Plane EC2. 
```
ping #HAProxy's IP
```
```
nc -v #HAProxy's IP 6443
```

  IT SHOULD THROW 
```
connection to <HAPorxy'sIP> 6443 port[tcp/*] successded!  
```
>==DO THE SAME FOR ALL THE CONTROL PLANE NODES TOO!==
>IF it throws error in anyway, There must be an error configuring HAProxy node or on the AWS Security Group Rules.

Out of these three control plane nodes, i need to bootstrap one node. *Applies to any cluster whether it is a DB, Webserver, Cache, Big Data Haddop and more* ANY CLUSTER BY ANY MEANS ANY FORM. Th concept of ==**BOOTSTRAPPING**== APPLIES TO ALL. 
> ==**BOOTSTRAPPING**== IS A TERM THAT IN A CLUSTER LIKE ENVIRONMENT, TAKING ALL THESE COMPONENTS IN ORDER TO LAUNCH FOR PREFLIGHT CHECK AND DEPLOY YOUR WORKLOADS.
> 
Here, will bring these clusters for both control plane node and compute plane node. In control plane node, we'll be bringing up only one node. The process of bringing up and getting things together in prior on the frontier for a launch is called **BOOTSTRAPPING**

Other nodes will be joining in the cluster such as other control planes, Load balancers or worker nodes to the frontier **bootstrapping control plane node**.
##### Stacked control plane and etcd nodes[](https://kubernetes.io/docs/setup/production-environment/tools/kubeadm/high-availability/#stacked-control-plane-and-etcd-nodes)
Steps for the first control plane node[](https://kubernetes.io/docs/setup/production-environment/tools/kubeadm/high-availability/#steps-for-the-first-control-plane-node)
1. Initialize the control plane:
syntax:
```sh
sudo kubeadm init --control-plane-endpoint "LOAD_BALANCER_DNS:LOAD_BALANCER_PORT" --upload-certs --pod-network-cidr 192.168.0.0/16
```
- `--control-plane-endpoint` flag should be set to the address or DNS and port of the load balancer.
- `--upload-certs` flag is used to upload the certificates that should be shared across all the control-plane instances to the cluster.
- `--pod-network-cidr` 192.168.0.0/16 applies CIDR to the network. This must not conflict with the network that the kubernetes nodes uses. If this series has been using CIDR already, Do not overlap by adding the same on top of it. Nodes and pods having same series. Another one will conflict with the same. Thats why we're using a different one. if not, it will ping either the pod or the node.
Ensure that HAProxy EC2 node is up and running to get align with the same. 

```sh
sudo kubeadm init --control-plane-endpoint "HAPROXY pvt IP":6443 --upload-certs --pod-network-cidr 192.168.0.0/16
```

 This will start to **bootstrapping** one of the node.  
the same output after bootstrapping will pop up telling that `The Kubernetes Control plane has initialized successfully.
To start using the clusters, configs will be given and `kubeadm apply` for **deploying pod network to the cluster** and `kubeadm join` to **join to any number of control plane nodes.**

Next, we gotta configure CNI. **CONTAINER NETWORK INTERFACE** FOR KUBERNETES to manage K8s Pods IP and networking stuff. Using **Calico CNI** for that. 

Ref: [](https://docs.tigera.io/calico/latest/getting-started/kubernetes/quickstart)
Since we all gave our pre configs such as CIDR and all, we can proceed straight to, [](https://docs.tigera.io/calico/latest/getting-started/kubernetes/quickstart#install-calico) + configure the custom CIDR while executing kubeadm command. Since we configured CIDR, Will get straight into installation.
###### Install Calico[​](https://docs.tigera.io/calico/latest/getting-started/kubernetes/quickstart#install-calico "Direct link to Install Calico") as per official documentation
1. Install the Tigera Calico operator and custom resource definitions.
```
kubectl create -f https://raw.githubusercontent.com/projectcalico/calico/v3.28.1/manifests/tigera-operator.yaml
```
2. Install Calico by creating the necessary custom resource. For more information on configuration options available in this manifest, see [the installation reference](https://docs.tigera.io/calico/latest/reference/installation/api).
```
kubectl create -f https://raw.githubusercontent.com/projectcalico/calico/v3.28.1/manifests/custom-resources.yaml
```

Now, do give `kubeadm join` command. Objective is to join with the control plane nodes. so from,
> compute plane -> kubeadm join -> control plane

```
kubeadm join ip --token token \ --discovery-ca-cert-hash sha256hasg \ --control-plane --cerficate-key key
```
> ON BOTH THE NODES.
> 
> does all the preflight checks and runs things. (static etcd. etcd will be running on the same control plane node not externally since it is node external etcd.)

After all these completed setup. We just need to check all the nodes available and running from the **BOOTSTRAPPED FRONTIER NODE**. Check that by doing,
```
kubectl get no -o wide
```

> All the nodes will be in ready status. 

**NOW, ALL OUR CONTROL PLANE NODES HAS BEEN SET UP TO MEET HIGH AVAILABILITY KUBERNETES CLUSTER**. IT'S ALL DONE!

Now, will complete the setup by connecting the same to the compute nodes to the control plane nodes, and NOW ALL THE CONTROL PLANE NODES ARE READY TO ACCEPT THE WORKER NODES join requests. `kubeadm join` for connecting it at the last.

Whatever request (even from worker nodes via kubectl) that goes to the cluster,
will be getting through 
**HAProxy** -> One of the backend **Control plane node** 1, 2, 3 -> Worker
```
kubectl get no
watch kubectl get no
```

To watch of the telemetry of the request, we can check that by tapping the logs.
```
tail -f /var/log/haproxy.log
```
USES THE ROUND ROBIN WAY OF DISTRIBUTING REQUESTS TO EACH NODE.

Next, we will be configuring and joining all the same with compute plane nodes.