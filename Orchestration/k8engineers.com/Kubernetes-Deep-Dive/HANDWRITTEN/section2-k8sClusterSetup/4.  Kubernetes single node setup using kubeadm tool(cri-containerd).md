Setting up Single-node Kube setup to create a cluster infra using Kubeadm tool:
In this demo, we use:
1) **Kubeadm** - to create cluster
2) **containerd v1.25** - Container runtime (plugin plugged in)
for system reference:
```bash
ipconfig/all
```
###### CREATE A VM
1) Any VM Engine as per your convenience. Ill be using Hyper-V. 
2) Create VM
	1) Name - vmName (kubeadm-Single-Node)
	2) ISO folder -Ubuntu 22.04 - live server jammy jelly
	3) Image
	4) Type
	5) Version

Hyper-V steps:
1) Open Hyper-V
2) New -> Create VM -> next
3) Name, Gen2, RAM (4096MB/2GB), Bridge Connection (for static IP) - > 4x Next
4) Name, Location, Storage Size (50gb) -> all default -> Next
5) OS (ISO location) 
6) Preview 
7) Finish
Right Click -> Settings
1) Security -> Disable Secure Boot
2) Apply convenient configurations
3) Apply and Ok
4) Open VM

Virtual Box Setup:
1) New
2) Name, ISO, Skip Unattended Installation
3) Spec: 4CPU, 4GB RAM
4) Storage
5) Pre-allocate : false

Ubuntu 24.0 Boot Settings:
1) Try Install
2) Language
3) Continue with or without (I'm continuing without updating)
4) Keyboard Layout (with default)
5) base OS Version (Ubuntu Server Standard)
6) Network Interface config 
`Note: We can see an IP allocated by default without config. This happened because of bridge Network where it automatically allocates an IP cause of DHCP given by router. Which isn't static or permanent. 
**We'd like to have a static IP to get the image be persistent.**
> Will configure the same.

2) (Tab) to eth0 => Edit IPv4 -> (say that it is Automatic(DHCP)(ip gets allocated just like your mobile gets connected to the wifi giving an IP) **These IPs comes from router**)
	`Need a fixed IP address, have to change automatic to static to assign a fixed IP to the system, in this case, VM`

3)  Enter -> Manual
` Open cmd -> ipconfig -> Ethernet adapter - Bridged, Have this aside for this reference)`

```shell
   Connection-specific DNS Suffix  . :
   Link-local IPv6 Address . . . . . : fe80::612f:24d5:4f01:4e62%17
   IPv4 Address. . . . . . . . . . . : 192.168.0.158
   Subnet Mask . . . . . . . . . . . : 255.255.255.0
   Default Gateway . . . . . . . . . : 192.168.0.1
```

4)  Edit eth0 IPv4 Configuration
	IPv4 Method: Manual
**-> Subnet is nothing but base IP Address. In cmd prompt, shows the IPv4 address - 192.168.0.0/24 based on the subnet mask which is 255.255.255.0. Based on the subnet mask, its /24.**  
If any 
	Subnet: 192.168.0.0 (subnet rule IP/Mask)
**-> Address is what Static IP that you'd wanted to assign to this persistent VM. (in this case, in my network, i've assigned what hasn't been used before)**
	Address: 192.168.0.==143==
**-> Gateway from base machine will not be much of a problem.**
	Gateway: 192.168.0.1
**-> here, you can use google, Cloudflare's, cisco name servers or the ISP Provided name servers.** 
	Name Servers: 8.8.8.8,8.8.4.4 //Google's
	Search Domains: -blank-

5) Now, Network Config Interface:
	eth0 - 
	static: 792.168.0.222/24

6) Configure Proxy, if required
	Proxy Address: 
7) Mirror Default
8) Storage Config (default) - OS to take care of it. 
	Option1
PREVIEW -> Done + Continue!

Profile Setup:
1) Name
2) Server (computerName)
3) username
4) password
5) confirm

Ubuntu Pro: Apply if required 
install Essentials - such as openSSH and more.
Done!

Checks:
1) Login
2) Ping
3) Check network
4) IP and more
5) Connect remote - ssh username@IP and password, yes and password  
6) check ip -a
all done.
> -- So far we have performed setting the VM for Kubernetes. Now will set-up kubernetes single node setup. --

prerequisite for kubernetes kubeadm single node setup,
COMPLETED SINGLE NODE KUBEADM VM SETUP WITH SSH COMPATIBILITY.
Now in this particular machine, will see how to 
#### ==Setup Single Node having One Control plane + worker node:==
Steps and Procedures to follow the same - 
USING KUBEADM TOOL TO SETUP KUBERNETES CLUSTER +  Containerd as Container runtime interface in the backend -> For kubernetes to launch the application.

**Prepared a document which** will have all the manifest and refer the same.
Reference: https://github.com/devopsyuva/kubernetes_latest_manifest
-> if docker, 2) k8s-setup-kubeadm-containerd.md
-> if containerd, 3) k8s-setup-kubeadm-containerd.md
All the documentation for the same have delivered here.

**Using K8s Setup using kubeadm *containerd*** [Doc](obsidian://open?vault=tutorialHell&file=Orchestration%2Fk8engineers.com%2Fkubernetes_latest_manifest%2FKubernetes%2F01-kubernetes-architecture-Installation%2F01-k8s-components)
How and why we're using containerd:
![[Pasted image 20240711151147.png]]
1) all the system specs 
> This documents includes both ==**Single and Two node setup.**==

**PREREQUISITES TO CONFIGURE CONTAINERD:**
1) sudo -i and apt updates
2)  verify if reboot required checking by -> `ls -l /vat/run/reboot-required/` 
-> if it doesn't exist, no reboot is required
4) Follow:
-> BEFORE INSTALLING KUBEADM + CONTAINERD -> We have to ==install some packages== + ==enable and modify some modules== + ==network configurations==:
enabling  
1) overlay
2) br_netfilter -> Bridge network Filter
with the cmd 
`modprobe overlay` and `modeprobe br_netfilter` 
> Problem with executing these commands standalone is that these command has to be executed after every reboot. Instead, will make all of those permanent. for that, save it in a file as script named `containerd.conf`. ,**MODULES ARE ENABLED SUCCESSFULLY.**

After that, we should to do some systemctl parameters. Will execute the third para comment to do the save it a file named `kubernetes-cri.conf`.  

After applying these, you need to enable these changes, for that can't reboot the system instead, `systemctl --system` 
```shell
modprobe overlay
modprobe br_netfilter
```
```sh
cat > /etc/modules-load.d/containerd.conf <<EOF
overlay
br_netfilter
EOF  
```
Setup required sysctl params, these persist across reboots.
```sh
cat > /etc/sysctl.d/99-kubernetes-cri.conf <<EOF
net.bridge.bridge-nf-call-iptables  = 1
net.ipv4.ip_forward                 = 1
net.bridge.bridge-nf-call-ip6tables = 1
EOF
```
```sh
sysctl --system
```
> to check and enable all of these,
>`sysctl --system`

**NOW ALL THE MINIMAL REQUIREMENTS ARE DONE! NEXT, WILL CONTINUE WITH THE REST INSTALLING OTHER COMPONENTS**

## `containerd` INSTALLATION
Will follow all the steps to install containerd package on all nodes. REFER THE SAME OR REFER OFFICIAL DOCKER DOCS. -> since we are installing containerd not docker packages.
- Set up the repository
- Install packages to allow apt to use a repository over HTTPS
```Shell
sudo apt-get install ca-certificates curl gnupg lsb-releases
```
 
or else, refer: 
[Reference:](https://github.com/rithishsamm/myTutorialHell/blob/main/Orchestration/k8engineers.com/kubernetes_latest_manifest/Kubernetes/01-kubernetes-architecture-Installation/03-k8s-setup-kubeadm-containerd.md#reference)
- [Containerd](https://docs.docker.com/engine/install/ubuntu/)
- [kubeadm install](https://kubernetes.io/docs/setup/production-environment/tools/kubeadm/install-kubeadm/#installing-kubeadm-kubelet-and-kubectl)
++
- [crictl](https://github.com/kubernetes-sigs/cri-tools/blob/master/docs/crictl.md)
- [containerd](https://kubernetes.io/blog/2018/05/24/kubernetes-containerd-integration-goes-ga/)
- [Docker and Containerd](https://kubernetes.io/docs/tasks/administer-cluster/migrating-from-dockershim/check-if-dockershim-deprecation-affects-you/)
or search [**==Docker install ubuntu==**](https://docs.docker.com/engine/install/ubuntu/). to check accessing ubuntu package index pages:
since we're using docker official docs. WE GOTTA IMPORT THE `GPG key` and store it in the local system. first to create a `dir` and store the same.
1) and 2) Set up repo + Add Docker's official GPG key:
```Shell
sudo mkdir -p /etc/apt/keyrings
sudo install -m 0755 -d /etc/apt/keyrings
sudo curl -fsSL https://download.docker.com/linux/ubuntu/gpg -o /etc/apt/keyrings/docker.asc
sudo chmod a+r /etc/apt/keyrings/docker.asc
```
basic packages to proceed further.  
AND, 
3) after that, need to create a repo.
```bash
# Add the repository to Apt sources:
echo \
  "deb [arch=$(dpkg --print-architecture) signed-by=/etc/apt/keyrings/docker.asc] https://download.docker.com/linux/ubuntu \
  $(. /etc/os-release && echo "$VERSION_CODENAME") stable" | \
  sudo tee /etc/apt/sources.list.d/docker.list > /dev/null
sudo apt-get update
```
whenever you create a new repo especially in the host ubuntu machine, run `apt update`. eg: 
```
ls -l /etc/apt/
```
run `apt update
> NOW, The `apt` can fetch the docker as well. 

After then, will install all the essential packages with the `containerd`
here, in order to install: 
	in docker docs, we have all the commands which we can install relevant run **Docker**,  but we need only `containerd` , to filter and install that, skip all except `containerd.io`:
```
sudo apt update && sudo apt upgrade && install -y containerd.io 
```

  Next,
  We are all done with the installation. NOW WE HAVE TO CONFIGURE THE `containerd`. which means, we need to save the configuration in a file as same as we did before - with minimum required parameters in the configuration. Create the same by,  Configure containerd,
  1) mkdir - Create directory
  2) load config - execute the given command below for saving the output in this file.
  3) changing a Cgroup from **false** to **true**. Can be performed both manually or by via the script.
```bash
mkdir -p /etc/containerd

containerd config default > /etc/containerd/config.toml

sed -i 's/SystemdCgroup = false/SystemdCgroup = true/g' /etc/containerd/config.toml
```
and Restart and check status containerd
```
systemctl restart containerd
systemctl status containerd
```
CONFIGURATIONS IS SUCCESSFULLY COMPLETED AND ITS UP AND RUNNING. 
Other steps to follow:
-- Since we're doing all container things with containerd but not docker, We need something to make communication and interact with containerd. (if docker, we could've executed it's client but not doing that here)

To interact with containerd, `crictl` is a solution where we can perform all the container creation, troubleshooting and the CRI stuff. -> It is available by 
Back to rest and continue following all the step installing crictl:
## `crictl` INSTALLATION
**INORDER TO INTERACT WITH THE `CONTAINERD` TO PERFORM ALL THE ACTIONS LIKE TROUBLESHOOTING, CONTAINER CREATIONS AND ALL THE CRI STUFF, SINCE WEREN'T USING DOCKER HERE TO USE ITS CLI COMMANDS. WE SHOULD HAVE `crictl` tool.** which is all available by default.
*To execute crictl CLI commands, ensure we create a configuration file as mentioned below*. 
Here,  
1) Creating config file for `crictl.yaml` 
2) Means that we're telling `crictl`, to follow this yaml config file to communicate with `containerd`
3) Create and save the data by running the command.
```SHELL
cat > /etc/crictl.yaml <<EOF
runtime-endpoint: unix:///run/containerd/containerd.sock
image-endpoint: unix:///run/containerd/containerd.sock
timeout: 2
EOF
```
now these `crictl` commands will communicate with the `containerd` to fetch kube information.
## `kubadm` `kubernetes`  INSTALLATION
Follow the same documentation or follow official docs. Following the official docs. to follow, search [==**kubeadm install**==](https://kubernetes.io/docs/setup/production-environment/tools/kubeadm/install-kubeadm/), scroll till - [Debian-based distributions](https://kubernetes.io/docs/setup/production-environment/tools/kubeadm/install-kubeadm/#k8s-install-0) section. Now, we have to install all the packages named,
- Kubeadm
- Kubectl
- Kubelet
1) before all of that, will do setup all the prerequisite to setup all of these. To do that,
do run, # apt-transport-https may be a dummy package; if so, you can skip that package. we're installing it.
```shell
sudo apt-get update
sudo apt-get install -y apt-transport-https ca-certificates curl gpg
```
2) Download `gpg keys` for the same:
```shell
curl -fsSL https://pkgs.k8s.io/core:/stable:/v1.30/deb/Release.key | sudo gpg --dearmor -o /etc/apt/keyrings/kubernetes-apt-keyring.gpg
```
3) Add K8s `apt` repo:
```shell
echo 'deb [signed-by=/etc/apt/keyrings/kubernetes-apt-keyring.gpg] https://pkgs.k8s.io/core:/stable:/v1.30/deb/ /' | sudo tee /etc/apt/sources.list.d/kubernetes.list
```

4) Update the `apt` package index, install kubelet, kubeadm and kubectl
> to check available versions of the same, check it by 
> `apt-cache policy kubelet` it'll install the latest if no version specified while installation. **all three of these packages shares and maintains the same version**. if specific version needed, you need to do an `=versionNO`
> ++
> 	WHY HOLD PACKAGES? - **FOR NO AUTO-UPGRADE**. If not, it will do an auto-update  where it is prone to mess the cluster.
```shell
sudo apt-get update
sudo apt-get install -y kubelet kubeadm kubectl
sudo apt-mark hold kubelet kubeadm kubectl
```
5) After installation, Enable the kubelet service before running kubeadm:
```shell
sudo systemctl enable --now kubelet
```
 > Now, WILL CREATE ALL THE KUBERNETES CLUSTERS BY **BOOTSTRAPPING THE SINGLE  NODE both Control Plane and Worker Node.** 

To do that,
SYNTAX:
> kubeadm init 
> - `[mapping IPAddr to apiserver]`do not use localhost or 127.0.0.1IP
> - `[referring config file for cri ]`
> - `[declaring destination network cidr block to deploy pods at the endpoint]`which is the server'S itself. POD NETWORK CIDR.
Every POD inside the Kubernetes Cluster will get an IPAddr from this series - 10.244.0.0/24. This is a class A Network. Execute the same and modify the parameters.

```sh
kubeadm init --apiserver-advertise-address=IPADDR --cri-socket=/run/containerd/containerd.sock --pod-network-cidr=10.244.0.0/16
```

```sh
kubeadm init --apiserver-advertise-address=192.168.0.143 --cri-socket=/run/containerd/containerd.sock --pod-network-cidr=10.244.0.0/16
```

1) Runs all the pre-flight checks => THIS PROCESS IS CALLED AS **BOOTSTRAPPING** the node.
Troubleshoot all of them :(, my encounters are,
-> Swap enabled, to turn off -> 
```sh
free -h
swapoff -a
```
but, it only disables temporarily it. for a permanent turnoff, do comment out the swap section. i gave this #'ed for your reference.
```sh
free -h
nano /etc/fstab 
#/swap.img none swap sw 0 0
```
comment out the hashed one.

**Re-run kubeadm. If any errors still persists. Troubleshoot it.
Since i cancelled the process in-between which already bootstrapped half way and not re-running next other steps-> Steps that i followed is,**
```
kubeadm reset 
iptables
ipvsadm --clear
#if not, sudo apt install ipvsadm
done!
```
or If you know what you are doing, you can `ignore-preflight-checks= whatever to skip or all`**
```sh
kubeadm init --apiserver-advertise-address=192.168.0.143 --cri-socket=/run/containerd/containerd.sock --pod-network-cidr=10.244.0.0/16 --ignore-preflight-checks=all
```
voila! Successfully initialized control-plane. must give the output of,
> [!NOTE] output
> ```
> sudo kubeadm init --apiserver-advertise-address=192.168.0.222 --cri-socket=/run/containerd/containerd.sock --pod-network-cidr=192.168.0.0/24 --ignore-preflight-errors=all
W0712 10:21:46.616343   14940 initconfiguration.go:125] Usage of CRI endpoints without URL scheme is deprecated and can cause kubelet errors in the future. Automatically prepending scheme "unix" to the "criSocket" with value "/run/containerd/containerd.sock". Please update your configuration!
[init] Using Kubernetes version: v1.30.2
[preflight] Running pre-flight checks
[preflight] Pulling images required for setting up a Kubernetes cluster
[preflight] This might take a minute or two, depending on the speed of your internet connection
[preflight] You can also perform this action in beforehand using 'kubeadm config images pull'
W0712 10:21:47.877835   14940 checks.go:844] detected that the sandbox image "registry.k8s.io/pause:3.8" of the container runtime is inconsistent with that used by kubeadm.It is recommended to use "registry.k8s.io/pause:3.9" as the CRI sandbox image.
[certs] Using certificateDir folder "/etc/kubernetes/pki"
[certs] Generating "ca" certificate and key
[certs] Generating "apiserver" certificate and key
[certs] apiserver serving cert is signed for DNS names [kubeadm1n kubernetes kubernetes.default kubernetes.default.svc kubernetes.default.svc.cluster.local] and IPs [10.96.0.1 192.168.0.222]
[certs] Generating "apiserver-kubelet-client" certificate and key
[certs] Generating "front-proxy-ca" certificate and key
[certs] Generating "front-proxy-client" certificate and key
[certs] Generating "etcd/ca" certificate and key
[certs] Generating "etcd/server" certificate and key
[certs] etcd/server serving cert is signed for DNS names [kubeadm1n localhost] and IPs [192.168.0.222 127.0.0.1 ::1]
[certs] Generating "etcd/peer" certificate and key
[certs] etcd/peer serving cert is signed for DNS names [kubeadm1n localhost] and IPs [192.168.0.222 127.0.0.1 ::1]
[certs] Generating "etcd/healthcheck-client" certificate and key
[certs] Generating "apiserver-etcd-client" certificate and key
[certs] Generating "sa" key and public key
[kubeconfig] Using kubeconfig folder "/etc/kubernetes"
[kubeconfig] Writing "admin.conf" kubeconfig file
[kubeconfig] Writing "super-admin.conf" kubeconfig file
[kubeconfig] Writing "kubelet.conf" kubeconfig file
[kubeconfig] Writing "controller-manager.conf" kubeconfig file
[kubeconfig] Writing "scheduler.conf" kubeconfig file
[etcd] Creating static Pod manifest for local etcd in "/etc/kubernetes/manifests"
[control-plane] Using manifest folder "/etc/kubernetes/manifests"
[control-plane] Creating static Pod manifest for "kube-apiserver"
[control-plane] Creating static Pod manifest for "kube-controller-manager"
[control-plane] Creating static Pod manifest for "kube-scheduler"
[kubelet-start] Writing kubelet environment file with flags to file "/var/lib/kubelet/kubeadm-flags.env"
[kubelet-start] Writing kubelet configuration to file "/var/lib/kubelet/config.yaml"
[kubelet-start] Starting the kubelet
[wait-control-plane] Waiting for the kubelet to boot up the control plane as static Pods from directory "/etc/kubernetes/manifests"
[kubelet-check] Waiting for a healthy kubelet. This can take up to 4m0s
[kubelet-check] The kubelet is healthy after 503.07131ms
[api-check] Waiting for a healthy API server. This can take up to 4m0s
[api-check] The API server is healthy after 9.504487982s
[upload-config] Storing the configuration used in ConfigMap "kubeadm-config" in the "kube-system" Namespace
[kubelet] Creating a ConfigMap "kubelet-config" in namespace kube-system with the configuration for the kubelets in the cluster
[upload-certs] Skipping phase. Please see --upload-certs
[mark-control-plane] Marking the node kubeadm1n as control-plane by adding the labels: [node-role.kubernetes.io/control-plane node.kubernetes.io/exclude-from-external-load-balancers]
[mark-control-plane] Marking the node kubeadm1n as control-plane by adding the taints [node-role.kubernetes.io/control-plane:NoSchedule]
[bootstrap-token] Using token: bqflzv.nbb753ji2xawnm6q
[bootstrap-token] Configuring bootstrap tokens, cluster-info ConfigMap, RBAC Roles
[bootstrap-token] Configured RBAC rules to allow Node Bootstrap tokens to get nodes
[bootstrap-token] Configured RBAC rules to allow Node Bootstrap tokens to post CSRs in order for nodes to get long term certificate credentials
[bootstrap-token] Configured RBAC rules to allow the csrapprover controller automatically approve CSRs from a Node Bootstrap Token
[bootstrap-token] Configured RBAC rules to allow certificate rotation for all node client certificates in the cluster
[bootstrap-token] Creating the "cluster-info" ConfigMap in the "kube-public" namespace
[kubelet-finalize] Updating "/etc/kubernetes/kubelet.conf" to point to a rotatable kubelet client certificate and key
[addons] Applied essential addon: CoreDNS
[addons] Applied essential addon: kube-proxy
HERE in bootstrapping these node means **BOOOTSRAPPING THE CONTROL PLANE NODE**.
>IN THE CLUSTER, makes it up one node first and joins with the rest.
all the 
1) The command gets initiated
2) Images been pulled
3) Files and configurations got created
4) Setups and addons has been applied
 YET TO COMPLETE CREATING THE KUBERNETES CLUSTER SETUP. THIS IS THE OUTPUT THAT WE SUPPOSED TO END UP WITH which is: Your Kubernetes control-plane has initialized successfully!
```sh
#Your Kubernetes control-plane has initialized successfully!
#To start using your cluster, you need to run the following as a regular user:

  mkdir -p $HOME/.kube
  sudo cp -i /etc/kubernetes/admin.conf $HOME/.kube/config
  sudo chown $(id -u):$(id -g) $HOME/.kube/config

#Alternatively, if you are the root user, you can run:
   export KUBECONFIG=/etc/kubernetes/admin.conf

#You should now deploy a pod network to the cluster.Run 
kubectl apply -f [podnetwork].yaml #with one of the options listed at:
  https://kubernetes.io/docs/concepts/cluster-administration/addons/

#Then you can join any number of worker nodes by running the following on each as root:

kubeadm join 192.168.0.143:6443 --token bqflzv.nbb753ji2xawnm6q \ --discovery-token-ca-cert-hash sha256:7b897f575d86f96ac4e3571bd19  0407253184e717bdeaf80b34e6f95c794f96b
```

>You can perform the following instructions, if
  -> **Single node setup**
```sh
###To start using your cluster, you need to run the following as a regular user:
  mkdir -p $HOME/.kube
  sudo cp -i /etc/kubernetes/admin.conf $HOME/.kube/config
  sudo chown $(id -u):$(id -g) $HOME/.kube/config

#Alternatively, if you are the root user, you can run:
#skip this:   export KUBECONFIG=/etc/kubernetes/admin.conf

#You should now deploy a pod network to the cluster. For that you need to install a CNI first. We'll be using calico. After that, run 
kubectl apply -f [podnetwork].yaml #with one of the options listed at:
https://kubernetes.io/docs/concepts/cluster-administration/addons/
```
> ->  If it is a **multi-node setup**, rest of it should be completed, to join whatever Compute plane worker node to the control plane node. Have to execute the command till that. which is simply, joining the control plane with `worker nodes`
> 
> you continue with the rest of it by,
```sh
#Then you can join any number of worker nodes by running the following on each as root:
kubeadm join 192.168.0.143:6443 --token bqflzv.nbb753ji2xawnm6q \ --discovery-token-ca-cert-hash sha256:7b897f575d86f96ac4e3571bd190407253184e717bdeaf80b34e6f95c794f96b
```
and the rest of these should be followed in order to start using the cluster. till at least `kubectl apply` if it is a single node setup.
>**ignoring the last since we're setting up a single node cluster setup.**
**BOOTSTRAPPING DONE SUCCESSFULLY!**
#### To start using your cluster, you need to run the following as a regular user:
```sh
mkdir -p $HOME/.kube
sudo cp -i /etc/kubernetes/admin.conf $HOME/.kube/config
sudo chown $(id -u):$(id -g) $HOME/.kube/config
```
here, 
1) creating `.kube` directory 
> `mkdir -p $HOME/.kube`
2) under that `.kube`, gotta store a **config** file. Why? If someone wants to communicate with the kubernetes cluster, must have a **kube-config** file. Without it, can't able to communicate with the Kubernetes Cluster regardless of platforms both on-prem or cloud. 
By this time, the kubeconfig presented in default and have to move it to home directory.  the file entries will be saved under the configured file location.
> `sudo cp -i /etc/kubernetes/admin.conf $HOME/.kube/config`
3) and, change permissions to the `config` files
> `sudo chown $(id -u):$(id -g) $HOME/.kube/config`

After that, can skip `export KUBECONFIG=/etc/kubernetes/admin.conf` since we already ran our thing before in prior.

NOW, WILL INSTALL AND ENABLE ADDONS WHICH IS VITAL RELATED TO ORCHESTRATING THE CLUSTERS: -> **A CNI - Container Network Interface**. -> We are using `Calicio` - network for kubernetes CNI for our use case. Alternatively, there are cilium Istio and more does exists.

**BEFORE RUNNING THIS,** should pull - calico network for kubernetes CNI. Then will run that `kubectl apply -f podnetwork.yaml https://kubernetes.io/docs/concepts/cluster-administration/addons/`
with the CNI + Addons to complete the same.

##### INSTALLING CALICO NETWORK IN OUR KUBERNETES CLUSTER:
SEARCH **Calico Kubernetes**. -> Official Docs -> Install Calico -> Kubernetes -> QuickStart -> [QuickStart for Calico on Kubernetes](https://docs.tigera.io/calico/latest/getting-started/kubernetes/quickstart#big-picture)
Following the docs, Installing Calico CNI
Install Calico[​](https://docs.tigera.io/calico/latest/getting-started/kubernetes/quickstart#install-calico "Direct link to Install Calico")
1) Install the Tigera Calico operator and custom resource definitions.
```sh
kubectl create -f https://raw.githubusercontent.com/projectcalico/calico/v3.28.0/manifests/tigera-operator.yaml
```
make you don't expose ports on the system for prior services.
kubectl **Output:**
```sh
kubectl create -f https://raw.githubusercontent.com/projectcalico/calico/v3.28.0/manifests/tigera-operator.yaml
namespace/tigera-operator created
customresourcedefinition.apiextensions.k8s.io/bgpconfigurations.crd.projectcalico.org created
customresourcedefinition.apiextensions.k8s.io/bgpfilters.crd.projectcalico.org created
customresourcedefinition.apiextensions.k8s.io/bgppeers.crd.projectcalico.org created
customresourcedefinition.apiextensions.k8s.io/blockaffinities.crd.projectcalico.org created
customresourcedefinition.apiextensions.k8s.io/caliconodestatuses.crd.projectcalico.org created
customresourcedefinition.apiextensions.k8s.io/clusterinformations.crd.projectcalico.org created
customresourcedefinition.apiextensions.k8s.io/felixconfigurations.crd.projectcalico.org created
customresourcedefinition.apiextensions.k8s.io/globalnetworkpolicies.crd.projectcalico.org created
customresourcedefinition.apiextensions.k8s.io/globalnetworksets.crd.projectcalico.org created
customresourcedefinition.apiextensions.k8s.io/hostendpoints.crd.projectcalico.org created
customresourcedefinition.apiextensions.k8s.io/ipamblocks.crd.projectcalico.org created
customresourcedefinition.apiextensions.k8s.io/ipamconfigs.crd.projectcalico.org created
customresourcedefinition.apiextensions.k8s.io/ipamhandles.crd.projectcalico.org created
customresourcedefinition.apiextensions.k8s.io/ippools.crd.projectcalico.org created
customresourcedefinition.apiextensions.k8s.io/ipreservations.crd.projectcalico.org created
customresourcedefinition.apiextensions.k8s.io/kubecontrollersconfigurations.crd.projectcalico.org created
customresourcedefinition.apiextensions.k8s.io/networkpolicies.crd.projectcalico.org created
customresourcedefinition.apiextensions.k8s.io/networksets.crd.projectcalico.org created
customresourcedefinition.apiextensions.k8s.io/apiservers.operator.tigera.io created
customresourcedefinition.apiextensions.k8s.io/imagesets.operator.tigera.io created
customresourcedefinition.apiextensions.k8s.io/installations.operator.tigera.io created
customresourcedefinition.apiextensions.k8s.io/tigerastatuses.operator.tigera.io created
serviceaccount/tigera-operator created
clusterrole.rbac.authorization.k8s.io/tigera-operator created
clusterrolebinding.rbac.authorization.k8s.io/tigera-operator created
deployment.apps/tigera-operator created
```

Done! Secondly, this. 
>Note: but I don't want to run the same directly. Because, We made few modifications in the pod CIDR = 192.168.0.0`/16` but we change it to `/24`  . So, Cant use this since we're using different CIDR. To apply the relevant config for the same.
2) Install Calico by creating the necessary custom resource. For more information on configuration options available in this manifest, see [the installation reference](https://docs.tigera.io/calico/latest/reference/installation/api).
-- Get binaries using `wget` command. 
```sh
wget https://raw.githubusercontent.com/projectcalico/calico/v3.28.0/manifests/custom-resources.yaml 
```

-- edit `custom-resource.yaml` using `vi` or `nano`
```sh
nano custom-resource.yaml
```
and modify the CIDR by finding it in the spec section, CIDR.
	cidr:  //in docs,  `10.244.0.0/16` 
since, we've ran `kubectl init` with all the ip, container config and CIDR

Run, To apply the settings:
```kubectl
kubectl apply -f custom-resources.yaml 
```
>**Output: Tigera.io's Operator of Installation and apiserver/default got created.**

To check, Run this by watching the running nodes and pods:
```kubectl
kubectl get no #for node
kubectl get po -A #or --all-namespaces for pods
```
ALL OF THESE PODS HAS TO BE UP AND RUNNING. Few might've initiated, others might've in pending. Wait till completion. Only after completion, you'll see the node's status is ready.. ALL GOOD!

For more info about the node, run 
```kubectl
kubectl get no -o wide
```
telling the role which is obviously `control plane`, time, version, IP for the control-plane, OS, Kernel and Container runtime.

### *==**KUBERENETES ON A SINGLE NODE SETUP HAS BEEN COMPLETED SUCCESSFULLY!!!**==*
==**USING KUBEADM WITH CONTAINERD AS A CONTAINER RUNTIME AND CALICO AS CNI HAS BEEN COMPLETED SUCCESSFULLY!!!***==

**NOW, We can deploy our applications**. If you want to deploy and launch applications on these nodes, first we have to test it. Will test it by launching a pod running our applications on it.

No experimentation. such testing a real world application on a POD. 
To do that, follow
```kubectl
kubectl run test-pod --image=nginx -t -i 
```
or whatever image as per your convenience. if you run this, throws
> error: timed out waiting for the condition
```kubectl
kubectl get po
kubectl get po -o wide
```
shows: **test-pod** status is **Pending** and nothing in all ways.
**WHY?, Why waiting for a condition? How to validate to resolve the same** 
LETS TROUBLESHOOT.
```kubectl
kubeclt get pod #will be in pending status
kubectl describe po test-pod #pod-name
```
Shows the description,
in the events section, if notification shows a 
>Type: Warning, Reason: Failed Scheduling, ,Message: untolerated taint. 

The Reason behind that is **Taints**. 
>We have only one node which is control plane node. So, the control plane by default will not allow me to run the pod/application.  Our objective is to practice kubernetes deploying applications on the single node.

Here, the single node is a control plane node. Now, we need to add few metrics to make compute plane work too. So we need to make the control plane node work as compute plane in order to launch pods on the same. 

In order to perform such actions.    We need to remove taints. To remove taints, remove taints. By doing,
```kubectl
kubectl get no -o wide #kubeadm1n in my case, to get the nodename to edit
```
```kubectl
kubectl edit no kubeadm1n #nodename
```
scroll down and go to a block named **TAINTS** in spec section. 
It will have a parameter called `effect: NoSchedule` This particular metric is the reason that we couldn't able to launch the pods. Means, for trying to deploy an application it is not possible. So
>Remove the **TAINTS** block. 

> [!What is TAINTS]
> Taints and tolerations work together to ensure that pods are not scheduled onto inappropriate nodes. One or more taints are applied to a node; this marks that the node should not accept any pods that do not tolerate the taints.
> 
>  Since our Single node is a control plane node, which also have compute plane components. Now we want our control plane node to act as a compute plane node in-order run our pods in ease without any taints and tolerations. For that, we simply have to disable it.

NOW, re-check. 
```kubectl
kubectl get po -o wide
```
Will be resulting in `ContainerCreating` or `Running`. How come now? Because, we have removed the taints to launch the application.

If not, you can't be able to practice doing both compute and control on a same one single node. Since it `kubectl` is tainted.

##### Conclusion
The reason behind this is that, By default, you can't launch your application, container or an instance in a control plane node, because it is a control plane node.  How to know what node consist what containers. Here, we made it by
```kubectl
kubectl get no #check the role of it, finding the node
kubectl edit no nodename #node-name + edited the TAINT
kubectl get po -o wide #made it work by removing the taints condition parameter 
```
no, we can able to launch the applications. to get the app status, copy the IP by getting info of that pod, 
```bash
curl 10.244.101.71
```

shows, welcome to nginx homepage output:
```html
<!DOCTYPE html>
<html>
<head>
<title>Welcome to nginx!</title>
<style>
html { color-scheme: light dark; }
body { width: 35em; margin: 0 auto;
font-family: Tahoma, Verdana, Arial, sans-serif; }
</style>
</head>
<body>
<h1>Welcome to nginx!</h1>
<p>If you see this page, the nginx web server is successfully installed and
working. Further configuration is required.</p>

<p>For online documentation and support please refer to
<a href="http://nginx.org/">nginx.org</a>.<br/>
Commercial support is available at
<a href="http://nginx.com/">nginx.com</a>.</p>

<p><em>Thank you for using nginx.</em></p>
</body>
</html>
```

Based on this, i can confirm that i can able to launch my application on the same SETTING UP AND CONFIGURING SINGLE NODE KUBERNETES CLUSTER SETUP WITH KUBEADM WITH CONTAINERD AND CALICO.

#### CONNECTING BACK TO A K8s Cluster after restart.
No need of anything. Just run it all right away after boot. Make sure you done all the config correct as per instructed.

--- FIN ---