RC - Scalein & Scaleout - Doc

# 3) Demo: RC scaleIn and scaleout

As the second part of the following `rc`, second example. 
Will see the same with more of the demonstration of `rc` with extra more parameters namely **scaleIn and scaleOut**.
++ with do the same with **multicontainer-pod + volumes and Volume Mounts**

base template of `rc` will take this as per our convenience,
`rc-basetemplate.yaml`
```yaml
apiVersion: v1
kind: ReplicationController
metadata:
  name: nginx-rc
  labels:
    app: nginx-rc
    type: rc-prod
spec:
  replicas: 1
  selector:
    app: nginx-rc
  template:
    metadata:
      labels:
        app: nginx-rc
```

For the `multicontainer-rc.yaml`, as we saw previously by launching a write container + a server container. here in this manifest:
- same base template here,
- more replicas this time, `replica: 3`
- multiple labels to the template for POD identification. Why? since we do three replicas, all will get assigned with this labels.
here is the `multicontainer-rc.yaml` manifest:
```yaml
apiVersion: v1
kind: ReplicationController
metadata:
  name: nginx-rc
spec:
  replicas: 3
  selector:
    app: nginx-multicontainer-rc
  template:
    metadata:
      labels:
        app: nginx-multicontainer-rc
        env: prod
        release: v1.0
    spec:
      containers:
      - name: write-app
        image: alpine
        command: ["/bin/sh"]
        args: ["-c", "while true; do date >> /var/log/index.html; sleep 10; done"]
        volumeMounts:
        - name: rc-shared-volume
          mountPath: /var/log

      - name: server-app
        image: nginx:latest
        ports:
        - containerPort: 80
        volumeMounts:
        - name: rc-shared-volume
          mountPath: /usr/share/nginx/html

      volumes:
      - name: rc-shared-volume
        emptyDir: {}
```

to verify:
```sh
kubectl get rc -o wide
kubectl get po -o wide
```

if u notice the NODEs, the replication does its thing by deploying the PODs on the available worker nodes + to see the labels of all those PODs - shows labels.
```sh
kubectl get po -o wide --show-labels
```
to verify the PODs are running,
```sh
curl allthreeIPs
```

###### To the main topic - ScaleIn and ScaleOut:
##### using Imperative Approach: (good for admin to automate the scaling - automation) + !recommended
How to scaleIn and scaleOut in this establishment?
```sh
kubectl get rc -o wide
```
here, we have deployed 3PODs under our `rc`. If we ended experiencing too much load in the application and insufficient to handle the request. will do **scaleIn and scaleOut**,

There are two types of scaling,
- Horizontal scaling - increasing the number of the resources PODs
- Vertical scaling - increasing the number of capacity of the system

Here, will perform horizontal scaleIn and Out since we can do that with K8s but not the vertical because it is system oriented to scale the same. 
++
**we have to perform the same by ==Imperative Approach== since all these workloads are Up and Running,** and its simple
```sh
kubectl scale --replicas=5 rc/rcName
```
and check the same by
```sh
kubectl get rc -o wide
kubectl get po -o wide
```
to verify the PODs are running,
```sh
curl allthreeIPs
```

To scaleDown
```sh
kubectl scale --replicas=2 rc/rcName
```
```sh
kubectl get po -o wide
```
PODs getting terminated can be seen. -> Based on the `Number of PODs` first running on the node and then the `Age` of the POD to evenly distribute the traffic. 

> This is **`RC` scaleIn and scaleout**
 
##### using Declarative Approach: (good for admin to edit on fly - manual)
means we are trying to tap into the manifest and edit the Number of replicas count for the **`RC` scaleIn and scaleout**.
```sh
kubectl edit rc/rcNAME
```
and edit `replica=n`. save and exit and it does its thing!. The desired state will be changed and voila.

##### Deleting Pods without deleting `rc`:
simply number the `replica=0`,
```sh
kubectl scale --replicas=0 rc/rcName
```
PODs will be deleted but the `rc` will exist!.
to restore the same, `replica=n` and respawns

> [!NOTE]  ##### FINAL file - demo manifest of a multicontainer `replicationcontroller`.  RC - MANIFEST
> ###### multicontainer-rc.yaml.yaml
> ```yaml
> apiVersion: v1
kind: ReplicationController
metadata:
  name: nginx-rc
spec:
  replicas: 3
  selector:
    app: nginx-multicontainer-rc
  template:
    metadata:
      labels:
        app: nginx-multicontainer-rc
        env: prod
        release: v1.0
    spec:
      containers:
      - name: write-app
        image: alpine
        command: ["/bin/sh"]
        args: ["-c", "while true; do date >> /var/log/index.html; sleep 10; done"]
        volumeMounts:
        - name: rc-shared-volume
          mountPath: /var/log
      - name: server-app
        image: nginx:latest
        ports:
        - containerPort: 80
        volumeMounts:
        - name: rc-shared-volume
          mountPath: /usr/share/nginx/html
      volumes:
      - name: rc-shared-volume
        emptyDir: {}

