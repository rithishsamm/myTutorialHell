##### Section 5: K8S Replication Controller
1.  Introduction to Replication Controller(`rc`)
		Intro `rc` - [Doc](obsidian://open?vault=tutorialHell&file=O`rc`hestration%2Fk8engineers.com%2FKubernetes-Deep-Dive%2Foffl-raw-docs%2Fkubernetes%20deepdive%2Fsection5-replication-controller%2F1.%20K8s%20Workloads%20-%20Intro%20`rc`.docx)
2.  Implement `rc` using declarative approach
		`rc` using declarative approach - [Doc](obsidian://open?vault=tutorialHell&file=O`rc`hestration%2Fk8engineers.com%2FKubernetes-Deep-Dive%2Foffl-raw-docs%2Fkubernetes%20deepdive%2Fsection5-replication-controller%2F2.%20`rc`%20using%20declarative%20approach.docx)
3.  `rc` scale in and scale out
		`rc` - Scale in & Scale out - [Doc](obsidian://open?vault=tutorialHell&file=O`rc`hestration%2Fk8engineers.com%2FKubernetes-Deep-Dive%2Foffl-raw-docs%2Fkubernetes%20deepdive%2Fsection5-replication-controller%2F3.%20`rc`%20-%20Scalein%20%26%20Scaleout.docx)
4.  `rc` and POD label selector
		`rc` & POD label selector -[Doc](obsidian://open?vault=tutorialHell&file=O`rc`hestration%2Fk8engineers.com%2FKubernetes-Deep-Dive%2Foffl-raw-docs%2Fkubernetes%20deepdive%2Fsection5-replication-controller%2F4.%20`rc`%20%26%20POD%20label%20selector.docx)
5.  How to delete `rc` using cascade option
		`rc` using Cascade option -[Doc](obsidian://open?vault=tutorialHell&file=O`rc`hestration%2Fk8engineers.com%2FKubernetes-Deep-Dive%2Foffl-raw-docs%2Fkubernetes%20deepdive%2Fsection5-replication-controller%2F5.%20`rc`%20using%20Cascade%20option.docx)
***
##### Section 5: K8S Replication Controller
#### 1.  Introduction to Replication Controller(`rc`)
###### Intro `rc` - [Doc](obsidian://open?vault=tutorialHell&file=O`rc`hestration%2Fk8engineers.com%2FKubernetes-Deep-Dive%2Foffl-raw-docs%2Fkubernetes%20deepdive%2Fsection5-replication-controller%2F1.%20K8s%20Workloads%20-%20Intro%20`rc`.docx)

So far we've covered about PODs and Workloads resou`rc`es:
Pods
- **PODs** are just a logical layer to  manages and runs containers in ease to group one or more containers to have common network and shared storage.
- **Challenges with these PODs**: Very volatile if any critical applications deployed on PODs lying on a unreliable node. If node gets down and restored back up, will be washed and nothing will be there. 
- to evade this issue, comes a concept name `replication` and there are components that exists based on this concept. 
- **Workload Resou`rc`es Components** such as,
 1) ``rc`` **ReplicationController** - *basic resou`rc`e to run multiple pods*
2) `rs` **ReplicaSet** -  alternate/ upgraded version of `ReplicationController` (except `selectors` which selects labels)
 3) `deploy` **Deployment** - uses `replicaset`  in backend to create replicas for scale in and out + rollback and rollout versions upgrade and downgrade, good for production environment. 
 4) `cron` **CronJobs** - run a specific script/job/program/process as jobs but run it on a specific time.
 5) `ds`**DaemonSet** - running mandatory PODs on each and every node in the cluster, for monitoring and stuff.
6) `sts` **Statefulset** - is to run stateful apps where above seen are stateless
 7) **Jobs** - run a specific script/job/program/process 
 8) (followed by Jobs) -> **Auto Clean-up** for finished jobs - Workspace cleanup
 
- Which ensures factors like,
1) **reliability** - by design, replication gets done by maintaining desired and actual state in equal to have things in multiples, ensures reliability  if anything goes down, there is always something for play the role means PODs which gets to exist on multiple nodes.
2) **load balancing** - since the PODs exist on multiple nodes. loads gets distributed even to each nodes and to serve content evenly from PODs to run in ease.
3) **scaling** -  can do scale in, scale out manual and automatic with horizontal scaling by expanding the spec and workload resou`rc`e of the POD and vertical scaling to increasing the number of PODs.

As we saw all things **Workload Resou`rc`es**, From here, we will be starting to see what each of the workload is all about. Starting form Replication Controller.

---

**K8s Workload Resou`rc`e: Introduction to Replication Controller ``rc``:**
So far continuing with the K8s Workload Resou`rc`es, Will start covering all of these one by one. Starting with **Replication Controller (`rc`)**

###### Context and Brief:
1. **It makes sure specified number of PODs (workloads) are running at any given point of time on the cluster.**
e.g: I want to run an application, `nginx`, `jenkins`, `apache` or whatever. 
> If you'd run `n` number of replicas in the backend, use `rc`. 
> If any one of the POD goes down, there will be no downtime since we run multiple number of PODs on the respective nodes. 
(not possible with Standalone POD)


2. **`Replication Controller` will make sure only specified number of PODs are running and will remove/add if it founds any mismatch.**
eg: maintains desired and actual state.
if i declare `replica=3`, 
deletes if PODs are more than 3,
creates if PODs are less than 3.
to match the desired state with the actual state.
> **This make it possible by  `kube-controller-manager`**


3. **`apiVersion`  for the ``rc`` object is `v1`**
`rc.yaml`
by writing a manifest for `rc`'s, for Declarative Approach: 
```
apiversion=v1
```


4. **Workflow: `Replication Controller` -> POD**
So far and previously, we have been creating PODs manually which are just simple standalone PODs. Created Manually all by yourself. 

Here, The POD created process is taken care by `rc`. Created, Managed and Controlled by the workload resources itself.
`rc` -> *creates* -> PODs. 


That simply it for the `scalabilty` as a factor.

5. **Its recommend to deploy application on K8s  with minimum workload resou`rc`e `ReplicationController`, instead of POD.**
simple. just deploy deploy mission critical real time applications with  here it's `rc` but based on any workload resource with the`replication` concept. 
> Here in `rc` , you can give `replica=1`.



 as we discuss earlier, the subject tells the same. for the `replication` factor. 
> `Replication` factor where we can scale in and out declaring actual and desired state. 



6. **We need to use `ReplicaSet`, instead of `ReplicationController` as `Deployment` Object uses `ReplicaSet`.** 


On the other side, we need to use **`ReplicaSet`**  instead of `replication-controller` **as we use Deployment as Object** which uses ReplicaSet to create such. We should use if `replication` matters on a larger scale.  both are moreover same either in terms of general functionalities or `replication`. BUT WHY? `replicaset` have something called `selector` which selects labels. Way of working with labels by defining and selecting is different. 

7. **``rc`` will use pod template to create POD with same specifications based on replica count (`.spec.replicas=1`)**



where that itself is a feature that ``rc`` does which replicates also the specifications of PODs based on replica count. if nothing specified under the `spec` section, the default is `.spec.replicas=1`. If need more than one, should define the parameter in the `spec` section `.spec.replicas=1`. 
OK? what is ***pod template***? - it is like **Launch Template** in AWS.
> Every POD will get created based on that template.
> -  how a pod has to be created, all the specs and stuff. 
> - how many containers to create 
> - naming such containers
> - all the images for such containers
> - ports to be exposed for such containers
> - volumes for that workloads
> - and volumeMounts to be mounted on such containers and more




8. **ScaleIn and Scaleout of replicas are done using** 
```
kubectl scale --replicas=n `rc`/`rc`Name 
```



9. **Only a `.spec.template.spec.restartPolicy` equal to *Always* is allowed, which is the default if not specified.**


Restart policy in ``rc`` is as same as the rest which is always, onFailure and never to be written on POD template not under spec section of the ``rc``,
-- `.spec.template.spec.restartPolicy = always`


10. **Pod Selector,  `.spec.template.metadata.labels` should be equal to `.spec.selector` to handle the PODs by `ReplicationController`**


	Since all of our PODs are getting created by `replication-controller`, how it identifies a POD?  
PODs will be created with the labels all defined and that will be the probable identification of the POD for the ``rc``. That's how to identify a POD using labels. 

How to define a label for a POD in template - `spec.template.sepc.restartPolicy`
How to select a POD with labels. - `spec.selector`.
HOW TO ASSIGN A LABEL FOR A POD?
```
spec: 
  template:
    metadata: 
      - labels
```
should selectable, identify and controlled for ``rc``. 




11. **If `.spec.selector` is undefined, by default it will set to `.spec.template,metadata.labels`**


If no `.spec.selector is defined`, what are all the labels has been defined in the `.spec.template.metadata.labels` will be taken in action for POD identification - labels.


12. **In order to delete POD managed under `rc`, delete `rc` not the POD. Nothing gets deleted by deleting PODs.**
```
kubectl delete `rc` `rc`Name
```
or to scale in by restricting the replica count.



12. **In order to delete POD managed under ``rc``, delete ``rc`` not the POD. Nothing gets deleted by deleting PODs.** 



13. **We can delete `rc` without deleting PODs managed by `rc`** 
```
kubectl delete --cascade=orphan
```


if you execute this with ``rc`/`rc`name`, all the PODs will be available, but not the ``rc``.
> WHY? can create new `replication-controller` using the same old pods. if any misconfiguration or forget to add a parameter and has been spinning up there for a while, where you can delete ``rc`` and recreate but not the pods, edit and relaunch the same.



14. **PODs controlled by `rc` can be isolated by changing the labels (to troubleshoot by moving out of the service LoadBalancer)**


 PODs which are controlled by ``rc`` can be isolated by changing the labels. 
 eg: ``rc`` setup that exist which them PODs can be identified with labels. On the other side where 
 - I'll be creating standalone PODs and declaring the same labels, THAT WILL ALSO BE CONTROLLED BY ``rc`` too. 
- Meanwhile, If I be removing a label of a POD which is controlled by ``rc``, the POD gets isolated and no longer will be controlled by ``rc``
 SO, SHOULD BE CAREFUL WITH DEFINING LABELS. These cases will be helpful for troubleshooting the PODs, Containers, Objects, Batch, Components and more. 

where you can isolate them out of ``rc`` by removing the label. After all the process gets done. then, you can add the labels which will get controlled and maintained under ``rc`` too. 



***
#### 2.  Implement `rc` using declarative approach
###### `rc` using declarative approach - [Doc](obsidian://open?vault=tutorialHell&file=O`rc`hestration%2Fk8engineers.com%2FKubernetes-Deep-Dive%2Foffl-raw-docs%2Fkubernetes%20deepdive%2Fsection5-replication-controller%2F2.%20`rc`%20using%20declarative%20approach.docx)

check the setup,
```
kubectl get no -o wide
```

> Implementing ``rc`` using declarative approach: Demonstration, 
>Short note: Commonly, we do work compute plane and control plane as two different things. But for some cases like lack of resou`rc`es and stuff. 
>Try working out with single node. To deploy things on single node,
```
kubectl edit node controlplane23 
```
> and remove 
```
> taints:
>   -effect: NoSchedule
>    key: node-role.kubernetes.io/control-plane
 ```
> and works just fine. 

BUT Good to have Control and Worker node as separate machines. LETS CREATE PODS USING ``rc`` using DECLARATIVE APPROACH:

Before passing manifests, will verify the naming conventions:
```
kubectl api-resou`rc`es | grep replicationcontroller
```
```
OUTPUT:
  NAME: replicationcontrollers
  SHORT: `rc`                             
  VERSION: v1
  true
  KIND: ReplicationController
```

file: **`testnginx-`rc`.yaml**
```yaml
apiVersion: v1
kind: ReplicationController
metadata:
  name: hexrapp
  labels:
    app: hexrapp
    type: `rc`-prod
spec:
  replicas: 3
  selector:
    app: hexrapp
  template:
    metadata:
      name: hexrapp
      labels:
        app: hexrapp
    spec:
      containers:
      - name: hexrapp
        image: nginx
        ports:
        - containerPort: 80
        resou`rc`es:
          limits:
            cpu: "0.5"
            memory: 200Mi
```
also, we can apply requests too, but that's for the another day.

boilerplate:
`apiVersion`: v1, as we all know. defining **version** of an object
`kind`: ReplicationController, defining what **kind** of the object
`metadata`: id
  `name`: hexrapp - name of the `rc`
  `labels`: - 
    `app`: hexrapp - name as label
    `type`: `rc`-prod - types as label

main context: specifying the specification of ``rc``, in ``rc`` or any other object to the matter of fact is not going to be equivalent. eg for ``rc`` we will be not defining container like we do for PODs. takes a different approach that aligns more with the `replication` factor just like we previously discussed and came across,
`spec`: 
  `replicas`: n - defining replicas. nothing but creating pods under ``rc`` for taking care of the **replication** factor. (if nothing defined in replicas, default is 1)
  `selector`: as we discussed earlier, this selector picks based on the labels of the pods defined in the manifests. THIS IS THE SELECTOR THAT THE ``rc`` is gonna use to identify the pods.
    `app`: hexrapp-`rc` - lets say that there is a pod defined under this label on the manifest's metadata.
   `template`: - **POD Template**, (all the pod that gets created by ``rc`` has to get created with the same template)
    `metadata`: - metadata under that template. (note: name of the selector and the app defined in the metadata must be the same with the **selector**) only then,  ``rc`` can identify PODs based on the label. if mismatch, ``rc`` cannot handle the pods)
```json
		  `name`: hexrapp
		  `labels`:
		    `app`: hexrapp-`rc`  - this must be identical to the `selector` defined above
	`spec`: - now we're talking. this `spec` is the container's spec. (off to metadata)
	    `containers`:
	       - `name`: hexrapp
	         `image`: nginx:lts (check docker hub for any image to pull the same.)
	         `ports`:
	          - `containerPort`: 80
	          `resou`rc`es`:
	            `limits`:
	             `cpu`: "0,2"
	              'image': nginx
```

so the boilerplate for the ``rc`` is 
```yaml
spec:
  replicas: 3
  selector:
    app: hexrapp
  template:
    metadata:
      name: hexrapp
      labels:
        app: hexrapp
```

NOW, CREATE `REPLCATION-CONTROLLER`;
```
kubectl create -f `rc`ManifestFile.yaml
```
check ``rc`` by
```
kubectl get `rc`
kubectl get `rc` -o wide
```
```
OUTPUT:
NAME      hexrapp  -name of the `rc`
DESIRED   3  - how many pods to be created 
CURRENT   3   - how many got created
READY   3  - how many that got the containers ready to be used 
AGE   8m   - age
CONTAINERS   hexrapp   - container (we have only one)
IMAGES   nginx   - images used
SELECTOR    app=hexrapp    - selector of this `rc` to identify these pods
```
The REPLICATION CONTROLLER HAS CREATED THE POD. 
```
kubectl get po -o wide
```
```
NAME            READY   STATUS    RESTARTS      AGE   IP              NODE           NOMINATED NODE   READINESS GATES
hexrapp-rqdd5   1/1     Running   1 (36m ago)   92m   10.244.188.15   workernode24   <none>           <none>
hexrapp-wlfjc   1/1     Running   1 (36m ago)   92m   10.244.188.16   workernode24   <none>           <none>
hexrapp-wx7l8   1/1     Running   1 (36m ago)   92m   10.244.188.17   workernode24   <none>           <none>
```
> IMPORTANT FACTOR TO MIND is the name of the POD. 
**prefix is the name of the pod + hyphen + some random characters.**

This way, the `pods` under the ``rc`` can be seen. but cannot be managed by pods standalone but by ``rc``.

Lets try handling those pods via standalone pods:
```
kubectl get po -o wide
kubectl delete po podName
kubectl get po -o wide
```
Won't get deleted where the ``rc`` is managing that pod and due to replication rules that we applied, it will recreate the POD. where  it maintains the desired (no of pods) state.

If anything gets crashed or deleted, PODs get created based on the same POD Template with given spec. 

Now will try deleting that ``rc``. to clean up the resou`rc`e which is ``rc`` which deletes the pod. 
```
kubectl delete `rc` `rc`Name
```
```
kubectl get `rc` -o wide
kubectl get po -o wide
```
either no resou`rc`e or will be getting terminated.

THAT's pretty much it about the ``rc``'s overview
***
#### 3.  `rc` scale in and scale out
###### `rc` - Scale in & Scale out - [Doc](obsidian://open?vault=tutorialHell&file=O`rc`hestration%2Fk8engineers.com%2FKubernetes-Deep-Dive%2Foffl-raw-docs%2Fkubernetes%20deepdive%2Fsection5-replication-controller%2F3.%20`rc`%20-%20Scalein%20%26%20Scaleout.docx)

**Simply adding and reducing more pods on replicas for the scaling in and out factor:**
###### `rc` Scale in and out - Demonstration:

Nothing fancy. Extra numbers on the `replicas` parameter and multiple labels for each container on the POD template.

app-scaleinoutcontainer.yaml
```yaml
apiVersion: v1
kind: ReplicationController
metadata:
  name: hexrapp
  labels:
    app: hexrapp
    type: `rc`-prod
spec:
  replicas: 3
  selector:
    app: hexrapp
  template:
    metadata:
      name: hexrapp
      labels: #extra labels
        app: hexrapp
        env: prod
        release: stable
    spec:      
      containers:
      - name: hexrapp-server
        image: alpine #os image
        command: ["/bin/sh"] #assessing shell
        args: ["-c", "while true; do date >> /var/log/index.html; sleep 10;done"] #args passing a command doing an action
        volumeMounts:
        - name: shared-data
          mountPath: /var/log
      - name: hexrapp-web
        image: nginx #to serve the arg
        ports:
        - containerPort: 80
        volumeMounts:
        - name: shared-data
          mountPath: /usr/share/nginx/html #getting that data fetch from the mount
      volumes:
      - name: shared-data
        emptyDir: {}
```

If any mistakes get to occur in the yaml file, the `apiserver` won't accept the manifest where it will go through **Authentication, Authorization and Schema Validation** which will not get executed and the resou`rc`e wont get created here.

For the scale in, scale out, we gave 
- spec: replicas + selector + pod Template Metadata
- a multi-container manifest
- one being generating data + one serving it on a port = mounting same on the emptyDir: {} Volume Mount.

to create ``rc``,
```
kubectl create -f nginx-scaleinoutContainer.yaml
```
to see or checking resou`rc`es,
```
kubectl get `rc`
kubectl get `rc` -o wide
kubectl get po
kubectl get po -o wide
```
to debug or troubleshoot in brief:
```
kubectl describe `rc` podName
kubectl events `rc` podName
```

**LABELS AND SELECTORS:**
to see all the labels of each, 
```
kubectl get po -o wide --show-labels
```
shows labels which are assigned to the pod. These are the labels that the selectors will be using to identify the pods. If it gets mismatched, the ``rc``'s selector will not identify these pods. 

**SCALE IN AND OUT the ``rc``:**


##### SCALE UP:
###### Imperative approach:
For instance, we have a cluster with three pods. under the ``rc``. **The application meets a spike and couldn't control the load**. Would like to scale out to 5replicas. 
> Simply we can just put out a replica number as 5 in the manifest but will try to do the same by using **Imperative Approach** for specific reasons. 

Will see how simple it is, (the number gets changed, not the container that gets add-on to this)
**(HIGHLY RECOMMENDED METHOD)** works while in the automation part when using Jenkins and Ansible.
```
kubectl get po -o wide
kubectl get `rc` -o wide
kubectl scale --replicas=5 `rc` hexrapp
kubectl scale --replicas=5 `rc`/hexrapp
```
check whether the scaled application is working:
```
curl podIP
```

###### Declarative approach:
not feasible in a automation but works when it is in operational or administrative level:
```
kubectl edit `rc` hexrapp 
kubectl edit `rc`/hexrapp 
```
and edit `replica` parameter. change values, save and exit.
**The action will get to start right away.**
###### SCALE DOWN:
just simply reduce the numbers,
```
kubectl scale --replicas=2 `rc` hexrapp
kubectl scale --replicas=2 `rc`/hexrapp
```

How does it know or On what basis that which pod to terminate and which one to keep it, THE YOUNGEST OF EM ALL. Deletes Pod based on the age and also sees that the number of PODs running on a particular node.

THAT's all for Scale in and out using **IMPERATIVE APPROACH**

Similar to the previous,
```
kubectl edit `rc` hexrapp 
kubectl edit `rc`/hexrapp 
```
and edit `replica` parameter. change values, save and exit.
**The action will get to start right away.**

WITHOUT DELETING ``rc`` CAN WE DELETED JUST THE `PODs` ALONE?
Yes. How? same scale in and out but the values is 0.
```
kubectl scale --replicas=0 `rc` hexrapp
kubectl scale --replicas=0 `rc`/hexrapp
```
check the same.
```
kubectl get `rc` -o wide
kubectl get po -o wide
```
``rc`` will exist but `PODs` doesn't. 

***
#### 4.  `rc` and POD label selector
###### `rc` & POD label selector -[Doc](obsidian://open?vault=tutorialHell&file=O`rc`hestration%2Fk8engineers.com%2FKubernetes-Deep-Dive%2Foffl-raw-docs%2Fkubernetes%20deepdive%2Fsection5-replication-controller%2F4.%20`rc`%20%26%20POD%20label%20selector.docx)

**``rc`` and `POD` label selector: Demonstration** (MATCH LABELS)
Following to Identify stuff using labels. 

WHAT ARE **MATCHLABELS** FOR ``rc`` FOLLOWING BY **MATCHEXPRESSIONS**?
*while writing ``rc`` manifests, if no labels written in the ``rc`` spec section, the labels that have declared in the POD template will be taken. AS A SELECTOR*

eg:
```
spec:
  replicas: 3
  # selector:
  #   matchLabels:
  #     app: hexrapp
  #     release: stable
  template:
    metadata:
      name: hexrapp
      labels:
        app: hexrapp
        env: prod
        release: stable
```
```
kubectl create -f replicaset `rc`Name
kubectl create -f replicaset/`rc`Name
```
THROWS, 
replicaset invalid, spec-selector: requires value, spec.template.metadata.labels: invalid
```
kubectl get `rc` -o wide --show-labels
```


#### Skills:

##### Tech:
Technical presentation,  Networks and Infrastructure,  Programming, SDLC, Software 0's and 1's, Tech Stacks, Uses, Pros and Cons WHAT/ WHY/ HISTORY of technologies, R&D, Apps, Platforms and A`rc`hitecture,  Operations, Cost management, Breaking down things + Apps and Business Metrics (BI - Data analysis) [ DAU, WAU, MAU ], Health Success and Failure Progress, OSS, Diagrams,  Data visualization (Querying - SQL, NoSQL) Keep up with Tech and Continuous learning.
 1) Avoid Unicorn thought- riding trend waves which are irrelevant to the context
2) Being Opinionated on technologies. Experts and engineers know whats best. 
##### Biz:
- General knowledge: Definition of
Revenue, Cost, Profit (Revenue- Cost), EBITDA (Earnings Before Interest, Tax, Depreciation and Amortization) - Tax, Interests, Non physical and Physical Expenses. Incomes, Earnings,  Expenses - What we own, what we owe.
- Revenue Models, streams and Pricing Strategy- Freemium, Advertising, Pay to play, Microtransactions, Subscriptions etc.
- Competitor's success and failures
- Emotional Intelligence,  Empathy, Governance without authority 
- Storytelling,  Presentation, Writing, Description, Diagramming a purposeful problem-solution, launch and landing
- Leadership 
- GTM and Positioning
- Opinionated, assign, backing off and leave the rest to experts such as legals, ads, finances etc

##### UX (Design):
1) Tools: Sketch, Figma, Draw.io
2) Elements and Components: Buttons, Menu's, Labels, Text box, forms and more
3) Concepts: User Centric, Data driven, Journey, Affordance, critically, admin and user ( authorization, dashboard, console)
4) Questioning:
















***
#### 5.  How to delete `rc` using cascade option 
###### `rc` using Cascade option -[Doc](obsidian://open?vault=tutorialHell&file=O`rc`hestration%2Fk8engineers.com%2FKubernetes-Deep-Dive%2Foffl-raw-docs%2Fkubernetes%20deepdive%2Fsection5-replication-controller%2F5.%20`rc`%20using%20Cascade%20option.docx)



















***
