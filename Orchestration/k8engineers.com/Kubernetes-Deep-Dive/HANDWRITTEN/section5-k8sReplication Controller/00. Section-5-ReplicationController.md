##### Section 5: K8S Replication Controller
1.  Introduction to Replication Controller(rc)
		Intro rc - [Doc](obsidian://open?vault=tutorialHell&file=Orchestration%2Fk8engineers.com%2FKubernetes-Deep-Dive%2Foffl-raw-docs%2Fkubernetes%20deepdive%2Fsection5-replication-controller%2F1.%20K8s%20Workloads%20-%20Intro%20rc.docx)
2.  Implement rc using declarative approach
		rc using declarative approach - [Doc](obsidian://open?vault=tutorialHell&file=Orchestration%2Fk8engineers.com%2FKubernetes-Deep-Dive%2Foffl-raw-docs%2Fkubernetes%20deepdive%2Fsection5-replication-controller%2F2.%20rc%20using%20declarative%20approach.docx)
3.  rc scale in and scale out
		rc - Scale in & Scale out - [Doc](obsidian://open?vault=tutorialHell&file=Orchestration%2Fk8engineers.com%2FKubernetes-Deep-Dive%2Foffl-raw-docs%2Fkubernetes%20deepdive%2Fsection5-replication-controller%2F3.%20rc%20-%20Scalein%20%26%20Scaleout.docx)
4.  rc and POD label selector
		rc & POD label selector -[Doc](obsidian://open?vault=tutorialHell&file=Orchestration%2Fk8engineers.com%2FKubernetes-Deep-Dive%2Foffl-raw-docs%2Fkubernetes%20deepdive%2Fsection5-replication-controller%2F4.%20rc%20%26%20POD%20label%20selector.docx)
5.  How to delete rc using cascade option
		rc using Cascade option -[Doc](obsidian://open?vault=tutorialHell&file=Orchestration%2Fk8engineers.com%2FKubernetes-Deep-Dive%2Foffl-raw-docs%2Fkubernetes%20deepdive%2Fsection5-replication-controller%2F5.%20rc%20using%20Cascade%20option.docx)
***

==**ReplicationController represents the configuration of a replication controller.**==
#### 1.  Introduction to Replication Controller(rc)
###### Intro rc - [Doc](obsidian://open?vault=tutorialHell&file=Orchestration%2Fk8engineers.com%2FKubernetes-Deep-Dive%2Foffl-raw-docs%2Fkubernetes%20deepdive%2Fsection5-replication-controller%2F1.%20K8s%20Workloads%20-%20Intro%20rc.docx)

**K8s Workload Resource: Introduction to Replication Controller`rc`:**
So far continuing with the K8s Workload Resources, Will start covering all of these one by one. Starting with **Replication Controller `rc`**
1. It makes sur specified number of PODs (workloads) are running at any given point of time on the cluster
eg: running an app
2. Replication Controller will make sure only specified number of PODs are running and will remove/add if it founds any mismatch
3. `apiVersion`  for the `rc` object is `v1`
4. Workflow: Replication Controller -> POD
5. Its recommend to deploy application on K8s  with minimum workload resource `ReplicationController`, instead of POD.
6. We need to use ReplicaSet, instead of ReplicationController as Deployment Object uses ReplicaSet
7. `rc` will use pod template to create POD with same specifications based on replica count (`.spec.replicas=1`)
8. Scale in and Scaleout of replicas are done using 
```sh
kubectl scale --replicas=n rc/rcName 
```
9. Only a `.spec.template.metadata.labels` should be equal to `.spec.selector` to handle the PODs by `ReplicationController`
10. Pod Selector,  `.spec.template.metadata.labels` should be equal to `.spec.selector` to handle the PODs by `ReplicationController`
11. If `.spec.selector` is undefined, default it will set to `.spec.template,metadata.labels`
12. In order to delete POD managed under rc, delete rc not the POD. Nothing gets deleted by deleting PODs.
13. We can delete rc without deleting PODs managed by rc 
```sh
kubeclt delete --cascade=orphan
```
14. PODs controlled by rc can be isolated by changing the labels (to troubleshoot by moving out of the service LoadBalancer)

###### Briefs:
###### 1. It makes sures that the specified number of PODs (workloads) are running at any given point of time on the cluster.
 eg: to run apps with `n` of replicas in the backend on different nodes, use `rc`.  

###### 2. Replication Controller will make sure only specified number of PODs are running and will remove/add if it founds any mismatch
**just like `rc` uses a concept of `REPLICATION` under the hood for its functionalities to manage PODs/Replicas.* by

Maintaining desired state, removes if exceeds, increases if anything gets down. this is performed by `kube-controller-manager`. 
No downtime will occur if any POD gets down since i have multiple PODs running on the side. *which is not possible in standalone pods.*

###### 3. **`apiVersion`  for the `rc` object is `v1`**
in declarative approach of creating `rc`, `apiVersion` for `rc=v1` rc.yaml's
```yaml
apiversion: v1
kind: ReplicationController
```

###### 4. Workflow: Replication Controller -> POD
Workflow of `rc:` So far, we've been the one who creating the workloads and PODs all on our won manually. All done by you even  the creation, provisioning and managing.

Here, `rc`  takes care of creating and managing PODs. 
always `rc` or workload resources that have `replication` nature to it are much recommended for working with PODs. 
where we can scale in and out declaring actual and desired state. 
which isn't possible in a standalone workload.

That simply it for the `scalabilty` as a factor.

###### 5. Its recommend to deploy application on K8s with minimum workload resource `ReplicationController`, instead of POD.
strictly, use the workload resources with the `replication` factor in prod. maintains desired state to keep the apps HA and replicates on multiple nodes in ease. 

###### 6. use `ReplicaSet`, instead of `ReplicationController` as `Deployment` Object uses `ReplicaSet`
On the other side, use **`ReplicaSet`**  instead of `replication-controller` **as we use `Deployment` as Object**
which uses `ReplicaSet` to spin the workloads. both are moreover same either in terms of general functionalities or `replication`. BUT WHY? `replicaset` have something called `selector` which ==**selects labels. Way of working with labels by defining and selecting is different.** ==

###### 7. `rc` will use *pod template* to create POD with same specifications based on replica count (`.spec.replicas=1`)
where that itself is a feature that `rc` does which replicates also the specifications of PODs based on replica count. if nothing specified under the `spec` section, the default is `.spec.replicas=1`. If need more than one, should define the parameter in the `spec` section `.spec.replicas=1`. 
OK? what is ***pod template***? - it is like **Launch Template** in AWS.
> Every POD will get created based on that template.
> -  how a pod has to be created, all the specs and stuff. 
> - how many containers to create 
> - naming such containers
> - all the images for such containers
> - ports to be exposed for such containers
> - volumes for that workloads
> - and volumeMounts to be mounted on such containers and more

written under the `replica` template. 

###### 8.`scaling` can be done in ease.  ScaleIn and Scaleout of replicas are done using command
```sh
kubectl scale --replicas=`n` rc/ rcName 
```
to **scale in and out**. 

###### 9. restartPolicy for `rc`
 Only a `.spec.template.spec.restartPolicy = always` is allowed and its default* if not defined,
Restart policy in `rc` is as same as the rest which is *`always`*, *`onFailure`* and never to be written on POD template not under spec section of the `rc`,
-- `.spec.template.spec.restartPolicy = always`

###### 10. Identifying PODs with **labels**: (no selector but in `rs`)
Pod Selector,  `.spec.template.metadata.labels` should be equal to `.spec.selector` to handle the PODs by `ReplicationController`.
	Since all of our PODs are getting created by `replication-controller`, how it identifies a POD?  
PODs will be created with the labels all defined and that will be the probable identification of the POD for the `rc`. That's how to identify a POD using **labels**. 

How to define a label for a POD in template - `spec.template.sepc.restartPolicy`
How to select a POD with labels. - `spec.selector`.
HOW TO ASSIGN A LABEL FOR A POD?
```yaml
spec: 
  template:
    metadata.labels: 
```
should selectable, identify and controlled for `rc`. 

###### 11.  If `.spec.selector` is undefined, default it will set to the labels given there `.spec.template,metadata.labels`
 If no `.spec.selector is defined`, what are all the labels has been defined in the `.spec.template.metadata.labels` will be taken in action for POD identification - labels.

###### 12. In order to delete POD managed under `rc`, delete `rc` not the POD. Nothing gets deleted by deleting PODs.
```sh
kubectl delete rc rcName
```
or to scale in by restricting the replica count.

###### 13) ==We can delete rc without deleting PODs managed by rc.==
is possible by passing this command, **HELPS IN SPECIAL CA**SES.
```sh
kubeclt delete --cascade=orphan
```
if you execute this with `rc/rcname`, all the PODs will be available, but not the `rc`.
> WHY? can create new `replication-controller` using the same old pods. i**f any misconfiguration or forget to add a parameter and has been spinning up there for a while, where you can delete `rc` and recreate but not the pods, edit and relaunch the same.**

###### 14. PODs controlled by `rc` can be isolated by changing the labels (to troubleshoot by moving out of the service LoadBalancer)
 PODs which are controlled by `rc` can be isolated by changing the labels. 
 eg: `rc` setup that exist which them PODs can be identified with labels. On the other side, where 
 - if I'll be creating standalone PODs and declaring the same labels, THAT WILL ALSO BE CONTROLLED BY `rc` too. 
- Meanwhile, If I be removing a label of a POD which is controlled by `rc`, the POD gets isolated and no longer will be controlled by `rc`
 SO, SHOULD BE CAREFUL WITH DEFINING LABELS. These cases will be helpful for troubleshooting the PODs, Containers, Objects, Batch, Components and more. 

where you can isolate them out of `rc` by removing the label. After all the process gets done. then, you can add the labels which will get controlled and maintained under `rc` too. 


***
# 2) Demo! Implement `rc` using declarative approach?
PRACTICAL DEMO OF DEPLOYNG THIS K8s WORKLOAD RESOUrcES IN REALTIME - `rc` ON MULTIPLE NODES CREATING **MULTI-NODE CLUSTER**:

Now, we will reflect back our setup and the systems environment that we are working with. 

Systems: 
- On-prem
- Virtual Machine environment
- Ubuntu 24.04 Platform 
1) K8s Control Plane - controlplane123 
- ipa-192.168.0.123
- 4096mb(4GB) RAM
- 4vCPU
- 25GB Storage
2)  K8s Worker Node - workernode124
- ipa-192.168.0.124
- 4096mb(4GB) RAM
- 4vCPU
- 25GB Storage

A multi-node (1control/1worker) K8s environment. 

From hereafter, we are going to cover all the same with extra more cluster. (nothing much, just 1node).

WHY? hereafter, we will be diving deep into these and to grasp the true nature of the `replication` concept. 

Nothing to worry here, we simply gonna add one more VM to spin up for - **workernode125** having one more worker node under this control plane. same setup to configure as above.  (this can be seen here in the K8s Setup Guide - [[00. section2-full-k8sClusterSetup#Kubernetes multi-node setup using kubeadm tool(CRI containerd)]]

In order to practice, upcoming K8s workload resources  such as
- replication controller `rc`
- replica set `rs`
- deployment `deploy`
- DaemonSet `ds`
- Statefulset `sts`
- jobs `jobs`
- cronjobs `cj`

So, gonna use this multi-node setup of 
- controlplane123 as Control plane
- workernode124 as worker node1
- workernode125 as worker node2

Task: 
- spin up a VM 
- get back to control plane and pass
```sh
kubeadm token create --print-join-command
```

if any queries with connecting a new node with the control plane, refer this [thread](https://stackoverflow.com/questions/51126164/how-do-i-find-the-join-command-for-kubeadm-on-the-master).


or launching PODs on `controlplane123` itself by passing 
```sh
kubectl edit node <controlplanenodename> 
```
edit to remove the **taints**. and save the changes.
>(a key value pair that says not to schedule any PODs here in this node which launched no POD here except the K8s Control Plane components.)

now, we can launch applications on the Control Plane itself. (If you don't want to, don't remove the taints and add another worker node instead.)

**BACK TO `rc`,** 
hereafter, with the concept of `replication` where we having two worker nodes and will be covering how we are launching the PODs with multiple-nodes. 

###### NOW, WILL SEE IN BRIEF ABOUT LAUNCHING `rc` using Declarative Approach?

`rc.yaml`: mostly as same as POD manifest but extra parameters for `replication` things, from `apiVersion` to `metadata` but for the `spec` of the `rc`defining 
- `replicas`,
- `selectors` to identify my pods using labels and 
- Pod `templates` - id template for the PODs are yet to get created
- `lables` !imp - the selector that we might use, has the labels to identify, first
then the `spec` with `containers` section: same as we declare for the rest:
```yaml
apiVersion: v1 #same as pod
kind: ReplicationController #object kind
metadata: 
#nothing here is gonna affect but to identify. 
  name: nginx-rc #as per convenience
  labels:
    app: nginx-rc
    type: rc-prod
spec:
  replicas: 1
  selector:
    app: nginx-rc
  template:
    metadata:
      labels:
        app: nginx-rc
    spec:
      containers:
      - name: nginx-app
        image: nginx:latest
        ports:
        - containerPort: 80
        resources:
          limits:
            memory: "300Mi"
            cpu: "0.2"
          requests:
            memory: "200Mi"
            cpu: "0.1"
```

Lets create the `rc` and verify:
```sh
kubectl create -f path/to/rc.yaml 
kubectl get rc -o wide
```
output:
```
NAME              nginx-rc 
DESIRED           1
CURRENT           1
READY             1
AGE               n mins
CONTAINERS        nginx-app
IMAGES            nginx:latest   
SELECTOR          app=nginx-rc
```
here, will understand the columns. 
we get the `replication` natures such as 
- desired state - how many you want to get created in the backend, 
- current state - how many are created, 
- ready - how many are up and running, 
- age, containers and its image
- selector - that we are using right now to identify the PODs.
 `rc` created all these. 

check the PODs,
```sh
kubectl get po -o wide
```
more or less the same. 
name, readiness, status, restarts, age, IP, node and more. 
here, in the name. -> see the name of the pod + a random suffix
> it is a standard naming convention that it follow. so, for that identification - we use selectors. 
```sh
curl IP
```
to verify the launch. 

> **`RC` is just a logical layer as same as the pod to have one or more containers under the same but for pod is just a static POD, for `rc` it is a replication functionalities to create replicas running multiple PODs in the backend.**

check the `replication` functionalities:

to clean up the resources:
```sh
kubectl get po -o wide
kubectl delete po podname
kubectl get po -o wide
```
no change, the desired state will be `1`. to delete, remove the `rc` itself no the POD.
```sh
kubectl delete rc rcName
kubectl delete -f path/to/rc.yaml
```

> [!NOTE]  ##### FINAL file - demo manifest of a simple `replicationcontroller`.  RC - MANIFEST
> ###### rc.yaml
> ```yaml
apiVersion: v1 #same as pod
kind: ReplicationController #object kind
metadata: 
#nothing here is gonna affect but to identify. 
  name: nginx-rc #as per convenience
  labels:
    app: nginx-rc
    type: rc-prod
spec:
  replicas: 1
  selector:
    app: nginx-rc
  template:
    metadata:
      labels:
        app: nginx-rc
    spec:
      containers:
      - name: nginx-app
        image: nginx:latest
        ports:
        - containerPort: 80
        resources:
          limits:
            memory: "300Mi"
            cpu: "0.2"
          requests:
            memory: "200Mi"
            cpu: "0.1"

---
# 3) Demo: RC scaleIn and scaleout

As the second part of the following `rc`, second example. 
Will see the same with more of the demonstration of `rc` with extra more parameters namely **scaleIn and scaleOut**.
++ with do the same with **multicontainer-pod + volumes and Volume Mounts**

base template of `rc` will take this as per our convenience,
`rc-basetemplate.yaml`
```yaml
apiVersion: v1
kind: ReplicationController
metadata:
  name: nginx-rc
  labels:
    app: nginx-rc
    type: rc-prod
spec:
  replicas: 1
  selector:
    app: nginx-rc
  template:
    metadata:
      labels:
        app: nginx-rc
```

For the `multicontainer-rc.yaml`, as we saw previously by launching a write container + a server container. here in this manifest:
- same base template here,
- more replicas this time, `replica: 3`
- multiple labels to the template for POD identification. Why? since we do three replicas, all will get assigned with this labels.
here is the `multicontainer-rc.yaml` manifest:
```yaml
apiVersion: v1
kind: ReplicationController
metadata:
  name: nginx-rc
spec:
  replicas: 3
  selector:
    app: nginx-multicontainer-rc
  template:
    metadata:
      labels:
        app: nginx-multicontainer-rc
        env: prod
        release: v1.0
    spec:
      containers:
      - name: write-app
        image: alpine
        command: ["/bin/sh"]
        args: ["-c", "while true; do date >> /var/log/index.html; sleep 10; done"]
        volumeMounts:
        - name: rc-shared-volume
          mountPath: /var/log

      - name: server-app
        image: nginx:latest
        ports:
        - containerPort: 80
        volumeMounts:
        - name: rc-shared-volume
          mountPath: /usr/share/nginx/html

      volumes:
      - name: rc-shared-volume
        emptyDir: {}
```

to verify:
```sh
kubectl get rc -o wide
kubectl get po -o wide
```

if u notice the NODEs, the replication does its thing by deploying the PODs on the available worker nodes + to see the labels of all those PODs - shows labels.
```sh
kubectl get po -o wide --show-labels
```
to verify the PODs are running,
```sh
curl allthreeIPs
```

###### To the main topic - ScaleIn and ScaleOut:
##### using Imperative Approach: (good for admin to automate the scaling - automation) + !recommended
How to scaleIn and scaleOut in this establishment?
```sh
kubectl get rc -o wide
```
here, we have deployed 3PODs under our `rc`. If we ended experiencing too much load in the application and insufficient to handle the request. will do **scaleIn and scaleOut**,

There are two types of scaling,
- Horizontal scaling - increasing the number of the resources PODs
- Vertical scaling - increasing the number of capacity of the system

Here, will perform horizontal scaleIn and Out since we can do that with K8s but not the vertical because it is system oriented to scale the same. 
++
**we have to perform the same by ==Imperative Approach== since all these workloads are Up and Running,** and its simple
```sh
kubectl scale --replicas=5 rc/rcName
```
and check the same by
```sh
kubectl get rc -o wide
kubectl get po -o wide
```
to verify the PODs are running,
```sh
curl allthreeIPs
```

To scaleDown
```sh
kubectl scale --replicas=2 rc/rcName
```
```sh
kubectl get po -o wide
```
PODs getting terminated can be seen. -> Based on the `Number of PODs` first running on the node and then the `Age` of the POD to evenly distribute the traffic. 

> This is **`RC` scaleIn and scaleout**
 
##### using Declarative Approach: (good for admin to edit on fly - manual)
means we are trying to tap into the manifest and edit the Number of replicas count for the **`RC` scaleIn and scaleout**.
```sh
kubectl edit rc/rcNAME
```
and edit `replica=n`. save and exit and it does its thing!. The desired state will be changed and voila.

##### Deleting Pods without deleting `rc`:
simply number the `replica=0`,
```sh
kubectl scale --replicas=0 rc/rcName
```
PODs will be deleted but the `rc` will exist!.
to restore the same, `replica=n` and respawns

> [!NOTE]  ##### FINAL file - demo manifest of a multicontainer `replicationcontroller`.  RC - MANIFEST
> ###### multicontainer-rc.yaml.yaml
> ```yaml
> apiVersion: v1
kind: ReplicationController
metadata:
  name: nginx-rc
spec:
  replicas: 3
  selector:
    app: nginx-multicontainer-rc
  template:
    metadata:
      labels:
        app: nginx-multicontainer-rc
        env: prod
        release: v1.0
    spec:
      containers:
      - name: write-app
        image: alpine
        command: ["/bin/sh"]
        args: ["-c", "while true; do date >> /var/log/index.html; sleep 10; done"]
        volumeMounts:
        - name: rc-shared-volume
          mountPath: /var/log
      - name: server-app
        image: nginx:latest
        ports:
        - containerPort: 80
        volumeMounts:
        - name: rc-shared-volume
          mountPath: /usr/share/nginx/html
      volumes:
      - name: rc-shared-volume
        emptyDir: {}


---
# 4) Demo: RS/RC & POD Label Selector (matchLabels)

Understanding `matchLabels` for ReplicaSet (NOT `rc` TO GET A BRIEF UNDERSTANDING ABOUT THE WORKINGS OF LABELS FOR `rc` AND `rs`): 

Since we saw about Labels before, if u refer the manifest:
```yaml
apiVersion: apps/v1
kind: ReplicaSet
metadata:
  name: nginx-rs
  labels:
    app: nginx-app
    env: prod
    release: v1.0
  
spec:
  replicas: 3
  # selector:
  #   matchLabels:
  #    app: nginx-rc
  #    env: prod
  #    release: v1.0
  template:
    metadata:
      name: nginx-rs
      labels:
        app: nginx-app
        env: prod
        release: v1.0
  
    spec:
      containers:
      - name: write-app
        image: alpine
        command: ["/bin/sh"]
        args: ["-c", "while true; do date >> /var/log/index.html; sleep 10; done"]
        volumeMounts:
        - name: rc-shared-volume
          mountPath: /var/log
      - name: server-app
        image: nginx:latest
        ports:
        - containerPort: 80
        volumeMounts:
        - name: rc-shared-volume
          mountPath: /usr/share/nginx/html
  
      volumes:
      - name: rc-shared-volume
        emptyDir: {}
```

here, if no `matchLabels` section declared under the spec section of `rc` (not the containers) the labels from the template section's metadata labels gets taken into consideration and get assigned to the `rc` as labels for the selector to identify:

**What happens to the `rs`?**
if we apply the manifest, error can be seen.
```
The ReplicaSet "nginx-rs" is invalid:
* spec.selector: Required value
* spec.template.metadata.labels: Invalid value: map[string]string{"app":"nginx-app", "env":"prod", "release":"v1.0"}: `selector` does not match template `labels`
```
Throwing this error. Since, we didn't write any selector, it couldn't pass the manifest and deploy the application.

>This isn't `rc` that don't mind labels. 
>Selectors matters and mandatory for `rs` for it to identify the resources. 

WITHOUT WRITING A SELECTOR, YOU CANNOT DEPLOY `rs` in K8s.

Since, there is none here, `rs` couldn't get applied.

> If `rc`, we didn't write any selector and though it gets applied and the selector will be adapted by the labels that are given under the template section. AND THIS IS NOT THE CASE FOR `rs`

`rs-multicontainer+matchLabels.yaml`
```yaml
apiVersion: apps/v1
kind: ReplicaSet
metadata:
  name: nginx-rs
  labels:
    app: nginx-app
    env: prod
    release: v1.0
  
spec:
  replicas: 3
  selector:
    matchLabels:
     app: nginx-app
     env: prod
     release: v1.0
     
  template:
    metadata:
      name: nginx-rs
      labels:
        app: nginx-app
        env: prod
        release: v1.0

    spec:
      containers:
      - name: write-app
        image: alpine
        command: ["/bin/sh"]
        args: ["-c", "while true; do date >> /var/log/index.html; sleep 10; done"]
        volumeMounts:
        - name: rc-shared-volume
          mountPath: /var/log

      - name: server-app
        image: nginx:latest
        ports:
        - containerPort: 80
        volumeMounts:
        - name: rc-shared-volume
          mountPath: /usr/share/nginx/html

      volumes:
      - name: rc-shared-volume
        emptyDir: {}
```

this gets applied in ease. ++ those labels has to match expressions in both the selector and in the metadata, - to match the labels in the `spec.template.metadata.labels`.
- The `selector.matchLabels` determines which Pods the ReplicaSet manages.
- The `spec.template.metadata.labels` defines the labels assigned to Pods created by the ReplicaSet.
- These two must align for the ReplicaSet to manage its Pods properly.

Label mismatch - PODs will not be controlled by the `rs`

Apply the `rs` and verify the same.
```sh
kubectl get rs -o wide
kubectl get po -o wide --show-labels
```

Let play around by removing one of the label of `rs`, to do that,  run the same and pick one of the PODs Label: or simply `kubectl edit rs/rsName` to perform the same.

```sh
kubectl label pods PodName app-
```
will give the output of `POD unlabeled`

AND,
```sh
kubectl get po -o wide --show-labels
```
now, you can able to see that the 
- unlabeled POD gets isolated, out of `rs`'s control to be orphaned.
- a new POD gets created  with the same labels.

WHY? if you want to do some patch, troubleshooting, run jobs inside the Container or such. But to perform the same outside a circle, isolate it by removing any one of the table and perform the same on a POD.

Delete, if needed. or bring back the same POD to the `rs`, after adding, the last one gets removed. Having a POD in isolation forever is of no use. To add back
```sh
kubectl label pods isolatedPodName app=label
```
all will be back to default by old one getting associated with the `rs` and one of the least qualified POD gets terminated. 

> **THIS IS HOW `matchLabels` works for `rs`**

Later, we will see in brief about a similar concept name `matchedExpressions`

> [!NOTE]  ##### FINAL file - demo manifest of a multicontainer `replicationcontroller`.  RC - MANIFEST
> ###### multicontainer-rc.yaml.yaml
> ```yaml
apiVersion: apps/v1
kind: ReplicaSet
metadata:
  name: nginx-rs
  labels:
    app: nginx-app
    env: prod
    release: v1.0
>
spec:
  replicas: 3
  selector:
    matchLabels:
     app: nginx-app
     env: prod
     release: v1.0
>
 template:
    metadata:
      name: nginx-rs
      labels:
        app: nginx-app
        env: prod
        release: v1.0
>
 spec:
      containers:
      - name: write-app
        image: alpine
        command: ["/bin/sh"]
        args: ["-c", "while true; do date >> /var/log/index.html; sleep 10; done"]
        volumeMounts:
        - name: rc-shared-volume
          mountPath: /var/log
>
      - name: server-app
        image: nginx:latest
        ports:
        - containerPort: 80
        volumeMounts:
        - name: rc-shared-volume
          mountPath: /usr/share/nginx/html
  >volumes:
      - name: rc-shared-volume
        emptyDir: {}

---
# 5) Demo: Deleting RC  using Cascade Option

Deleting `rc` without deleting the PODs which are created by `rc`itsef.

referring the same .yaml what we've been using earlier,

```yaml
apiVersion: v1
kind: ReplicationController
metadata:
  name: nginx-rc
  
spec:
  replicas: 3
  selector:
    app: nginx-multicontainer-rc
  template:
    metadata:
      labels:
        app: nginx-multicontainer-rc
        env: prod
        release: v1.0
        
    spec:
      containers:
      - name: write-app
        image: alpine
        command: ["/bin/sh"]
        args: ["-c", "while true; do date >> /var/log/index.html; sleep 10; done"]
        volumeMounts:
        - name: rc-shared-volume
          mountPath: /var/log

      - name: server-app
        image: nginx:latest
        ports:
        - containerPort: 80
        volumeMounts:
        - name: rc-shared-volume
          mountPath: /usr/share/nginx/html
  
      volumes:
      - name: rc-shared-volume
        emptyDir: {}

# apiVersion: v1
# kind: ReplicationController
# metadata:
#   name: nginx-rc
# spec:
#   replicas: 3
#   selector:
#     app: nginx-multicontainer-rc
#   template:
#     metadata:
#       labels:
#         app: nginx-multicontainer-rc
#         env: prod
#         release: v1.0
#     spec:
#       containers:
#       - name: write-app
#         image: alpine
#         command: ["/bin/sh"]
#         args: ["-c", "while true; do date >> /var/log/index.html; sleep 10; done"]
#         volumeMounts:
#         - name: rc-shared-volume
#           mountPath: /var/log
#       - name: server-app
#         image: nginx:latest
#         ports:
#         - containerPort: 80
#         volumeMounts:
#         - name: rc-shared-volume
#           mountPath: /usr/share/nginx/html
#       volumes:
#       - name: rc-shared-volume
#         emptyDir: {}
```
so, simply and create and verify resources,
```sh
kubectl create -f /path/to/rcmulticontainer.yaml
kubectl get rc -o wide
kubectl get po -o wide 
```
Voila! We have all the PODs and there is an `rc` which are managing the PODs, replica of three PODs in total. Done! Now,

So , the main objective for the current context is to remove the `rc` but not the PODs. We have seen the reverse of it where we removing the PODs but not the `rc` by simply reducing the number of replicas to `0` where there will be no pods, 
```sh
kubectl scale --replicas=0 rc/rcName 
```

What is cascade? it is the straight opposite of the previous one where you keep the PODs but delete `rc` which are even created by `rc`. to perform the same (deleting `rc` but keeping the PODs)
```sh
kubectl delete --cascade=orphan rc rcName
```
> **`rc` gets deleted but all those PODs remains.**  

What happens if I try to recreate the same `rc`?  
```sh
kubectl create -f /path/to/rcManifest.yaml
```
adapts and takes to be reused to manage the PODs created by `rc` previously, instead of creating own. if any changes has been applied in the `replica=n` either creates new or terminates existing ones. 

This is what `cascade` used for. Why? 
- for any troubleshooting
- any revision or changes has to be made in the manifest to reapply.
cascade back -> edit manifest -> apply manifest yaml file.

==RARE SCENARIO== BUT TINKERING `RC`
Example: 
- `replica=3`
- `image: nginx:latest` -> `nginx:20.2`
-  any tother changes can be made in manifest as per convenience
> THE PODs are still reused,
```sh
curl -I IP
```
with the previous version. 

and if we change manifest, the older versions of the image will still get persistent and eventually mismatch with the one in the manifest. 

> IT HAS TO MATCH! so pods must be relaunched, to do that 
> 1) you can simply stop all the PODs one by one
> 2) or simply scale down and up to re-spin with the updated version of the image
```sh
kubectl scale --replicas=0 rc rcName
kubectl scale --replicas=n rc rcName
```
Versions gets created and updated. Verify the same with
```sh
curl -I IP
```

this has shown How to manage `cascade` under  `rc` keeping the PODs without a workload resource. 

--- 

