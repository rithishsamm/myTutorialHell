##### Section 13: K8s Storage (PV, PVC)

Section 14: K8s Storage (PV & PVC)
1.  Introduction to Persistent Storage
	Introduction to Persistent Storage - [Doc]
2. Understanding Persistent Volume Access Modes
	Understanding Persistent Volume Access Modes - [Doc]
3.  `Static PV` and `PVC` (`volume plugin: hostPath`)
	Static `PV` and `PVC` (`volume plugin hostPath`) - [Doc]
4. `PV` and `PVC` management (`hostPath`)
	`PV` and `PVC` management (`hostPath`) - [Doc]
5. `Static PV` and `PVC` (`volume plugin: nfs`)
6. Persistent Volume Reclaim Policies (`nfs`: retain - (default RECLAIM POLICY), recycle, delete)
7. Introduction to `AccessModes` for `PV` and `PVC`
	Introduction to `AccessModes` for `PV` and `PVC` - [Doc]
8. `AccessModes`
	AccessModes - [Doc]
9. Understanding `PV` phases
	Understanding `PV` phases - [Doc]

---

# Section 13: K8s Storage (PV, PVCs)
# 1. Introduction to Persistent Storage (PV, PVCs)
Introduction to Persistent Storage - [Doc]

So far, we have seen about `Volumes` and its `types` in brief on handling and storing data across K8s Clusters. And understood ==that if the POD gets deleted, also the data. The K8s doesn't have a way of chance having `APIs` to communicate with or the handle volumes in order to manage and administrate it in ease. 
And we have somehow mitigated the situation with `nfs` and such. But,==

###### **HOW DO WE AVOID SUCH CASE AND WHAT IS THE SOLUTION FOR IT ??** - **==Persistent Storage,
Under that, there are few types to it,
1) **Persistent Volume (PV)**
2) **Persistent Volume Claim (PVC)**
3) **Storage Class (sc)** 
4) **Volume Expansion** (advanced)
5) **Volume Snapshots** (advanced)

> **How does this `PersistentVolume` differs from `Volumes`?** 
Volumes gets deleted if the POD gets deleted (even in `nfs`, the volume gets deleted but not the data). Here, that is not the case with `PV`, The Volumes won't get deleted.

**==`PersistentVolume` is a whole separate object.==**. These `PVs` are not binded to the PODs. 
**Here, has some `APIs` that the K8s Clusters that can communicate to, to fetch information related to the storage. where we can able to handle, manage and administrate the storage volumes in ease.** 

So, tldr, even if the POD gets deleted, `PersistentVolume` `PVs` doesn't. here, there are 

###### PersistentVolume (`PV`) Core feature components:
1) **Provision Types:**
- Static
- Dynamic

2) **Access Modes:**
- ReadWriteOnce (`RWO`)
- ReadWriteMany (`RWX`)
- ReadOnlyMany (`ROX`)
- ReadWriteOncePod (`RWOP`)

3) **Binding**
- `PV` bind with `PVC`
- `PVC` bind with `PV` 
(both are Bi-directional)

4) **Reclaim Policy,**
- retain - (default RECLAIM POLICY)
- Delete
- Recycle

###### PersistentVolume Claims (`PVC`) Core feature components:
Nothing but claim along with or to the `PV`, PV can be created either by an administrator (static PV)or by Storage Class (Dynamic PV).


##### Persistent Volume Solutions for `K8s`
- In order to provide `Volume` as Object, which is controlled through API by administrator for workloads (POD)
- `PersistentVolume` life cycle is `independent` to the POD workloads utilized by it. 
- Even  after deleting workloads (pod, deployment, daemonset etc.,) volume object should exist (persistent)
- `PersistentVolume` will be provisioned by administrator **statically** or by StorageClass **dynamically**.  
- Plugins that are supposed by Volumes are also supported in PV (most of them)
	- `hostPath, nfs, local, iSCSI` etc.
- `PresistentVolumeClaim` is a request from workloads to consume `PersistentVolume`.
- We can also create `dymanic PVC` as well under `StatefulSet` `sts`  Object with ` VolumeClaimTemplates `
- `PVC` requests `storage size` and `access modes` (RWO, RWX, ROX, RWOP)
- `PV` and `PVC` binding is `one-to-one` mapping (bi-directional)
- One `PVC` can be attached to multiple workloads (PODs)
- `AccessModes` are controlled by `kube-controller-manager` and `kubelet` only specified PODs are allowed to access `PV`.
- Storage Object use Protection will ensure, `PVC` gets deleted only if it is not actively used by any `POD` (workloads)
- `Persistent Volume` supports two modes: `FIlesystem` (default) and `Block` (RAW)
- `PV` is a **cluster level** object and `PVC` is **namespace level** Object. 

##### In order to provide `Volume` as Object, which is controlled through `API` by administrator for workloads (POD)
If there is POD with multiple containers, they share data in between those containers using a shared volume. 

***Storage Volumes:*** As mentioned before, there has been no authority over the volume and it has been heavily dependent on the state of the POD but not the K8s. So, if the POD gets deleted, so that the volumes of it too. **Not the DATA Fyi but still the volume that contains it, gets lost when deletion.** 

***PersistentVolumes***: and there is POD with some containers, here there will be a thing called 
-  `PVC` (PersistentVolumeClaim) will be attached to the PODs. either a static one gets dynamically created by the dev/user by raising a claim request via `API` calls or a Dynamic one which can be created by StorageClass . 
- `PV` (PersistentVolume) which will be created by (we have to create it by ourselves) ***admin*** to back the PODs with `PVC`. This `PV` is a separate object which is not a part of the PODs, IT IS A WHOLE SEPARATE OBJECT. 
So, here if the POD gets deleted, (since these are brought and introduced out of the POD object, Which itself is a separate object created by the `K8s`.) the `PVC` and the `PV` will not be deleted. The whole volumes of it, persists still. Unless and until, we deleted it manually. 

With the same, we can determine the `PVs` volume type as same as the normal volumes here too (hostPath, local, emptyDir, nfs etc). 

In `PVs`, the Volumes will be considered as a separate object, which is not dependent to not just PODs but any, acting as a separate object and THAT, can be controlled through `API` provisioned by the K8s cluster which can be controlled by the administrator or the storage class (sc) in the clusters. 

##### `PersistentVolume` life cycle is `independent` to the POD workloads utilized by it. 
As the point speaks for itself, The `PVs` life is independent (it will persists without depending on any object), not dependent on the Workloads utilized by it. So, the PODs carrying the workloads will not affect the volumes if a deletion or a downtime occurs, the `PVs` persists. 

##### Even  after deleting workloads (pod, deployment, daemonset etc.,) volume object should exist (persistent)
Even  after deleting workloads by Any type of **Workloads Object** (be it pod, deployment, daemonset etc.,), the PVs volume objects will/should exist (persistent). 

##### `PersistentVolume ` will be provisioned by administrator **statically** or by StorageClass **dynamically**.  
 `PV` can be created either by an administrator (static `PV`) or by StorageClass by K8s (Dynamic `PV`).

##### Plugins that are supposed by Volumes are also supported in PV (most of them)
	- `hostPath, nfs, local, iSCSI` etc.
The Plugins, the volume types that are supported by the normal Volumes are also supported by the `PVs` as well too. The supported Plugins for the `PVs` are 
 - HostPath,
 - nfs,
 - local,
 - iSCSI
- AWS EBS and other types of the same as well. 
Whatever we have seen in the volumes, can be utilized by the `PVs` as well. 

##### `PersistentVolumeClaim(PVC)` is a request from workloads to consume `PersistentVolume(PV)`. 
  A workload that having a `PVC` is there on behalf of `PV` since the POD will not be attached to `PV`. So, `PVC` is a Claim that provide communication between the PV and the PODs.  (raised by a DEV or USER) goes through a PV. 

#####  We can also create `dymanic PVC` as well under `StatefulSet` `sts`  Object with ` VolumeClaimTemplates `
So far, we have been studying of Dynamically creating PVs with StorageClass (sc). **How about PVCs? -**
**We can create `dymanic PVC`  under `StatefulSet` `sts`  Object only  with ` VolumeClaimTemplates `**
(not the other Workload Object such as `PODs`, `rc`, `rs`, `deploy`, `jobs`, `cron` or such)


#####  `PVC` requests `storage size` and `access modes` (`RWO, RWX, ROX, RWOP`)
`PVC` requests `storage size` and `access modes` (`RWO, RWX, ROX, RWOP`). Means, 
(`RWO, RWX, ROX, RWOP`) on
- who to access,
- how to access, 
- multi POD access
- How much of storage size needed?
- And such things like that,
Will see in practice when we write manifests for `PV` and `PVC` concept. 

##### `PV` and `PVC` binding is `one-to-one` mapping (bi-directional)
`PV` and `PVC` binding is `one-to-one` mapping (bi-directional). 
**One PVC can be attached to only one PV. And vice versa, One PV can be attached to one PVC. IT IS Bi-directional AND IT IS A One-to-one mapping.** 

##### One `PVC` can be attached to multiple workloads (PODs)
But , ONE PVC can be attached to Multiple PODs. It can be assessed by the Access Modes (`RWO, RWX, ROX, RWOP`) of the PVCs. 
With this access mode with the right permissions, one PVC can be attached to multiples workloads running on the same node or different mode. 

##### `AccessModes` are controlled by `kube-controller-manager` and `kubelet` only specified PODs are allowed to access `PV`
`AccessModes` are controlled by `kube-controller-manager` and `kubelet`, running on that node. only specified PODs are allowed to access `PV`.

##### `Storage Object use Protection` will ensure, `PVC` gets deleted only if it is not actively used by any `POD` (workloads)
**`Storage Object use Protection`** is a feature that will ensure, 
That If a POD exists and actively using and running a workload, the `PVC` won't be deleted, it gets deleted only if it is not actively used by any`POD` (workloads)


##### `Persistent Volume` supports two modes: `Filesystem` (default) and `Block` (RAW)
`Persistent Volume` supports two modes:
-  `Filesystem` - a storage written with a FS and ready to use (default) and 
- `Block` (RAW) - we have to provision the machine and configure `FS` in it all by ourselves. 

##### `PV` is a **cluster level** object and `PVC` is **namespace level** Object. 
-  `PV` is a **cluster level** object, an external global object.  and
- `PVC` is **namespace level** Object. (cannot create PVC in one namespace and create POD on the other.)

##### And there **Volume plugins that supports Raw Block Volume.** 
- `cephfs` - CephFS Volumes
- ==**`csi` - Container Storage Interface (CSI)==** - For CLOUD STORAGE INTEGRATION
- `fc` - Fibre Channel (FC) storage
- `hostPath` - HostPath Volume (for single node testing only; WILL NOT WORK) in a multi-node cluster; consider using `local` volume instead)
- `iscsi` - iSCSI (SCSI over IP) storage
- `local` - local storage devices mounted on nodes. 
- `nfs` - Network File System (NFS) storage
- `rbd` - Rados Block Device (RBD) volume. (Ceph Block Device)

Other Block storages such as, 
- AWS EBS - Amazon Elastic Block Store 
- AzureDisk 
- FC (Fibre Channel)
- GCEPersistentDisk 
- OpenStack Cinder 
- VsphereVolume

Deprecated volume plugins, and Removed plugins, 
```
Deprecated
- awsElasticBlcokStore - AWS Elastic Block Store (EBS) (in v1.17)
- azureDisk - AzureDisk (in v1.19)
- azureFile - Azure File (in v1.21)
- cinder - Cinder - OpenStack block storage (v1.18)
- flexVolume - FlexVolume (in v1,23)
- GCEPersistentDisk - GCE Persistent Disk (v1.17)
- portworxVoule - Portworx Volume (1.25)
- VsphereVolume - Vsphere VMDK volume (in v1.19)

Removed
- `photonPersistentDisk` - Photon controller persistent disk 
```

>  When we work with `PVs`, should concentrate more on the **ACCESS MODES and RECLAIM POLICY**. 


---
# 2 . Understanding Persistent Volume `AccessModes`
Understanding Persistent Volume Access Modes - [Doc]

[Documentation:](https://kubernetes.io/docs/concepts/storage/persistent-volumes/#persistent-volumes)
```cardlink
url: https://kubernetes.io/docs/concepts/storage/persistent-volumes/#persistent-volumes
title: "Persistent Volumes"
description: "This document describes persistent volumes in Kubernetes. Familiarity with volumes, StorageClasses and VolumeAttributesClasses is suggested.Introduction Managing storage is a distinct problem from managing compute instances. The PersistentVolume subsystem provides an API for users and administrators that abstracts details of how storage is provided from how it is consumed. To do this, we introduce two new API resources: PersistentVolume and PersistentVolumeClaim.A PersistentVolume (PV) is a piece of storage in the cluster that has been provisioned by an administrator or dynamically provisioned using Storage Classes."
host: kubernetes.io
favicon: https://kubernetes.io/icons/favicon-64.png
image: https://kubernetes.io/images/kubernetes-open-graph.png
```


>  NOTE: When to use these access modes heavily depends on `Volume Plugin`, which means we can't use as per our need for anything and everything.

Before moving on with `PVs`, we must have basic understanding of these Access modes available and the purpose does that provides, There are four AccessModes,
- ReadWriteOnce (RWO)
- ReadWriteMany (RWX)
- ReadOnlyMany (ROX)
- ReadWriteOncePod (RWOP)

| Access Modes              | Description                                                                                                                                                   |
| ------------------------- | ------------------------------------------------------------------------------------------------------------------------------------------------------------- |
| ReadWriteOnce (`RWO`)     | If we need to give Read and Write Permissions for the PODs running on single node. This mode is not suitable for PODs running on multiple nodes (`hostPath`). |
| ReadWriteMany (`RWX`)     | We need to enable if there is requirement to use storage by multiple `PODs` running on different nodes in the cluster (`NFS`)                                 |
| ReadOnlyMany (`ROX`)      | In order to give read-only access to `PODs` running on multiple nodes across the cluster, where write access should be blocked.                               |
| ReadWriteOncePod (`RWOP`) | In certain conditions, we need to give Read and Write to single `POD` running on single node (not all the `PODs` running on single node)                      |

###### 1) ReadWriteOnce (`RWO`)
 **If we need to give Read and Write Permissions for the PODs running on single node. This mode is not suitable for PODs running on multiple nodes (`hostPath`).**

Eg: Say, there are PODs running on **single node** be it any type of workloads, and there is PVs attached to those Workload Objects. 
>  All those PODs running on that node out of any Workload Object can READ & WRITE on the PV.  Like `hostpath`

 ==(IF THERE ARE WORKLOADS RUNNING ON MULTIPLE NODES, **ReadWriteOnce** IS NOT THE RIGHT CHOICE. )==


##### 2 ) ReadWriteMany (`RWX`)
**We need to enable if there is requirement to use storage by multiple `PODs` running on different nodes in the cluster (`NFS`)**

Here, PODs running on different **multiple nodes**. And there we have a `PV` and 
>  `PV` has to be utilized by all the workloads running on different nodes. This goes well with **ReadWriteMany** AccessModes. (ReadWrite Permissions on all those nodes, appropriate example: `nfs`)

##### 3) ReadOnlyMany (`ROX`)
**In order to give read-only access to `PODs` running on multiple nodes across the cluster, where write access should be blocked.**
 As same as **ReadWriteMany** AccessModes but **ReadOnly**. 
 Means, 
 >  the applications that are running in that container cannot write the data but to read in the **volumeMount** that we use. (across nodes.)

##### 4 ) ReadWriteOncePod (`RWOP`)
**In certain conditions, we need to give Read and Write to single `POD` running on single node (not all the `PODs` running on single node)**
From the newer versions of K8s namely (>=v 1.22),
>  Same **ReadWrite** Permissions, but **assigning that rule only to a specific/particular POD on a specific/particular node.** 
>  (not to all the PODs, if so `ReadWriteMany` would work great. )

>  When to use AccessModes: it is heavily depends on `Volume Plugin`, 
>  which means we can't use all these AccessModes to all the available plugins, (depends on CSI driver, support of the VolumeType and more)
> Can't use as per our need for anything and everything.

Will see how to use and which to use in brief given in the below table:

> **Important!** A volume can only be mounted using one access mode at a time, even if it supports many.

Here are the list of **VolumePlugins** and its available **AccessModes**:

| Volume Plugins | ReadWriteOnce         | ReadOnlyMany          | ReadWriteMany                      | ReadWriteOncePod      |
| -------------- | --------------------- | --------------------- | ---------------------------------- | --------------------- |
| AzureFile      | ✓                     | ✓                     | ✓                                  | -                     |
| CephFS         | ✓                     | ✓                     | ✓                                  | -                     |
| CSI            | depends on the driver | depends on the driver | depends on the driver              | depends on the driver |
| FC             | ✓                     | ✓                     | -                                  | -                     |
| FlexVolume     | ✓                     | ✓                     | depends on the driver              | -                     |
| HostPath       | ✓                     | -                     | -                                  | -                     |
| iSCSI          | ✓                     | ✓                     | -                                  | -                     |
| NFS            | ✓                     | ✓                     | ✓                                  | -                     |
| RBD            | ✓                     | ✓                     | -                                  | -                     |
| VsphereVolume  | ✓                     | -                     | - (works when Pods are collocated) | -                     |
| PortworxVolume | ✓                     | -                     | ✓                                  | -                     |


--- 
# 3 .  Static `PV` and `PVC` (volume plugin: `hostPath`)
Static `PV` and `PVC` (volume plugin `hostPath`) - [Doc]

[Documentation:](https://kubernetes.io/docs/concepts/storage/persistent-volumes/#persistent-volumes)

**Demonstration: Static `PV` and `PVC` (volume plugin: `hostPath`)**

Manifests:

#### Practicals
on creating a **Static**
1)  **`PersistentVolume` PV and a**
2) **`PersistentVolumeClaim` PVC.** for a `hostPath` volume plugin. 

```
kubectl api-resources | grep PersistentVolume
```

Here are the AccessModes available for the `hostPath` PV VolumeType:

| Volume Plugins | ReadWriteOnce | ReadOnlyMany | ReadWriteMany | ReadWriteOncePod |
| -------------- | ------------- | ------------ | ------------- | ---------------- |
| HostPath       | ✓             | -            | -             | -                |

>  From the same, we know that the `hostPath` only supports **`ReadWriteOnce`** AccessMode. 

1) **PV** - **Creating a static `PV`, creating manually all by ourselves:** (Statically)
Manifest:
```yaml
apiVersion: v1
kind: PersistentVolume
metadata:
  name: static-pv-hostpath
  labels:
    type: hostpath
    env: prod
    release: v1.0

spec:
  storageClassName: hostpath
  capacity:
    storage: 5Gi
  accessModes:
    - ReadWriteOnce
  hostPath:
    path: "/var/tmp"
```

Start and verify,
```sh
kubectl create -f /path/to/hostPathPV.yaml
```

Probably the `pv` might show the status either - `Available` or `Pending`. If it is 
- `Available` - it is ready to be attached with the claim of the `pvc`. 
- `Pending` - is waiting to be ready and to be attached with the claim of the `pvc`

IDEA is, In order to get the `PV` associated with the `PODs`, `PVC` 's are essential for it to be the claim between the `PV` and the `PODs`.

**So, now to Create 
2) **`PVC`, for the PODs to be on and the PV's to be on too.** (Statically)
```yaml
apiVersion: v1
kind: PersistentVolumeClaim
metadata:
  name: static-pvc-hostpath
spec:
  storageClassName: hostpath
  accessModes:
    - ReadWriteOnce
  resources:
    requests:
      storage: 5Gi

```

>  **note**: here in the Storage, if you misvalue the amount, It will be applied as same as the (proportionate and bi-directional to the) `PV`. Eg: in the manifest, of `PV` is 5 GB, but in the `PVC` is 3 GB, the `PVC` will be 5 GB too. 

Recalling here, [[00.Section14-K8sStorage(PV, PVC)#`PV` and `PVC` binding is `one-to-one` mapping (bi-directional)]]

+*This is a `claim` that will be created by either a user or developer.* 

Now if you notice,
```
kubectl get pv,pvc -o wide
```
The `pvc` status shows as **bound** means binded/bounded, it gets claimed automatically and show to which `PV` it got bounded too and the capacity. 

**One `PVC` can be attached to only one `PV`.** 
>  And it success rate is heavily depend on the **VolumeType** object and the **AccessMode** of the `PV` and `PVC` for it to be succeed. 


**SUCCESSFULLY CREATED A STATIC `PV`, `PVC`!**, 
Now, will deploy a POD/workload to be attached with the `PVC`, which can be utilized and backed by `PV`. In further node, 

Recall [[00.Section14-K8sStorage(PV, PVC)#`PV` is a **cluster level** object and `PVC` is **namespace level** Object.]] make sure the POD falls under the appropriate namespace.  You cannot create the PODs in a different namespace and expect to be connected with the `PVC` it is not possible. SAME PODs IN THE SAME NAMESPACE. (`pv` cluster level, `pvc` namespace level)


Please be aware while creating the POD. The expressions really matter here. 
PODs Manifests:
```yaml
apiVersion: v1
kind: Pod
metadata:
  name: nginx-pod
  labels:
    app: app
    env: prod
    release: v1.0

spec:
  containers:
    - name: app
      image: nginx:1.7.9
      ports:
        - containerPort: 80
          hostport: 8090
          protocol: TCP

      volumeMounts:
        - name: static-pvc-hostpath
          mountPath: /usr/share/nginx/html

  volumes:
    - name: static-pvc-hostpath
      persistentVolumeClaim:
        claimName: static-pvc-hostpath
```
Just a standalone POD which is utilizing `PV` which we attached a claim to it and backing by a `PV`.

```
kubectl create -f /path/to/PODmanifest.yaml
```
```
curl PodIP
```

Probably shows forbidden because there are no specs we did specified `/var/tmp` but there will be the **VolumeName** and **`index.html`** to show.  

So, 
- Identify the node that the POD is running on,
- Ssh into it, 
- Create `index.html` inside the dir `/var/tmp/index.html` and write some content in it.

```
curl PodIP
```
==**And so far, the `PVs` are created, gets bounds and shows. But the volumes are not actaully implemented but simply shows that are exists.** 
 **And only When you launch the POD, the volumes get launched and utilized, until then, it lies in the `etcd` waiting to scheduled when the `POD` gets launched on a specific node and `ns`, `PV` just works.**==

==**Starts created the volume under the node and the POD utilizes it.**==

HOW TO VERIFY: (incase if to troubleshoot) - SIMPLY **DESCRIBE** THE POD and SHOW ALL THE INFO. 
```
kubectl describe po podName
kubectl describe po nginx-pod  | grep Vol
```
- Check the events
- And there will be a section named `Volumes` contains the details 

And to verify further, get into the shell of the PODs and, 
```
kubectl exec -it podName -- bash 
```

```
echo "<h1>ExtraRemarks-Some random content</h1>" > /usr/share/nginx/index.html
exit
```
```
curl PodIP
```
Shares volumes in ease. 

So, the concept of `PV` is that the data (from `PV` and `PVC` which isn't a part of the workload resource since it is a separate object)Persists even when the workload gets deleted. 

```
kubectl delete po podName
kubectl get po -o wide
```

Even the workloads gets deleted, the `PV` and `PVC` should not get deleted. 
```
kubectl get pv,pvc -o wide
```
Stills bounded and exists still.

Even, you can relaunch the same POD and it works well still. (if it is launching on the same node and namespace.)

```
kubectl create -f /path/to/PODmanifest.yaml
```
```
curl PodIP
```
Works as same as before. Using the same `pv`  volume and voila! With the hostpath type with **ReadWriteOnce** AccessModes. 



---
# 4 . `PV` and `PVC` management (`hostPath`)
PV and PVC management (hostPath) - [Doc]

###### MANAGING THE **PersistentVolumes**: (hostPath)
Previously, we have just experimented the state of `PVs` and working with it, here, will dig deep into tinkering with the `PVs`.

+Previously we didn't do much but deleted and redeployed the POD which worked pretty well. 

How about `PVs`. What happens we delete the `PV and PVC`? Lets see. 
 
```
kubectl get pv,pvc -o wide
```
```
kubectl delete pvc pvcName
```
And check the status, claim status and such especially `PV's`, the status says `released`. Why?
Cause, we use the `reclaim policy` as **'retain - (default RECLAIM POLICY)'**. Will see in brief of what is it all about!

And check deploying the same POD and see what happens, 
```
kubectl create -f /path/to/podManifest.yaml
```

And check back the `PV` 's status.
```
kubectl get pv,pvc -o wide
```

Still using the same `StorageClass` name but the status will be `Pending`. Even though all the parameters match, there are no `PV` s are in available status, so it halts waiting for it to be bounded. 

How to revert back and restore the `PVC` to make it available. REMOVE THE CLAIM. Then, it will be back. And the `PV` back to bound. 
```
kubectl edit persistentvolume/pvname
```
`ClaimReference` section under spec: (and the data will be available still but not the volume.) to make it available, delete the section for the `PV` to reclaim it back. 
```
#remove
claimRef:
   apiVersion: v1
   kind: PersistentVolumeClaim
   name: static-pvc-hostpath
   namespace: default
   resourceVersion: "300477"
   uid: 753fdbbb-3c38-45c7-adb1-3b01e5ba3244
```
Save, exit and check status. 

And `PV` will be in  `available`. Now if the patterns match with the `PVC` that we recreates, it gets bounded. (happens with the help of `reclaim policy)`.

**HOW TO REMOVE `PV` AND `PVC`? DELETE IT!**
As an Administrator, it won't get deleted unless and until we manually delete it. (since it is a separate object and it is not binded in anyway with the POD, so it should be manually deleted.) 

First delete `PVC` and then delete `PV` later,  
```
kubectl delete pvc pvcName
kubectl delete pv pvName
```
But the `hostPath` exists still. The data is not deleted yet, on the node where it was launched earlier. (as same as volume where even if we delete the volume, the data in it still exist - the hostPath exists).

**Here, the PVC and its volume containing it, will be binded to the POD but not the `PV`.**

###### type:
In the volumes section under `PV` 's object - manifest, we can also declare `type` - as we have referred in the [[7 . hostPath - Demo for hostPath volume type (File and FileOrCreate)]] and [[6. hostPath - Demo for hostPath volume type (Directory and DirectoryOrCreate)]] In the `hostPathPV.yaml`, we can able to **point to or create a file or a directory if not exists** with the help of hostPath's **`FileOrCreate`** and **`DirectoryOrCreate`** feature. 

`hostPathPV.yaml`
```yaml
hostPath:
  path: "/var/tmp/nginx-data"
  type: DirectoryOrCreate
  #or FileOrCreate
```
Apply the `PV` and the rest `PVC` and the `POD`. it works since it is a `hostPath` and the `kubelet` takes care of `File` or  `Dir` creation. 

And cross check by `ssh` into the node. And go to `/var/tmp/VolumeName`. 

It gives for `error 404` if i `curl`. Its obvious tho. 
Solve it by, 
- going into the container and `/usr/share/nginx/html` or 
- go to the node and create file by `/var/tmp/nginx-data/index.html` and write in it and 
- `curl`
>  **Cons: Can't use across multiple nodes despite of having all the procs of PVs and such, end of the day, it is a hostpath**.


##### Cleanup the objects and the data:
```
kubectl delete pv/pvName pvc/pvcName pod/podName
rm -rf /all/the/dirs&files
```


---
# 5 . Static `PV` and `PVC` (volume plugin: `nfs`)
[Documentation:](https://kubernetes.io/docs/concepts/storage/persistent-volumes/#persistent-volumes)

###### SAME **PersistentVolumes** but with `nfs`
**To evade all the cons and clutter we had with `hostPath`**. To get the perks of both the `PVs` and also `nfs`
- To keep the data `PersistentVolume`
- And the `nfs` to keep and manage data across all nodes with **multiple workload objects**.
WILL SEE THE `PV` VOLUME PLUG IN WITH `nfs`.  
##### Prerequisites: (Same setup what we have referred for `nfs` volume here for the `PV` too. For reference: [[00.Section13-K8sStorage(Volumes)#9. `NFS` Setup `NFS` server for Kubernetes Volume Demo]])
- A dedicated `nfs` server,
- `nfs-server` tool in it. 
- Check docs for which `AccessModes` are available for `nfs`
**AccessModes:**

| Volume Plugins | ReadWriteOnce | ReadOnlyMany | **==ReadWriteMany==** | ReadWriteOncePod |
| -------------- | ------------- | ------------ | --------------------- | ---------------- |
| NFS            | ✓             | ✓            | ✓                     | -                |
(except RWOP). 

**==Main USP: Use `nfs` if you are deploying across nodes(like replica and such).==** 
- **FOR THIS, USE `ReadWriteMany` AccessMode.** 
- Use **ReadWriteOnce**, if you deploy only one single workload. (even if its not working with ReadWriteOnce, use **ReadWriteMany** straightaway)

Prerequisites steps:
```
ssh admin123@192.168.0.123
cd /var/nfs/
sudo mkdir /var/nfs/volumeName
ls -l #check permissions (change root to nobody,nogroup)
```
```
kubectl get pv -o wide
```

Lets write a `yaml` for `PV`,
`pv.yaml`
```yaml
apiVersion: v1
kind: PersistentVolume
metadata:
  name: static-nfs-pv
  labels:
    name: app
    env: prod
    release: v1.0
spec:
  storageClassName: nfs
  capacity:
    storage: 3Gi #<Size> of availeble nfs server, check that using `df -h` 
  accessModes: #refer docs that which accessModes are allowed for nfs
    - ReadWriteMany #ReadWriteOnce if you want to use it for single pod, Many if multiple pods
  nfs:
    server: 192.168.0.126
    path: /var/nfs/nfspv
```
`pvc.yaml`
```yaml
apiVersion: v1
kind: PersistentVolumeClaim
metadata:
  name: static-nfs-pv
  labels:
    name: app
    env: prod
    release: v1.0
spec:
  storageClassName: nfs
  accessModes:
    - ReadWriteMany
  resources:
    requests:
      storage: 3Gi
```

Create and verify,
```
kubectl create -f /path/to/nfspv.yaml
kubectl create -f /path/to/nfspvc.yaml
```
**The `PersistentVolumes` must be bounded.** 

```yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: jenkins-deploy
  labels:
    app: jenkins-deploy
    env: prod
    release: v1.0

spec:
  replicas: 1
  selector:
    matchLabels:
      app: jenkins-deploy
      env: prod
      release: v1.0

  template:
    metadata:
      labels:
        app: jenkins-deploy
        env: prod
        release: v1.0

    spec:
      restartPolicy: Always
      containers:
      - name: jenkins-deploy
        image: jenkins/jenkins:latest
        ports:
        - name: jenkins-port
          containerPort: 8080
          protocol: TCP
        resources:
          limits:
            memory: "2Gi"
            cpu: "1000m"
          requests:
            memory: "1Gi"
            cpu: "500m"
        volumeMounts:
        - name: jenkins-data
          mountPath: /var/jenkins_home

      volumes:
      - name: jenkins-data
        persistentVolumeClaim:
          claimName: static-nfs-pv
```

Create and Verify:
```
kubectl create -f path/to/deploy.yaml
kubectl get all -o wide
```

To troubleshoot, check logs
```
kubectl logs podName 
```
Probably permission issues.

Fix it by changing permissions to the mount for the container to write data in it. So, modify the directory permissions with `uid` and `gid`.

To the `nfs` server,
```
ssh username@nfsVMmachineIP
sudo ls -l /var/nfs/
```
Shows the `jenkins-data` directory permissions goes to `root`, change that by
```
sudo chown -R 1000:1000 /var/nfs/jenkins-data #jenkins:jenkins
```

And run. Works, set deployment:
```
apiVersion: v1
kind: Service
metadata:
  name: nfs-nodeport-svc
  labels:
    name: app
    # type: nfs 
    env: prod
    release: v1.0

spec:
  type: NodePort
  selector:
    app: jenkins-deploy
    env: prod
    release: v1.0
  ports:
    - protocol: TCP
      port: 80
      targetPort: 8080
```


And run it. In port: `8080`.
Perform something that stays persistently, and delete and restart again,

Recreate the POD again, 
And the state of the Workloads run as same the old one where we left. 



---
# 6 . Persistent Volume Reclaim Policies (`nfs`: `retain - (default), recycle, delete`)


Even though, there are volumes and VolumeType's such as `hostPath, nfs` and such, those are all simple volume which they does it's thing as a volume. 

**Reclaim policy - what should happen if when we delete volumes.**  

By the actions and behaviors of those volumes are depends in the `Policies` which that the volume which has to act accordingly. 

Here, the default **Reclaim Policy** for every `PersistentVolume` objects is - Retain. Retains `PV` even after the workload and the POD's deletion. 


| Reclaim Policy                                             | Description                                                                                                                                                                                                                                                                                                                                                                                                                                                                                             | Tldr                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                         |
| ---------------------------------------------------------- | ------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- | ------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------ |
| Retain (default)                                           | Even after deletion of the `PVC, PV` still exists with "Released" status, but it is ready for another claim (PVC).<br>Administrator has to manually reclaim the PV. We need to enable `Retain`, only when we want to `re-use` the PV.<br><br>Status of `PV` s,<br>-  `pv` when created - `available`, to be attached to a `pvc`<br>- `pvc` created and attached says - `bound`<br>- `pvc` deleted or deattached says -  `released` <br><br>Use `Retain` reclaim policy in order to reuse existing `PV`. | Nothing but the volume just retains, even after the workload object's and the PVC deletion,. `PV` retains and persists. (bound -> available, means Released).<br><br>You can perform the same by passing:<br><br>- `kubectl get pv,pvc -o wide`<br><br>- `kubectl delete pvc/pvcName`<br><br>- `kubectl get pv,pvc -o wide`<br><br><br>**Retain** is the default Reclaim Policy.                                                                                                                             |
| Recycle (policy can be for `hostPath, nfs` volume plugins) | If Volume Plugin supports this feature, we can re-use PV, but looks like a new storage with no data (`rm -rf /volume/*).`<br><br>This reduces the overhead of administrator, where no manual reclaim is needed.                                                                                                                                                                                                                                                                                         | If `pvc` gets deleted, the `pv` won't as we knew before.<br><br>Unlike `Retain` policy, <br>if `pvc` gets deleted, the status will turned to **`available`** instead of `released`.<br><br>In the back-end, the volume data will be wiped out (performs basic scrub by *`rm -rf /volume/*`* where the directories will be there, the data will be lost) and turned to be fully new that it shows as ` Available `. Since it is all new. <br><br>Helps admin to reduce overhead and no manual reclaim needed. |
| Delete                                                     | If Volume Plugin supports this feature, deleting a PVC will delete the bound PV and backend Volume Plugin resource as well.                                                                                                                                                                                                                                                                                                                                                                             | Delete `pvc` and the `pv` will also be deleted.  also the Volume. <br><br>Few VolumeType supports this reclaim policy.                                                                                                                                                                                                                                                                                                                                                                                       |
> [!NOTE] NOTE:
> 1) Reclaim policy will be applied only for `PV`, not for `PVC`.
> 2) We can modify/change reclaim policy by administrator after object creation using patch or edit
> 3) Best Practice is to control the `PV` creation dynamically, where we can define `Reclaim Policy` as template. 
##### Perform a manual claim with PV, PVC existing with,

### Retain:
You can change by removing the **Reclaim Policy** of an PV object by deploying it to default and edit the Object using,
```
kubectl get pv -o wide
kubectl edit pv pvNames

#under `spec` section, find and REMOVE `claimRef` section Fully:
claimRef:
   apiVersion: v1
   kind: PersistentVolumeClaim
   name: static-nfs-pv
   namespace: default
   resourceVersion: "742052"
   uid: 39167af0-f8cf-4777-b30f-e25794f96ca3
```
```
kubectl get pv,pvc -o wide
```
**THE `PV` Retains.** 

Also, if you delete `PV`, the data will still persist in the `nfs` too. Perform Cleanup by `ssh` into it and clear `dir`. No brainer. 

###### When to use `Retain`
If, There should be no `PVC` but `PV` should, `Retain` - Reclaim Policy fits the description. 

These two Available Reclaim Policy - `Recycle`, `Delete`  should be used based on the Volume Plugin/ VolumeType is heavily depends on which one you use, 


### Recycle:
Supports only if the filesystem is an `nfs` or `hostpath`

If Volume Plugin supports this feature, we can re-use PV, but looks like a new storage with no data (`rm -rf /volume/*).` This reduces the overhead of administrator, where no manual reclaim is needed.

If `pvc` gets deleted, the `pv` won't as we knew before. Unlike `Retain` policy, if `pvc` gets deleted, the status will turned to **`available`** instead of `released`.

In the back-end, the volume data will be wiped out (performs basic scrub by *`rm -rf /volume/*`* where the directories will be there, the data will be lost) and turned to be fully new that it shows as ` Available `. Since it is all new. 

Helps admin to reduce overhead and no manual reclaim needed.
 
### Delete: (works well on managed services)
If Volume Plugin supports this feature, deleting a PVC will delete the bound PV and backend Volume Plugin resource as well.

Delete `pvc` and the `pv` will also be deleted.  Also the Volume. few VolumeType supports this reclaim policy.

Recall:
> [!NOTE] NOTE:
> 1) Reclaim policy will be applied only for `PV`, not for `PVC`.
> 2) We can modify/change reclaim policy by administrator after object creation using patch or edit - by removing `claimRef:` section.
> 3) ==Best Practice is to control the `PV` creation dynamically, where we can define `Reclaim Policy` as template. **Always create `PV` dynamically using `StorageClass`, avoid or less likely to create it manually by admin.**==

Will see all these `reclaim policy` in practice and observe how things works. We have already seen `Retain`, will see how `Recycle` and `Delete` works. 

##### Practicals:
Here is the base template for the same. Just replace - `persistentVolumeReclaimPolicy` parameter to retain or delete, (recycle is deprecated).

```yaml
apiVersion: v1
kind: PersistentVolume
metadata:
  name: static-nfs-pv
  labels:
    name: app
    env: prod
    release: v1.0
    
spec:
  persistentVolumeReclaimPolicy: Recycle
  storageClassName: nfs
  capacity:
    storage: 3Gi #<Size> of availeble nfs server, check that using `df -h` 
  accessModes: #refer docs that which accessModes are allowed for nfs
    - ReadWriteOnce #ReadWriteOnce if you want to use it for single pod, Many if multiple pods
  nfs:
    server: 192.168.0.126
    path: /var/nfs/nfspv
```
###### Recycle: (deprecated)
```
kubectl create -f path/to/RecyclePolicy-nfsDeployPV.yaml
kubectl create -f path/to/RecyclePolicy-nfsDeployPVC.yaml
kubectl create -f path/to/RecyclePolicy-nfsDeploy.yaml
kubectl create -f path/to/RecyclePolicy-svc.yaml
```

```
kubectl get all -o wide
```

CAN ABLE TO ACCESS APP AND VOILA!. 
>  `Recycle` works if performed. 

1) delete deployment first, (which also deletes the POD.)
```
kubectl delete -f /path/to/deploy.yaml
```
2) now `pv` and `pvc`,
```
kubectl delete pvc pvcName
```
3) check `pv` - says `Released` then i turned to be `Available` - as per `Recycle` reclaim policy.
- Takes minutes of time
- Has to get into the backed
- To wipe and scrub all the data 
- Then provision to be used. 

Verify the cleanup by checking the data,
 - `ssh` into the `nfs` machine,
```
ls -al /var/nfs/jenkins_data
```
EMPTY!,
- Nothing but `rm -rf /var/nfs/jenkins_data/*` has happened here. 

```
kubectl delete pv pvName
```

From Bound -> Released(after deletion) -> Available. Cleans up the volume and making it available to reuse. -> After older versions - **`RECYCLE`** DOESN'T WORK BUT THE `PV` GETS CREATED THO.

BETTER SKIP **RECYCLE** POLICY. 

###### Retain:
Change - 
Keeps the volume. `persistentVolumeReclaimPolicy` parameter to `Retain`.

###### Delete:
Works only with VolumePlugins such as for AWS EBS, GCP Persistant disks, Azure disks and Open Stack center  and such, ALL the vendor lockin infra stack. (doesn't work on nfs or hostpath plugin - simply throws failed).

Simply deletes everything from filesystem to the volumes within a deletion.  
Keeps the volume. `persistentVolumeReclaimPolicy` parameter to `Retain`.



---
# 7 . Introduction to `AccessModes` for `PV` and `PVC`
Introduction to AccessModes for PV and PVC - [Doc]

![[PV's-AccessModes]]

**Understanding AccessModes for PersistantVolumes:**

You are having an administrator who is managing the Cluster, there he creates a `PV` of different types. As in locally for RWOP, or in `hostpath` for RWO or `nfs` for RWO. For whatever uses case as it is for example. All the static `PVs` for now. 

Lets say that a developer has requested to created two `PVCs` (static one), created it. 
Now, the admin has attached one of the PVC to `hostpath` and the other to the `nfs` by identical parameters that matches to get bound PVCs with PVs together. 

As per the diagram, for hostpath's PVC having Access mode of RWO, nfs's PVC have RWX.  

SAY THAT WE EVEN DEPLOYED OUR APPLICATION. 

Eg: lets say we have 2 controlplane node. And we have workloads running in it. Say there are few replicas on one WN or other few on other WN. 

Since one PVC can be used by multiple pods, but that doesn't comply with the Accessmodes. 
- PVC with RWO - node to pvc  works only on that node. 
- PVC with RWX - workloads can access PVC's running across nodes. 

Will see in brief and in practice of these Access Modes. 


---
# 8 . `AccessModes`
AccessModes - [Doc]

**Demo on Access Modes:** Docs and Refer by searching: **k8s pv**
Ref: [Persistent Volumes \| Kubernetes](https://kubernetes.io/docs/concepts/storage/persistent-volumes/#access-modes) 's 

**==Access modes - How do you want to access the data and utilize it.==** 
As per the document: 

The access modes are:
- **RWO - ReadWriteOnce**
- **ROX - ReadOnlyMany**
- **RWX - ReadWriteMany**
- **RWOP - ReadWriteOncePod**

> [!NOTE] ReadWriteOnce
> 
> the volume can be mounted as read-write by a single node. ReadWriteOnce access mode still can allow multiple pods to access (read from or write to) that volume when the pods are running on the same node. For single pod access, please see ReadWriteOncePod.

> [!NOTE] ReadOnlyMany 
> the volume can be mounted as read-only by many nodes.

> [!NOTE] ReadWriteMany
> 
> the volume can be mounted as read-write by many nodes.
> 


> [!NOTE] ReadWriteOncePod - FEATURE STATE: `Kubernetes v1.29 [stable]`
> 
> the volume can be mounted as read-write by a single Pod. Use ReadWriteOncePod access mode if you want to ensure that only one pod across the whole cluster can read that PVC or write to it.
> 

>  Note: The `ReadWriteOncePod` access mode is only supported for [CSI](https://kubernetes.io/docs/concepts/storage/volumes/#csi) volumes and Kubernetes version 1.22+. To use this feature you will need to update the following [CSI sidecars](https://kubernetes-csi.github.io/docs/sidecar-containers.html) to these versions or greater:
>  - [csi-provisioner:v3.0.0+](https://github.com/kubernetes-csi/external-provisioner/releases/tag/v3.0.0) 
>  - [csi-attacher:v3.3.0+](https://github.com/kubernetes-csi/external-attacher/releases/tag/v3.3.0)
>  - [csi-resizer:v1.3.0+](https://github.com/kubernetes-csi/external-resizer/releases/tag/v1.3.0)
> 
> And
> 
>  *Kubernetes uses volume access modes to match PersistentVolumeClaims and PersistentVolumes. In some cases, the volume access modes also constrain where the PersistentVolume can be mounted. Volume access modes do **not** enforce write protection once the storage has been mounted. Even if the access modes are specified as ReadWriteOnce  ReadOnlyMany, or ReadWriteMany, they don't set any constraints on the volume. For example, even if a PersistentVolume is created as ReadOnlyMany, it is no guarantee that it will be read-only. If the access modes are specified as ReadWriteOncePod, the volume is constrained and can be mounted on only a single Pod.*


How do i know that which access modes that can be used with which VolumePlugins? As we have refferred with [[00.Section14-K8sStorage(PV, PVC)#2 . Understanding Persistent Volume `AccessModes`]] 

*A volume can only be mounted using one access mode at a time, even if it supports many.*

|Volume Plugin|ReadWriteOnce|ReadOnlyMany|ReadWriteMany|ReadWriteOncePod|
|---|---|---|---|---|
|AzureFile|✓|✓|✓|-|
|CephFS|✓|✓|✓|-|
|CSI|depends on the driver|depends on the driver|depends on the driver|depends on the driver|
|FC|✓|✓|-|-|
|FlexVolume|✓|✓|depends on the driver|-|
|HostPath|✓|-|-|-|
|iSCSI|✓|✓|-|-|
|NFS|✓|✓|✓|-|
|RBD|✓|✓|-|-|
|VsphereVolume|✓|-|- (works when Pods are collocated)|-|
|PortworxVolume|✓|-|✓|-|
*Refer this and see, which one of the available AccessMode to use with which one of the plugin type.* which is supporting which. 

Lets see in brief, in detail and practically about all these available 
AccessModes -  **RWO - ReadWriteOnce**, **ROX - ReadOnlyMany**, **RWX - ReadWriteMany**, **RWOP - ReadWriteOncePod**

Also these can be modified and be accessed programmatically, refer docs if so. Will go there in future, as of now, this ain't the scope. 

For verifying the same setup and compliant to this [[PV's-AccessModes]] diagram, 
Will have some sort of verification identification to identify the same, 

`ssh` into all the available nodes and create dir in `/var/www/nginx-data/` and create `index.html`  and write message on it.

```
ssh username@VMmachineIP 
```
```
ls /var/tmp #check
mkdir /var/tmp/nginx-data
touch /var/tmp/nginx-data/index.html
nano /var/tmp/nginx-data/index.html #or vi
```
```
welcome to k8s nodeName #uniqueProperties for identification
```
```
cat /var/tmp/nginx-data/index.html
```


### AccessMode - Hostpath - RWO & RWX
`pv.yaml` 
```
apiVersion: v1
kind: PersistentVolume
metadata:
  name: static-pv-hostpath
  labels:
    type: hostpath
    env: prod
    release: v1.0

spec:
  storageClassName: hostpath
  capacity:
    storage: 5Gi
  accessModes:
    - ReadWriteOnce
  hostPath:
    path: "/var/tmp/nginx-data"
    type: DirectoryOrCreate
```
```
kubectl create -f /path/to/hostpathPV.yaml
kubectl get pv -o wide
```

Lil intrusive thought, what if we write a wrong access modes to the same, 

`pvc.yaml`
```
apiVersion: v1
kind: PersistentVolumeClaim
metadata:
  name: static-hostpath-pvc
spec:
  storageClassName: hostpath
  accessModes:
    - ReadWriteOnce
  resources:
    requests:
      storage: 5Gi
```
If mismatch the AccessMode for `ReadWriteOnce` -> `ReadWriteMany`? - Status - Pending. Coz,
- Incompatible AccessMode
- Mismatching paramenters (pv - rwo, pvc - rwx/StorageClass/)
- If PV < PVC in size, fails. 
- Troubleshoot more by passing `kubectl describe pvc pvcName` - ProvisioningFailed.
```
> Warning  ProvisioningFailed  3m47s (x461 over 118m)  persistentvolume-controller  storageclass.storage.k8s.io "hostpath" not found
```

To make it all work, fill all that and recreate the same,
```
kubectl force-recreate -f /path/to/pvc.yaml
```

And launch app using the PV:
```
apiVersion: v1 
kind: Pod
metadata:
  name: static-nginx-hostpathpvcpod #1,2
  labels:
    app: app
    env: prod
    release: v1.0

spec:
nodeName: wn141
containers:
    - name: app
      image: nginx:1.20.0
      ports:
        - containerPort: 80
          hostPort: 8090
          protocol: TCP
      volumeMounts:
        - name: static-hostpath-pvc
          mountPath: "/usr/share/nginx/html"
volumes:
    - name: static-hostpath-pvc
      persistentVolumeClaim:
        claimName: static-hostpath-pvc
```

```
kubectl create -f /path/to/pod.yaml
curl PodIP
```

> **PVC can be used by multiple pods.** 
> For hostPath - provisions and provides same data if it is in the same node. 
> For `NFS` - data from pods across nodes. 

To know in brief of PV,
```
kubectl describe pvc pvcName
```
Will tell you which Workloads (PODs)) are using those PVC, in a parameter :
>  `Used By:       static-nginx-hostpathpvpod`

And fo PV goes the same way, by passing the command below and parameters
```
kubectl describe pv pvName
```
```
StorageClass:    hostpath
Status:          Bound
Claim:           default/static-hostpath-pvc
Reclaim Policy:  Retain
Access Modes:    RWO
Source:
    Type:          HostPath (bare host directory volume)
    Path:          /var/tmp/nginx-data
    HostPathType:  DirectoryOrCreate
```
since it is `hostPath`, it just binds only to the pods running on to a specific node, where `pvc` can be attached with the workloads running on any nodes. 

To delete multiple pods at once (except cluster pods) from the current space. 
```
kubectl delete pv,pvc,po --all
```
Since it doesn't work if it is from the different node, lets tinker the same. 

**Also, despite of the concept of scheduling, with this we can able to command where this resource should be deployed.** - Launch Workloads (PODs) on a specific node, controlling scheduler with the help of `nodeName` parameters to comply for the convenience of **`PVs`**.

Add and specify `nodeName`, `param`,`hostpathpvpod-nodeSpecific.yaml`:
```
spec:
  nodeName: wn149
  containers:
```
And verify by: -> `curl` podIP,
```
curl podIP 
```
-> Verify with the identifier, do the same by deploying multiple workloads. 


>  Tldr: 
>  Your PV which shares the data running on the specific node. PV that was used or shared can be used only to that particular node. 

**Serving different data since it not the same PV,  despite attaching same `pvc`  to those workloads. If shared across node, would've seen the same output.** why - cause of **`RWO - Pods which are running on specific node and sharing the same volume can display the same content`**

If same PVC got used there for all the
- PODs running on that particular node they displays same content. 
- If PODs running on different nodes and uses same PVC still the PV is different for each and everynode - since it is hostpath VolumePlugin, as we have refferred in the [[PV's-AccessModes]] diagram. 

VOILA!

###### Cleanup:
```
kubectl delete pv,pvc,po --all 
```


### AccessMode - NFS - RWO & **RWX**

### Lets do all the same with deployment object across nodes to get use of this PVs. 

NFS:
And you know `deployment` object works, deploys workloads across nodes with no control of scheduler -> hostPath being hostPath, data will not be the same. 

```
apiVersion: apps/v1
kind: Deployment
metadata:
  name: nginx-deploy
  labels:
    app: nginx-app
    env: prod
    release: v1.0

spec:
  replicas: 3
  selector:
    matchLabels:
      app: nginx-app
      env: prod 
      release: v1.0

  template:
    metadata:
      name: nginx-deploy
      labels:
        app: nginx-app
        env: prod
        release: v1.0

    spec:
      containers:
      - name: write-app
        image: alpine
        command: ["/bin/sh"]
        args: ["-c", "while true; do date >> /var/log/index.html; sleep 10; done"]
        # resources:
        #   limits:
        #     cpu: 100m
        #     memory: 100Mi
        #   requests:
        #     cpu: 200m
        #     memory: 200Mi

        volumeMounts:
        - name: deploy-shared-volume
          mountPath: /var/log

      - name: serve-app
        image: nginx
        ports:
        - containerPort: 80
        # resources:
        #   limits:
        #     cpu: 100m
        #     memory: 100Mimi
        #   requests:
        #     cpu: 200m
        #     memory: 200Mi
        volumeMounts:
          - name: deploy-shared-volume
            mountPath: /usr/share/nginx/html
      volumes:  
      - name: deploy-shared-volume
        persistentVolumeClaim:
         claimName: static-hostpath-pvc
```
```
kubectl create -f /path/to/deploy.yaml
kubectl get po -o wide
```
```
curl PodIP1
curl PodIP2
 curl PodIP3
```
Since, a PV can be attached to only one PVC, where the PVC can be attached to multiple PODs.   
Each displaying different content, since all those workloads aren't running on the same node. OBVIOUS!.  - > THAT'S HOW THIS ONE WORKS. 

Here , in terms of 
##### NFS - PV, PVC:

PV:
```
apiVersion: v1
kind: PersistentVolume
metadata:
  name: static-nfs-pv
  labels:
    name: app
    # type: nfs
    env: prod
    release: v1.0
    
spec:
  # persistentVolumeReclaimPolicy: Recycle
  storageClassName: nfs
  capacity:
    storage: 3Gi #<Size> of availeble nfs server, check that using `df -h` 
  accessModes: #refer docs that which accessModes are allowed for nfs
    - ReadWriteMany #ReadWriteOnce if you want to use it for single pod, Many if multiple pods
  nfs:
    server: 192.168.0.222
    path: /var/nfs/nfspv
```

PVC:
```
apiVersion: v1
kind: PersistentVolumeClaim
metadata:
  name: static-nfs-pvc
  labels:
    name: app
    # type: nfs
    env: prod
    release: v1.0
    
spec:
  storageClassName: nfs
  accessModes:
    - ReadWriteMany
  resources:
    requests:
      storage: 3Gi
```

CREATE:
```

```




---

# 9 . Understanding `PV` phases
Understanding `PV` phases - [Doc]


---
---
---


---

Skills:
*Concepts and Practices:*
Development: Bottom up approach, Agile, DevSecOps: Shift Left approach

*Tools and Tech:*
Infrastructure and tools: Kubernetes - Kubeadm, K3s, Helm, Rancher, Portainer, Docker Compose, Podman, Buildpacks,  Cloud  - (AWS - EC2, EKS, ECS), On-Premise VMS

AWS -Services on compute, storage, databases, networking, security, developer, and
Management tools, boto3. (such services are (EC2, EKS, ECS, ECR, S3, IAM, VPC, RDS))

Systems: Linux - Debian, Ubuntu, Kali, Arch, Artix, Talos, Windows, WSL

IaC, Automation & Configuration management - Shell, Bash, Powershell, Cron, Ansible, IaC - Terraform, Pulumi  

Networking: HAProxy, Keepalived, MetalLB, ELB, Nginx, Wireguard VPN, DNS, TLS/SSL, CNI - Calico, Flannel, Cilium

CI/CD and Gitops: Jenkins, Github Actions, Gitlab, ArgoCD, Harbor, Docker Hub

Testing and Security - DevSecOps (for PreCommit checks, SAST, DAST, SCA with SBOM analysis): SonarQube, Dependency Check, Aquasec Trivy, Docker Scout, Gitleaks, OWASP ZAP, Checkmarkx, nmap, Burp suite, Wireshark and more. 

Monitoring and Observability: Prometheus, Grafana, OpenTelemetry, Uptime Kuma, rsyslog

Storage: K8s PVs/PVCs, Longhorn, Minio, CephFS, NFS, AWS ELB

Developer Tools: SCM - Git, VCS - Github, Gitlab, Gitea, API - Postman, Bruno, IDE - Vim, Neovim, VSCode

AI/MLOps Tools, Frameworks and Libraries: Kubeflow, Pandas, Jupyter Notebooks, Numpy, Langchain, N8n, OpenAI Platform.

Programming and Stack: C, PHP, Python, LAMP, MERN, HTML, CSS, JS, Bootstrap, SQL, RDBMS - MySQL, Postgres, SQLite, No-SQL - MongoDB, Apache, Nginx, REST/SOAP APIs, JSON, XML, YAML

Project Management Tools and Practices - Agile - Scrum, Jira, Confluence, Notion, Obsidian, Excalidraw

~~Looking forward to learn and upskill:~~
~~Monitoring: ELK Stack, Datadog, Splunk~~
~~SRE: SLO, SLI, SLA and others SRE related metrics.~~  
~~Security, Compliance and tools: FIPS, ISO, PCI DSS, SOCK, HIPAA - Wazuh, OpenSCAP~~

---

Strength and Good at: 
R&D, Agile. High available and resilient systems, consults and advocates for best practices, SRE who keeps up with SLI, SLAs and SLOs, prioritizes security, project management 

---

Characteristics: 
Grit, Doer, Optimistic, Passionate, Curious, Continuous Learner, Collaborative, Transparent. Proactive, Early Adopter

---

Professional Experience
DevOps/IT Engineer @Hexr Factory Immersive Tech Pvt Ltd - One Man DevOps - Chennai, TN, IN. JAN 2024 - Present  

AWS Technical Staff @ Power Centre Private Limited - Team Strength-3, Nov 2021 - Present // Chennai, TN, India
As an SME, I help consulting solutions for clients cloud requirements. I -
- Manage and provide architectural support on clients’ AWS accounts.
- Help clients migrate from on-prem to the cloud. 
- Re-architect and design robust cloud infrastructure.
- Do ops such as routine health checks, monitoring logs, remediate issues, and reports.
- Ensure security and compliance to evade failure and security risks. – perform strategic initiatives to optimize different cloud environments.
Key Takeaway – all things cloud, AWS services, cloud ops, well-architecture framework, Cost optimization, and client handling.

Recruiter - HCL @ Live connections - Jul 2021 – Oct 2021 // Chennai, TN, India 
- Sourcing profiles based on relevant Job Descriptions.
- Resume Screening and end-to-end recruitment to get people for Onboarding on various departments.
- Reaching out to candidates and processing the profiles for onboarding.
Key Takeaway – Industry Knowledge, trending and relevant technologies, a wider range of tech stacks, roles and responsibilities for relevant positions, communication.

---

Projects: 
Arrakis - My Homelab - under progress (name is by the the Dune Series) - Highly available Kubernetes homelab using K3s running self-hosted services in my local network.  (running pihole for adblocking, wireguard VPN, N8N, and some landchad services)

Mytutorialhell - my personal documentation of all my devops and other tech learnings, soon to be a documentation based tutorial under [DevOpsGround](https://github.com/rithishsamm/devopsground)
- Link: github.com/rithishsamm/myTutorialHell/ 

---

Achievements and Certification
Amazon Web Services: https://www.credly.com/users/rithish-sam-m  
- AWS Certified Cloud Practitioner (CLF - C01)
- AWS Certified Solutions Architect (SAA - C03)

---

Education:
Vels Institute of Science, Technology and Advanced Studies (VISTAS) - Pallavaram, Chennai.
Department of Science – B.Sc. (CS), - 2018 - 2021

---

Areas of Interest and Desired Roles:
Looking forward to pivot towards roles like SRE, Platform Engineering, ML, Data Engineering, Data science and such roles to get into manage cutting edge technologies and workloads at an enterprise scale   

Feel free to ask questions or let know if any more info needed to add here in my resume.

---


