Control Plane Components - [Doc]( obsidian://open?vault=tutorialHell&file=Orchestration%2Fk8engineers.com%2Fofficial%2FMaster%20Docker%20and%20Kubernetes%2FIntroduction%20to%20Kubernetes%20control%20plane%20components)
++   all the sub-components one by one individually in brief. 
#### **kube-apiserver**â˜¸ï¸
> [!NOTE]  **1) Kube-apiserver**â˜¸ï¸
>  **Apiserver** - > the first component who will receive the information or command from the administrator via kubectl.
>Communicates with each of the kube-components but the rest are all isolated and have no business to do with the rest of the components. 
>
>	All Components --- only through ---> **kube-apiserver**â˜¸ï¸
>	**kube-apiserver**â˜¸ï¸ --- can talk to --->  All components
>	
>	`there will be all the authentication and authorization in-order to communicate with each other

> *kube-apiserver* -  an API Service for K8s Cluster that runs on Control Plane node as a front end first facing component. which means,
> 	which means, whenever run or we make a request as API call via **Kubectl** , 
> 	-> the request goes first to the ***kube-apiserver*** 

####### **kube-apiserver**â˜¸ï¸ - 1) request --> 2) authenticate and authorize request --> (mutation, validation --> )3) schema validation --> 4) objects under control control --> 5) pass info to store it in **etcd** ðŸ«™

Why (these are all the valid reason that the request goes to the *kube-apiserver* first)
-- Provides ==Authentication==, ==Authorization==, ==Admission Controller== and ==Schema Validation==. (once all these checks have been passed -> request gets through -> the rest of the backend components that are running on both control plane node and worker node)
###### ==Authentication== - (validating how you are) Can be in many common form of auth such as 
- username/password (using LDAP service/server -> that can be integrated to *kube-apiserver* and can get authenticated), 
- Certificate (based auth such as SSO),
- Token (cloud, IAM) 
- and more.
###### ==Authorization== - (What are all the permissions you have) can be achieved through common practices such as 
- RBAC (role based auth), 
- Node (services such as Kubelet which has its permissions to auth) (separate nodes that would communicate to *kube-apiserver*), 
- Webhooks (via third party apps outside the cluster or in the cluster too to do webhook based auth to the cluster) 
- ~~ABAC (attribute based auth)~~ (<- DEPRECATED).
after the Authentication and Authorization gets passed. we got
###### ==Schema Validation== - will be done before creating the object on K8s Cluster, if passed resource details which has all the specs and object parameters, GETS STORED IN **etcd ðŸ«™**, + CREATE OBJECTS ON THE CLUSTER.  
(working with it after getting in) cases like creating objects and such. That way getting into clusters by including with some specific parameters regarding the specification gets validated. IF NO PARAMETERS EXIST, I'LL MOVE FORWARD WITH THE DEFAULT ONE. (some are mandatory otherwise, will exit) SO THAT, the **kube-apiserver**â˜¸ï¸ - will do the  schema validation. 

> [!NOTE] **BONUS** (Making the same setup **highly available**)
> -- ==In HA SETUP on the same== - we need to run multiple control plane nodes and make sure **kube-apiserver**â˜¸ï¸is running on each node and Loadbalance the traffic to **apiserver**â˜¸ï¸
> To set up a **Highly available K8s Cluster**, We need and should run ***multiple CONTROL PLANE NODES***. Why?
> 	- High availability
> 	- Data Redundancy
> 	- Minimal downtime
> That way, you'll end up with multiple control plane nodes. In this case, all of those **kube-apiserver**â˜¸ï¸ should be running on all of these **control plane nodes in each**.
> 
> ==**Solution:==** = **Load Balancer** ðŸŸ°
> (how will you know where to redirect the traffic to which cluster or which node towards the **kube-apiserver**â˜¸ï¸?) -
> By setting up a Load-balancer among each of the control plane nodes that communicates with the **kube-apiserver**â˜¸ï¸of each. (you cannot able to communicate  with only one when ou have multiples -> gets bloated on one side and nothing on the other) - Communication has to be distributed!
> > 	Kubectl / API CALL / USERS
> > 	Traffic --> **Load Balancer** ðŸŸ° => 
> > 	(splits/distributes (according to the spec defined) all the traffic to)  
> > 	--> **kube-apiserver**â˜¸ï¸
> > That's how it works in real time on a **HA Setup.**
> 

###### ==Admission Controlling== -  Controlling objects by the nature of K8s
> [!NOTE] **BONUS+** - NATURAL RULE OF THE CONTROL PLANE (most K8s Components) except etcd ðŸ«™
> None of the component such as controller-manager, scheduler, etcd can talk to each other. They can only be able to communicate via **kube-apiserver**â˜¸ï¸
> 
> > **kube-apiserver**â˜¸ï¸ will be the POINT OF CONTACT. Can able to Authenticate and Authorize only through **apiserver** for all the same before acknowledging all the calls and request to allow or make contact to the components that exists on the node.
> 
> Such cases like this for the communication between all the control plane components -> Once the decision has been made by the 
> > **kube-apiserver**â˜¸ï¸as per our input such as **create, delete, update** and more, to an object of K8s Cluster. Once the request gets validated or passed:
> 
> 	**kube-apiserver**â˜¸ï¸ -- passes info (commence to work with objects) --> **kubelet**ðŸ›ž (which is in worker node) -- contacts -- >  **CRI** âš™ï¸ --> creates -->  POD ðŸª£

 THE CORE, **kube-apiserver**â˜¸ï¸ itself, will be running as a **static pod** on the Control plane node controlling by kubelet service.
 > **Static Pod**?
 >  is a default fixed static pod defined the configuration which exist (a self healing nature by restoring it back to its state)

> [!NOTE]
You cannot able to control the static pod by any kubectl or REST-API call. EVEN IF YOU DO SO, 
it will restore back (also Self healing nature) -> If it gets deleted, damaged and gets downtime (intentionally or unintentionally), it'll get recreated. `this will make more sense when we cover pods (and that too about STATIC PODS`
these been made possible where the 
> > **kube-apiserver**â˜¸ï¸ itself will be running as **static pod** on the Control plane node 
> > controlled by kubelet service. 

 & the default **kube-apiserver**â˜¸ï¸port is `6443`. (should open the port on the firewall or SG)

This is the pictorial representation of the workflow that we've seen above:
![[Pasted image 20240625114137.png]]
**apirequest** -> header -> **2Auth** -> (mutation, validation) -> **Schema Validation** -> gets stored in **etcd**

1) **API Request** will go to the **HEADER**
2) Then, the request goes through the process of **Authentication and Authorization** 
3) (will cover more on **Mutation and Validation**, will cover the same in brief soon)
4) once all the auth gets passed, Will go through **Schema Validation**.
5) Once, the schema validation gets passed, The data gets stored in **etcd**.
---
#### etcd ðŸ«™
> [!NOTE] **2) etcd ðŸ«™**
>**etcd** - a distributed key-value kube datastore component
-- if exist -> throws the existing response
-- if not -> stores it and send back the response
response = a.k.a **Acknowledgement**

**etcd** (v3) is a distributed key-value pair datastore storage components for the K8s Cluster that runs on control plane nodes. the entire cluster information will be stored here.  
- Fetching data, 
- checking current status, 
- store information related to the K8s or cluster. 
eg:
- Secrets
- ConfigMaps
- No of Pods 
>  etcd ðŸ«™ is the primary default backend storage. But, this datastore can be also integrated with any other third party tool/cd or service as well. eg: AWS Console. ETCD is default recommended

++ all the information, specs and configs, actual state, default state, desired state and all.

**etcd** ðŸ«™- which runs as a static POD, ***Continuously monitors all the changes and status of the cluster and gets it all stored there.***

> Natural Rule: Should be accessible to fetch data only via **kube-apiserver**â˜¸ï¸ and nodes(also comms with **apiserver**) that need access. + has to go through **2auth** + etcdctl can be used too.

etcd can be house to implement docker overlay network across separate docker engines (without docker swarm) ðŸ¤¯

Most of the versatile tools and technologies are powered with **etcd ðŸ«™**. such as K8, CoreDNS, Rook (Ceph storage - on-prem cluster storage solution), and more.
> must take regular ***backups*** of the **etcd ðŸ«™** ON A REGULAR BASIS **==(using Jobs or Cron)==** as a part of DR strategy (when cluster gets lost). Why? the entire storage cluster is housed under this datastore, you can recover the same state without any hassle. **you dont wanna lose this.**

++
> ==**For the HA Setup:==**
need to run etcd on each of these control plane and Load balance the traffic and evenly distributes the storage. Since **etcd ðŸ«™** has to be standalone, it must talk to **LB** -> **apiserver** -> **etcd ðŸ«™**

###### Few primary commands related to ==BACKUP, STATUS CHECKS AND STORE SNAPSHOT== of **etcd ðŸ«™**
```etcd
etcdctl snapshot save snapshotName //to save
etcdctl snapshot status snapshotName //for status, can print in desired format
etcdctl snapshot restore snapshotName //to restore
```

**STEPS TO PERFORM A BACKUP and RESTORE:** 
Backup:
> Stop or Running doesn't matter. Take Snapshot

Restore: (procedural)
> 1) First, stoop all the running instances/ nodes of the **apiserver**
> 2) Restore snapshot
> 3) Restart/ start/ spin-up all the **apiserver**.

 & the default **etcd ðŸ«™** port is `2379`. 

---
#### kube-controller managerðŸ›‚
> [!NOTE] **3) kube-controller managerðŸ›‚**
> it will watch/ overlook the entire kubernetes cluster. Gather all the information about the cluster such as availability, load, status, errors, faults, downtimes, mismatch, self-heal and more.
> 
> 	In this controller manager, there are bunch of components that exists inside behind the scenes (each has its duty to its nature and perform their relevant jobs):
> 	1) Node Controller
> 	2) Job Controller
> 	3) Endpoint Slice Controller
> 	4) Service Account Controller
> 	and there are many more to it.
These are all the components that you will see on an on-premise Kubernetes Cluster. Will see all the same when we deploy the same with Kubeadm in brief

> **PURPOSE**: 
> **kube-controller manager**ðŸ›‚controls and ensures the current state of the cluster the meeting and matches its desired state (also checks and stores the same in **etcd**ðŸ«™) and takes prompt actions to maintain the same if any mismatch occurs by any possible ways.

**WORKFLOW**: eg: If something gets mismatched, 
**kube-controller manager**ðŸ›‚ --- updates Info to -->   **kube-apiserver**â˜¸ï¸ -- talks to -- > **scheduler**âŒ› -- (TAKES APPROPRIATE ACTION WORKING WITH MAINTAINING STATES OF THE COMPONENTS OR THE OBJECTS)
	eg: If POD, It gets launched/spins up on the respective node.

> The job of collecting the information and all the status of the whole K8s are gets fetched by **kube-controller manager**ðŸ›‚ not etcd, 
> **etcd**ðŸ«™simply stores it. (SINCE IT FETCHES BUT CAN'T STORE IT --> TALKS TO **kube-apiserver**â˜¸ï¸ --> TELL IT TO STORES THE SAME --> ON **etcdðŸ«™** )

Also, handles multiple controller processes to do the specific task -> t*hat together works as a single binary.* 

It is simply a component named **kube-controller manager**ðŸ›‚-> that itself contains a bunch of controllers as binaries held in it. which are 
1) Node Controller
	-- checks all the nodes are up and running or not. if it gets down, it updates the api-server and to the etcd to fetch the spec and restore the same.
	-- + it does all the checks, polling and iteration to spin the same. if it doesn't IT SHOWS THE STATUS AND THE STATUS CAN BE SEEN and stored IN THE **etcd ðŸ«™**

2) Job Controller - 
	-- SIMILARLY, CONTROLS JOBS. eg: you are trying to create a job on K8s cluster. The job has to create a pod in the backend. this controller controls the same on creating it. -> Kubelet -> Pods get created

3) Replication Controller - 
	-- Same controller maintains the replication of the Pods. If three replicas assigned? job is to maintain the 3 replicas pods for the same. If anything gets down, it'll checks the current and desired state of it. If any mismatches, it'll takes prompt action to maintain the same.
4) Service Account Controller
5)  Deployment Controller and more

ALL THE SAME APPLIES TO THE REST OF THE CONTAINER! 
**Idea behind this:** Controllers are nothing but a combination of various but specific processes. all these individual process them combined called -> **a single binary**

**kube-controller manager**ðŸ›‚ talks to **etcdðŸ«™** to *fetch the desired state of cluster* only via **kube-apiserver**â˜¸ï¸-> bypass auth to make communication within each other.
Also runs as a static pod on control plane node. Managed by **KubeletðŸ›ž** on the control plane node.

& the default **etcd ðŸ«™** port is `10257`. Port has to be open on the node, network and the firewall as well

>These are the nature and the behavior of **kube-controller manager**ðŸ›‚

---
#### cloud-controller managerâ˜ï¸
> [!NOTE] **4) cloud-controller managerâ˜ï¸**
> >This case will be the same as kube-control manager but in cloud, there will be all the components that exists relevant to the Cloud.
> 
> 	Same but cloud relevant components:
> 	1) Node Controller
> 	2) Job Controller
> 	3) Endpoint Slice Controller
> 	4) Service Account Controller
> 	and there are many more to it.
These are all the components that you will see on a cloud based Kubernetes Managed Service. Will see all the same when we deploy the same on a Cloud platform managed kubernetes services.

>!IMPORTANT
>	Can able to see the component on the control plane node 
>	*only if all the clusters are on the cloud*.
>	any managed Kubernetes services such as AKS, EKS , GKE and all. it does
>	on-prem, bare-metal, shell tools such as kubeadm, minikube means, it doesn't 

**PUPROSE:** 
(got a reason behind the statement that)  **cloud-controller managerâ˜ï¸** is a control plane component of K8's Cluster and provides access to cloud services.
WHY? My K8s might want to talk to another K8's service. They might or can get hybrid.
Services for example, such as 
-- **AWS cloud-controller managerâ˜ï¸** FOR AWS
-- **Azure cloud-controller managerâ˜ï¸** for Azure and more
Purpose is same but the implementation and the platform varies.

It will ensure that **current state** is matched with the **desired state** stored in etcd and makes necessary decision if mismatched on the cloud.
++ it handles multiple controller processes together to do the specific task and work together as a single binary.
(the/my **cloud-controller managerâ˜ï¸** is a single binary) ni them, we got particular controller processes. 

What are all the controller processes:
1) Route Controller - 
2) Service Controller
3) Node Controller
4) Job Controller
5) Deployment Controller 
6) Node IPAM Controller
7) Volume Controller and more.
	all those same controller that exists on the **kube-controller manager**ðŸ›‚exists here too! but with more of cloud related controller process binaries too!. to enable control and communication between the managed service on the cloud and the admin

 as same as the rest, **cloud-controller managerâ˜ï¸** talks to etcd to fetch the desired state of the cluster only via **kube-apiserver**â˜¸ï¸ after bypassing all the 2auth

>& this will be also running as a static pod and the default port of it is also as same as kube-controller-manager `10257`

---
#### kube-scheduler**âŒ›
> [!NOTE] **5) kube-scheduler**âŒ›
> validates each worker node, pods in each node, picks the available & suitable pod (for relevant config). schedules deployment).
> 
> 	in this, there are two major components exist inside the same.
> 	1) Filtering
> 	2) Scoring

PURPOSE:
**is to schedule the PODS** for whatever applied reason based the K8s present logic. 
eg: deploying some particular POD which contains any app, workload or resources.
if nothing, no node gets specified in any of it
> **kube-scheduler**âŒ›steps in decide based the all the various logics factors to which node it has to be scheduled to get deployed 

***DECISION TAKER OF WHAT POD SHOULD BE RUNNING ON WHAT NODE BASED ON ALL THE RELEVANT FACTORS***

ensure the PODs which are not assigned to any node **(like pending pods)** will be scheduled by the **kube-scheduler**âŒ›

>==ALL OF THESE DECISION IS MADE BASED ON SEVERAL FACTORS SUCH AS 
>	1) HARDWARE,
>	2) SOFTWARE,
>	3) RESOURCE UTILIZATION,
>	4) LOAD, 
>	5) TRAFFIC, 
>	6) NETWORK, 
>	7) AVAILABILITY,  
>	8) TAINTS AND TOLERATIONS,
>	9) NODE NAME,
>	10) NODE SELECTOR   
>	11) AFFINITIES SUCH AS NODE AFFINITY,
>	12) NODE ANTI-AFFINITY 
>	13) POD AFFINITY, and more.==
>HANDLED BY SCHEDULER TO TAKE DECISIONS

eg: I want to launch a POD on a specific node. that will be done by affinity itself.
what if I haven't written any affinity and directly scheduling my pod -> decision gets taken by **kube-scheduler**âŒ›itself.


If no node will be available for the pods to get deployed, the PODS status will be shown as **PENDING**.
All Things will make sense when we see all these in practical sessions. See why shows pending and when we cover about taints and tolerations as well!

Scheduling will be done by two primary action factors:
- Filtering 
- Scoring
eg: We have some number of Worker Nodes. The pods cannot be scheduled on all of these ports right? Only one node gets selected to scheduled for the deployment. 

**FILTERING:**
how that particular node got selected/picked up by scheduler?
By **filtering**, filtering how? Based on 
1) Resource Consumption
2) Resource Utilized
3) Available resource left
4) What the Pod is looking for
5) How much resource will get consumed by the pending POD
6) Is it feasible and efficient or not?
all of these types of decision will comes under **filtering**

**SCORING**
After then? We got some hand full couple of qualified nodes in hand to be deployed.
Those nodes, each have its score! Based on the scoring, which one got the highest. That node will be taken to consideration to deploy it on the pod.
> The two major functionalities  of the **kube-scheduler**âŒ›will use in the backend to schedule a POD on one particular single node.

Default scheduler: **kube-scheduler**âŒ› why?
++ we can also integrate and use multiple scheduler in the cluster and specify which should be used while scheduling the objects like PODS to worker nodes.

To choose the scheduler, it has to be specified   while scheduling the objects. If nothing specified, proceeds with the default **kube-scheduler**âŒ›. this will also run as a static pod on a control plane node.

> & the default port number for the scheduler is `10259`
