### Section 1:K8S Architecture
#### 1.  High level view of Kubernetes architecture and components
###### Architecture and Components - [Doc](obsidian://open?vault=tutorialHell&file=Orchestration%2Fk8engineers.com%2Fofficial%2FMaster%20Docker%20and%20Kubernetes%2FCommon%20components%20for%20Control%20plane%20and%20Compute%20plane%20nodes)
#### 2.  Common components for control and compute plane nodes
###### Common Components -  [Doc](obsidian://open?vault=tutorialHell&file=Orchestration%2Fk8engineers.com%2Fofficial%2FMaster%20Docker%20and%20Kubernetes%2FCommon%20components%20for%20Control%20plane%20and%20Compute%20plane%20nodes)
#### 3.  Introduction to Kubernetes control plane components
###### Control Plane Components - [Doc]( obsidian://open?vault=tutorialHell&file=Orchestration%2Fk8engineers.com%2Fofficial%2FMaster%20Docker%20and%20Kubernetes%2FIntroduction%20to%20Kubernetes%20control%20plane%20components)
#### 4.  Kubernetes control plane components working principle
#### 5.  Introduction to Kubernetes compute plane components
#### 6.  Kubernetes compute plane components working principle
#### 7.  Kubernetes Components ports and protocols
----
## 1.  High level view of Kubernetes architecture and components
Architecture and Components - [Doc](obsidian://open?vault=tutorialHell&file=Orchestration%2Fk8engineers.com%2Fofficial%2FMaster%20Docker%20and%20Kubernetes%2FCommon%20components%20for%20Control%20plane%20and%20Compute%20plane%20nodes)

**Kubernetes Architecture in brief:** For
1) Single Node &
2) Multi Node

Here, the **single node** is that
	1. control plane & components + 1 compute/worker components = runs on the ***same** One single node.*
the same works like this (in the below given image)
![[k8sArchitecture1.excalidraw]]

> Good for practicing and experimentation purposes when you lack in resource

**multi node** (irl, we use two on more than that)
	2.  all of those nodes runs on its own node. 
	Control plane works on control plane node, compute plane works on compute plane node.
the same works like this (in the below given image)

![[k8sArchitecture2.excalidraw]]

Control Plane Node:
- etcd
- Kube-API-server
- Kube-Scheduler
- Kube/Cloud-Controller manager
> Production purposes, In real time workloads.   

Compute/Worker Plane Node:
- Kubelet
- Kube-proxy
- CRI-containerd -> 
- + Pods

> [!NOTE: when you refer K8's Architecture]
> Theorotically, we differentiate the nodes technically as its own, but
> Practically speaking, them components works on the same node 
![[k8sArchitecture1.excalidraw]]
Plus, you can refer the same here
.
Also, you can see in the diagram that, there are three nodes -
called - **HA (High Availability) SETUP.**

Multi nodes configured in a manner name HA Setup.
**HIGH AVAILABILTY = 3 OR 5 OR MORE CONTROL Plane gets running**. This get managed by LOAD BALANCER.
###### WHY HA Setup?
if there is one control particular control plane is running and the work or a particular job is down, the rest (the application) would still exist and will keep on working.

if IRL, Prod Env with multi-node
**Kubectl**   -- *req* --> **LoadBalancer** (gets distributed)-- *req* -->  **apiserver**
> **HIGH AVAILABLITY SETUP**
>will the more on the same in practice.

> THESE ARE ALL THE HIGH LEVEL OVEREVIEW OF THE POINTS THAT WE'VE COVERED.
So far, covered the architectures of -
1) Single node setup
2) Multi node setup
3) High Availability (HA) Setup

How to communicate or setup and configure such environment:
###### **Kubectl** - the way admins can interact with K8s clusters - Admins 👨‍🏭
-- deploy applications
-- administrate them
-- develop on the same

when the users try to access the application -> they get redirected to 
###### Kube-proxy - users gets here 👤
takes the responsibility of sending the traffic to the relevant/appropriate application.
running as a container inside the kubernetes cluster. **This also runs as a container inside the k8s cluster.**

this is the high-level overview so far. In detail, refer the diagram given below.
![[highlevel-k8sArchitecture.excalidraw]]


**USECASE**:


**Objective**: As an administrator, you have three pods which are created logically. As an admin, **you want to spin up three pods** like the same as the diagram.

> [!NOTE] ***POD***
> >POD: one or a group of containers

**Action**:
Where the administrator's request will be sent?

#### CONTROL PLANE
👤(spin three pods) **kubectl** 🐚  --- *req* ---> LB -- *req* --> **kube-Apiserver** ☸️

> [!NOTE] ***API-SERVER***
>  ***Apiserver*** - > the first component who will receive the information or command from the administrator via kubectl.
>Communicates with each of the kube-components but the rest are all isolated and have no business to do with the rest of the components. 
>
>	All Components --- only through ---> **kube-apiserver**☸️
>	**kube-apiserver**☸️ --- can talk to --->  All components
>	
>	`there will be all the authentication and authorization in-order to communicate with each other`

**kube-apiserver**☸️--*req*--> **etcd** 🫙 (checks the request  exist or not) --*res*-> **kube-apiserver** ☸️

> [!NOTE] ***etcd***
>***etcd*** - a distributed key-value kube datastore component
-- if exist -> throws the existing response
-- if not -> stores it and send back the response
response = a.k.a **Acknowledgement**

**kube-apiserver**☸️(decide where to launch the app) --*pass Info* --> **kube-scheduler** ⌛ (validates and schedules job) -- *reply* --> **kube-apiserver**☸️
> [!NOTE] ***kube-scheduler***
> validates each worker node, pods in each node, picks the available & suitable pod (for relevant config). schedules deployment)

**kube-apiserver**☸️(datastores the config)-->*sends*--> **etcd**🫙-->*acknowledges* -->**kube-apiserver**☸️

**kube-apiserver**☸️--> *launches the pod* --> (available) **Kubelet**🛞 [on Worker node] (which was decided by the **scheduler**)

then, what is the purpose of Control manager?
**control-manager** 🛂--> updates and commands all the info of the cluster to  -->  **kube-apiserver**☸️ (--> that stores the same to etcd🫙)
> [!NOTE] ***kube-controller manager***
> it will watch/ overlook the entire kubernetes cluster. Gather all the information about the cluster such as availability, load, status, errors, faults, downtimes, mismatch, self-heal and more.

#### WORKER NODE / COMPUTE PLANE
who receive all these commands and information of all these configuration get defined within these nodes and clusters.

**kube-apiserver**☸️ -- (*config*) --> **kubelet**🛞 (receives info)
> [!NOTE] ***Kubelet***
>  Who should be the one who takes input from **kube-apiserver**☸️ to create container,  **kubelet**🛞 not to the CRI itself
>  
> >  **kube-apiserver**☸️ -> input --> **kubelet**🛞  --> **CRI** ⚙️ --> creates -->  POD 🪣

**kubelet**🛞-- *talks to* --> **CRI** ⚙️(container runtime interface) --> POD 🪣 (creates them in the backend)

> [!NOTE] ***CRI***
> CRI - Container Runtime Interface. K8s doesnt create containers in the backend. its is a platform. Container Runtime does! 
These Container runtime interfaces talks to the CONTAINER ENGINE WHICHEVER THAT WORKS ON THE SAME. such as Containerd, Docker-Engine, CRI-O

> WILL SEE MORE ON THE SAME CONTROL PLANE NODE AND COMPUTE PLANE WORKER NODE AS SEPERATE IN DETAIL and see what are all the responsibility of each of these components briefly.

---
## 2.  Common components for control and compute plane nodes
Common Components -  [Doc](obsidian://open?vault=tutorialHell&file=Orchestration%2Fk8engineers.com%2Fofficial%2FMaster%20Docker%20and%20Kubernetes%2FCommon%20components%20for%20Control%20plane%20and%20Compute%20plane%20nodes)

Covered the Architecture. Will see what are all the components that are available to work with K8s in brief on both CONTROL PLANE NODE AND COMPUTE PLANE WORKER NODE
Naming convention: To the context that called 
Control Plane -  Master node
Compute Plane - Worker node

###### Common components on BOTH CONTROL PLANE NODE + COMPUTE WORKER PLANE NODE
just for all the recap, copying and referring the same.=
> [!NOTE] **1) CRI ⚙️**
> CRI - Container Runtime Interface. K8s doesnt create containers in the backend. its is a platform. Container Runtime does! 
These Container runtime interfaces talks to the CONTAINER ENGINE WHICHEVER THAT WORKS ON THE SAME. such as Containerd, Docker-Engine, CRI-O

> Containerd -> Recommended and Working in real time. 

> [!NOTE] **2) Kubelet 🛞**
>  Who should be the one who takes input from **kube-apiserver**☸️ to create container,  **kubelet**🛞 not to the CRI itself
>  
>  > **kube-apiserver**☸️ -> input --> **kubelet**🛞  --> **CRI** ⚙️ --> creates -->  POD 🪣

> Kubelet will be running on both **CONTROL PLANE** AND **WORKER NODE** AS WELL. (client -server model)

++ what else?
> [!NOTE] **3) kube-proxy 🔀**
> a proxy server which takes care handling the network managing things such as communication between the Pods and traffic. Kube-proxy knows how to send/redirect the traffic where it has to point to.

User 👤 -->  Node  --> **kube-proxy** 🔀 (redirects) -- PODS
 
> COMPONENTS COMMON ON BOTH OF THESE NODES.

---
## 3. + 4.  Introduction to Kubernetes Control plane (Master Node) components and its Working Principles: 
Control Plane Components - [Doc]( obsidian://open?vault=tutorialHell&file=Orchestration%2Fk8engineers.com%2Fofficial%2FMaster%20Docker%20and%20Kubernetes%2FIntroduction%20to%20Kubernetes%20control%20plane%20components)
++   all the sub-components one by one individually in brief. 

#### **kube-apiserver**☸️
> [!NOTE]  **1) Kube-apiserver**☸️
>  **Apiserver** - > the first component who will receive the information or command from the administrator via kubectl.
>Communicates with each of the kube-components but the rest are all isolated and have no business to do with the rest of the components. 
>
>	All Components --- only through ---> **kube-apiserver**☸️
>	**kube-apiserver**☸️ --- can talk to --->  All components
>	
>	`there will be all the authentication and authorization in-order to communicate with each other

> *kube-apiserver* -  an API Service for K8s Cluster that runs on Control Plane node as a front end first facing component. which means,
> 	which means, whenever run or we make a request as API call via **Kubectl** , 
> 	-> the request goes first to the ***kube-apiserver*** 

####### **kube-apiserver**☸️ - 1) request --> 2) authenticate and authorize request --> (mutation, validation --> )3) schema validation --> 4) objects under control control --> 5) pass info to store it in **etcd** 🫙

Why (these are all the valid reason that the request goes to the *kube-apiserver* first)
-- Provides ==Authentication==, ==Authorization==, ==Admission Controller== and ==Schema Validation==. (once all these checks have been passed -> request gets through -> the rest of the backend components that are running on both control plane node and worker node)
###### ==Authentication== - (validating how you are) Can be in many common form of auth such as 
- username/password (using LDAP service/server -> that can be integrated to *kube-apiserver* and can get authenticated), 
- Certificate (based auth such as SSO),
- Token (cloud, IAM) 
- and more.
###### ==Authorization== - (What are all the permissions you have) can be achieved through common practices such as 
- RBAC (role based auth), 
- Node (services such as Kubelet which has its permissions to auth) (separate nodes that would communicate to *kube-apiserver*), 
- Webhooks (via third party apps outside the cluster or in the cluster too to do webhook based auth to the cluster) 
- ~~ABAC (attribute based auth)~~ (<- DEPRECATED).
after the Authentication and Authorization gets passed. we got
###### ==Schema Validation== - will be done before creating the object on K8s Cluster, if passed resource details which has all the specs and object parameters, GETS STORED IN **etcd 🫙**, + CREATE OBJECTS ON THE CLUSTER.  
(working with it after getting in) cases like creating objects and such. That way getting into clusters by including with some specific parameters regarding the specification gets validated. IF NO PARAMETERS EXIST, I'LL MOVE FORWARD WITH THE DEFAULT ONE. (some are mandatory otherwise, will exit) SO THAT, the **kube-apiserver**☸️ - will do the  schema validation. 

> [!NOTE] **BONUS** (Making the same setup **highly available**)
> -- ==In HA SETUP on the same== - we need to run multiple control plane nodes and make sure **kube-apiserver**☸️is running on each node and Loadbalance the traffic to **apiserver**☸️
> To set up a **Highly available K8s Cluster**, We need and should run ***multiple CONTROL PLANE NODES***. Why?
> 	- High availability
> 	- Data Redundancy
> 	- Minimal downtime
> That way, you'll end up with multiple control plane nodes. In this case, all of those **kube-apiserver**☸️ should be running on all of these **control plane nodes in each**.
> 
> ==**Solution:==** = **Load Balancer** 🟰
> (how will you know where to redirect the traffic to which cluster or which node towards the **kube-apiserver**☸️?) -
> By setting up a Load-balancer among each of the control plane nodes that communicates with the **kube-apiserver**☸️of each. (you cannot able to communicate  with only one when ou have multiples -> gets bloated on one side and nothing on the other) - Communication has to be distributed!
> > 	Kubectl / API CALL / USERS
> > 	Traffic --> **Load Balancer** 🟰 => 
> > 	(splits/distributes (according to the spec defined) all the traffic to)  
> > 	--> **kube-apiserver**☸️
> > That's how it works in real time on a **HA Setup.**
> 

###### ==Admission Controlling== -  Controlling objects by the nature of K8s
> [!NOTE] **BONUS+** - NATURAL RULE OF THE CONTROL PLANE (most K8s Components) except etcd 🫙
> None of the component such as controller-manager, scheduler, etcd can talk to each other. They can only be able to communicate via **kube-apiserver**☸️
> 
> > **kube-apiserver**☸️ will be the POINT OF CONTACT. Can able to Authenticate and Authorize only through **apiserver** for all the same before acknowledging all the calls and request to allow or make contact to the components that exists on the node.
> 
> Such cases like this for the communication between all the control plane components -> Once the decision has been made by the 
> > **kube-apiserver**☸️as per our input such as **create, delete, update** and more, to an object of K8s Cluster. Once the request gets validated or passed:
> 
> 	**kube-apiserver**☸️ -- passes info (commence to work with objects) --> **kubelet**🛞 (which is in worker node) -- contacts -- >  **CRI** ⚙️ --> creates -->  POD 🪣

 THE CORE, **kube-apiserver**☸️ itself, will be running as a **static pod** on the Control plane node controlling by kubelet service.
 > **Static Pod**?
 >  is a default fixed static pod defined the configuration which exist (a self healing nature by restoring it back to its state)

> [!NOTE]
You cannot able to control the static pod by any kubectl or REST-API call. EVEN IF YOU DO SO, 
it will restore back (also Self healing nature) -> If it gets deleted, damaged and gets downtime (intentionally or unintentionally), it'll get recreated. `this will make more sense when we cover pods (and that too about STATIC PODS`
these been made possible where the 
> > **kube-apiserver**☸️ itself will be running as **static pod** on the Control plane node 
> > controlled by kubelet service. 

 & the default **kube-apiserver**☸️port is `6443`. (should open the port on the firewall or SG)

This is the pictorial representation of the workflow that we've seen above:
![[highlevel-kubeapiUnderTheHood.excalidraw]]
**apirequest** -> header -> **2Auth** -> (mutation, validation) -> **Schema Validation** -> gets stored in **etcd**

1) **API Request** will go to the **HEADER**
2) Then, the request goes through the process of **Authentication and Authorization** 
3) (will cover more on **Mutation and Validation**, will cover the same in brief soon)
4) once all the auth gets passed, Will go through **Schema Validation**.
5) Once, the schema validation gets passed, The data gets stored in **etcd**.
---
#### etcd 🫙
> [!NOTE] **2) etcd 🫙**
>**etcd** - a distributed key-value kube datastore component
-- if exist -> throws the existing response
-- if not -> stores it and send back the response
response = a.k.a **Acknowledgement**

**etcd** (v3) is a distributed key-value pair datastore storage components for the K8s Cluster that runs on control plane nodes. the entire cluster information will be stored here.  
- Fetching data, 
- checking current status, 
- store information related to the K8s or cluster. 
eg:
- Secrets
- ConfigMaps
- No of Pods 
>  etcd 🫙 is the primary default backend storage. But, this datastore can be also integrated with any other third party tool/cd or service as well. eg: AWS Console. ETCD is default recommended

++ all the information, specs and configs, actual state, default state, desired state and all.

**etcd** 🫙- which runs as a static POD, ***Continuously monitors all the changes and status of the cluster and gets it all stored there.***

> Natural Rule: Should be accessible to fetch data only via **kube-apiserver**☸️ and nodes(also comms with **apiserver**) that need access. + has to go through **2auth** + etcdctl can be used too.

etcd can be house to implement docker overlay network across separate docker engines (without docker swarm) 🤯

Most of the versatile tools and technologies are powered with **etcd 🫙**. such as K8, CoreDNS, Rook (Ceph storage - on-prem cluster storage solution), and more.
> must take regular ***backups*** of the **etcd 🫙** ON A REGULAR BASIS **==(using Jobs or Cron)==** as a part of DR strategy (when cluster gets lost). Why? the entire storage cluster is housed under this datastore, you can recover the same state without any hassle. **you dont wanna lose this.**

++
> ==**For the HA Setup:==**
need to run etcd on each of these control plane and Load balance the traffic and evenly distributes the storage. Since **etcd 🫙** has to be standalone, it must talk to **LB** -> **apiserver** -> **etcd 🫙**

###### Few primary commands related to ==BACKUP, STATUS CHECKS AND STORE SNAPSHOT== of **etcd 🫙**
```etcd
etcdctl snapshot save snapshotName //to save
etcdctl snapshot status snapshotName //for status, can print in desired format
etcdctl snapshot restore snapshotName //to restore
```

**STEPS TO PERFORM A BACKUP and RESTORE:** 
Backup:
> Stop or Running doesn't matter. Take Snapshot

Restore: (procedural)
> 1) First, stoop all the running instances/ nodes of the **apiserver**
> 2) Restore snapshot
> 3) Restart/ start/ spin-up all the **apiserver**.

 & the default **etcd 🫙** port is `2379`. 

---
#### kube-controller manager🛂
> [!NOTE] **3) kube-controller manager🛂**
> it will watch/ overlook the entire kubernetes cluster. Gather all the information about the cluster such as availability, load, status, errors, faults, downtimes, mismatch, self-heal and more.
> 
> 	In this controller manager, there are bunch of components that exists inside behind the scenes (each has its duty to its nature and perform their relevant jobs):
> 	1) Node Controller
> 	2) Job Controller
> 	3) Endpoint Slice Controller
> 	4) Service Account Controller
> 	and there are many more to it.
These are all the components that you will see on an on-premise Kubernetes Cluster. Will see all the same when we deploy the same with Kubeadm in brief

> **PURPOSE**: 
> **kube-controller manager**🛂controls and ensures the current state of the cluster the meeting and matches its desired state (also checks and stores the same in **etcd**🫙) and takes prompt actions to maintain the same if any mismatch occurs by any possible ways.

**WORKFLOW**: eg: If something gets mismatched, 
**kube-controller manager**🛂 --- updates Info to -->   **kube-apiserver**☸️ -- talks to -- > **scheduler**⌛ -- (TAKES APPROPRIATE ACTION WORKING WITH MAINTAINING STATES OF THE COMPONENTS OR THE OBJECTS)
	eg: If POD, It gets launched/spins up on the respective node.

> The job of collecting the information and all the status of the whole K8s are gets fetched by **kube-controller manager**🛂 not etcd, 
> **etcd**🫙simply stores it. (SINCE IT FETCHES BUT CAN'T STORE IT --> TALKS TO **kube-apiserver**☸️ --> TELL IT TO STORES THE SAME --> ON **etcd🫙** )

Also, handles multiple controller processes to do the specific task -> t*hat together works as a single binary.* 

It is simply a component named **kube-controller manager**🛂-> that itself contains a bunch of controllers as binaries held in it. which are 
1) Node Controller
	-- checks all the nodes are up and running or not. if it gets down, it updates the api-server and to the etcd to fetch the spec and restore the same.
	-- + it does all the checks, polling and iteration to spin the same. if it doesn't IT SHOWS THE STATUS AND THE STATUS CAN BE SEEN and stored IN THE **etcd 🫙**

2) Job Controller - 
	-- SIMILARLY, CONTROLS JOBS. eg: you are trying to create a job on K8s cluster. The job has to create a pod in the backend. this controller controls the same on creating it. -> Kubelet -> Pods get created

3) Replication Controller - 
	-- Same controller maintains the replication of the Pods. If three replicas assigned? job is to maintain the 3 replicas pods for the same. If anything gets down, it'll checks the current and desired state of it. If any mismatches, it'll takes prompt action to maintain the same.
4) Service Account Controller
5)  Deployment Controller and more

ALL THE SAME APPLIES TO THE REST OF THE CONTAINER! 
**Idea behind this:** Controllers are nothing but a combination of various but specific processes. all these individual process them combined called -> **a single binary**

**kube-controller manager**🛂 talks to **etcd🫙** to *fetch the desired state of cluster* only via **kube-apiserver**☸️-> bypass auth to make communication within each other.
Also runs as a static pod on control plane node. Managed by **Kubelet🛞** on the control plane node.

& the default **etcd 🫙** port is `10257`. Port has to be open on the node, network and the firewall as well

>These are the nature and the behavior of **kube-controller manager**🛂

---
#### cloud-controller manager☁️
> [!NOTE] **4) cloud-controller manager☁️**
> >This case will be the same as kube-control manager but in cloud, there will be all the components that exists relevant to the Cloud.
> 
> 	Same but cloud relevant components:
> 	1) Node Controller
> 	2) Job Controller
> 	3) Endpoint Slice Controller
> 	4) Service Account Controller
> 	and there are many more to it.
These are all the components that you will see on a cloud based Kubernetes Managed Service. Will see all the same when we deploy the same on a Cloud platform managed kubernetes services.

>!IMPORTANT
>	Can able to see the component on the control plane node 
>	*only if all the clusters are on the cloud*.
>	any managed Kubernetes services such as AKS, EKS , GKE and all. it does
>	on-prem, bare-metal, shell tools such as kubeadm, minikube means, it doesn't 

**PUPROSE:** 
(got a reason behind the statement that)  **cloud-controller manager☁️** is a control plane component of K8's Cluster and provides access to cloud services.
WHY? My K8s might want to talk to another K8's service. They might or can get hybrid.
Services for example, such as 
-- **AWS cloud-controller manager☁️** FOR AWS
-- **Azure cloud-controller manager☁️** for Azure and more
Purpose is same but the implementation and the platform varies.

It will ensure that **current state** is matched with the **desired state** stored in etcd and makes necessary decision if mismatched on the cloud.
++ it handles multiple controller processes together to do the specific task and work together as a single binary.
(the/my **cloud-controller manager☁️** is a single binary) ni them, we got particular controller processes. 

What are all the controller processes:
1) Route Controller - 
2) Service Controller
3) Node Controller
4) Job Controller
5) Deployment Controller 
6) Node IPAM Controller
7) Volume Controller and more.
	all those same controller that exists on the **kube-controller manager**🛂exists here too! but with more of cloud related controller process binaries too!. to enable control and communication between the managed service on the cloud and the admin

 as same as the rest, **cloud-controller manager☁️** talks to etcd to fetch the desired state of the cluster only via **kube-apiserver**☸️ after bypassing all the 2auth

>& this will be also running as a static pod and the default port of it is also as same as kube-controller-manager `10257`

---
#### kube-scheduler**⌛
> [!NOTE] **5) kube-scheduler**⌛
> validates each worker node, pods in each node, picks the available & suitable pod (for relevant config). schedules deployment).
> 
> 	in this, there are two major components exist inside the same.
> 	1) Filtering
> 	2) Scoring

PURPOSE:
**is to schedule the PODS** for whatever applied reason based the K8s present logic. 
eg: deploying some particular POD which contains any app, workload or resources.
if nothing, no node gets specified in any of it
> **kube-scheduler**⌛steps in decide based the all the various logics factors to which node it has to be scheduled to get deployed 

***DECISION TAKER OF WHAT POD SHOULD BE RUNNING ON WHAT NODE BASED ON ALL THE RELEVANT FACTORS***

ensure the PODs which are not assigned to any node **(like pending pods)** will be scheduled by the **kube-scheduler**⌛

>==ALL OF THESE DECISION IS MADE BASED ON SEVERAL FACTORS SUCH AS 
>	1) HARDWARE,
>	2) SOFTWARE,
>	3) RESOURCE UTILIZATION,
>	4) LOAD, 
>	5) TRAFFIC, 
>	6) NETWORK, 
>	7) AVAILABILITY,  
>	8) TAINTS AND TOLERATIONS,
>	9) NODE NAME,
>	10) NODE SELECTOR   
>	11) AFFINITIES SUCH AS NODE AFFINITY,
>	12) NODE ANTI-AFFINITY 
>	13) POD AFFINITY, and more.==
>HANDLED BY SCHEDULER TO TAKE DECISIONS

eg: I want to launch a POD on a specific node. that will be done by affinity itself.
what if I haven't written any affinity and directly scheduling my pod -> decision gets taken by **kube-scheduler**⌛itself.


If no node will be available for the pods to get deployed, the PODS status will be shown as **PENDING**.
All Things will make sense when we see all these in practical sessions. See why shows pending and when we cover about taints and tolerations as well!

Scheduling will be done by two primary action factors:
- Filtering 
- Scoring
eg: We have some number of Worker Nodes. The pods cannot be scheduled on all of these ports right? Only one node gets selected to scheduled for the deployment. 

**FILTERING:**
how that particular node got selected/picked up by scheduler?
By **filtering**, filtering how? Based on 
1) Resource Consumption
2) Resource Utilized
3) Available resource left
4) What the Pod is looking for
5) How much resource will get consumed by the pending POD
6) Is it feasible and efficient or not?
all of these types of decision will comes under **filtering**

**SCORING**
After then? We got some hand full couple of qualified nodes in hand to be deployed.
Those nodes, each have its score! Based on the scoring, which one got the highest. That node will be taken to consideration to deploy it on the pod.
> The two major functionalities  of the **kube-scheduler**⌛will use in the backend to schedule a POD on one particular single node.

Default scheduler: **kube-scheduler**⌛ why?
++ we can also integrate and use multiple scheduler in the cluster and specify which should be used while scheduling the objects like PODS to worker nodes.

To choose the scheduler, it has to be specified   while scheduling the objects. If nothing specified, proceeds with the default **kube-scheduler**⌛. this will also run as a static pod on a control plane node.

> & the default port number for the scheduler is `10259`

---
## 4. + 5.  Introduction to Kubernetes compute plane worker node components and its Working Principles

###### Common components on COMPUTE WORKER PLANE 
just for all the recap, copying and referring the same.=
![[k8sArchitecture3.excalidraw]]

#### Kubelet🛞
> [!NOTE] **1) Kubelet 🛞**
> is the one who is responsible to create all the PODS and the containers in the backend not by itself but who that gets the input from the **apiserver**!
>  > **kube-apiserver**☸️ -> input --> **kubelet**🛞  --> **CRI** ⚙️ --> creates -->  POD 🪣
> 
> note: Kubelet will be running on both **CONTROL PLANE** AND **WORKER NODE** AS WELL. (client -server model)
>********
Who should be the one who takes input from **kube-apiserver**☸️ to create container,  **kubelet**🛞 not to the CRI itself

**kubelet** - will be running on both control and compute plane as well. 
> ***logically speaking***. At the end of the day, all of the components are simply a static pod, which is obviously created, managed and controlled directly by the kubelet. SIMPLE!

This component will be running as a **daemon service** on all the nodes in the cluster not a static pod. obviously!
> **logically speaking** kubelet is to be the one who is supposed to be create PODS, how a POD creator can be end up as a static POD who creates it by themselves?

All the control plane nodes are STATIC PODS but not the Worker node Pods which are 
**daemon-service** -> is a service running on the node in the backend. 

OKAY! BIG QUESTION?
How will i be able to run if the **kubelet**🛞is a **daemon-service? ==Systemctl==**

>Once the decision has been made and the POD gets scheduled on a node where the input of this came from **kube-apiserver**☸️,
> > **kubelet**🛞 service will makes sure all the PODS and Containers gets launched for the POD by 
> > `pulling images --> create/build the same with the image + the app -->  start/spin up/launch the containers.
On the node, where the POD has been scheduled.

Here, the daemon service of it (a binary or a daemon system itself) has to have a **workdir** or **rootdir** to make this thing all work? How that works? Here, the naming convention of it named as

> 	**DATA DIRECTORY**:
> the kubelet config files will be stored under `/var/lib/kubelet`
even all the POD + Kubelet related information within itself will get stored under this **data directory**. a.k.a ==**kubelet data directory**==

**==BACKUPS FOR THESE ARE MUST AS WELL.==** If needed.

  
> [!NOTE] !IMPORTANT
> **Static PODS** are controlled by Kubelet service which is a **daemon** by itself
> (apart from PODS scheduled by **apiserver**) 
> -> that includes the Control Plane components too!
> 
Instead of using REST API calls, to launch pods with the help of **kube-apiserver**☸️, we use Who should be the one who takes input from **kube-apiserver**☸️ we use **kubelet**🛞 service itself not by **apiserver**
.
PODS like such which was created and controlled by the **kubectl** but not with **apiserver** which are static and you can do nothing on it. -> THOSE ARE CALLED AS 
> > **STATIC PODS**
>  
what i mean by nothing means?
  even if you ***try to delete the POD*** means, The Static PODs will get **auto-spined up**
> > BECAUSE IT IS CONTROLLED BY **KUBELET🛞**

When the POD gets failed? what happens BTS?
**KUBELET🛞** will restart the POD. THE CONTAINER GETS RECREATED/RESTART `SELF HEALING`
BTS -> Kubelet will take care of it. RESTART = RECREATING THE CONTAINER IN THE BACKEND!

 >even the storage of the POD Workload is controlled/managed by **KUBELET🛞**
 eg: volumes with NFS, hostpath, volume binding and more, THERE **KUBELET🛞** takes care of it too creating the particular volume/storage for your workload/POD.

& default port of **KUBELET🛞** will be `10250`.
IF SOMEONE WANTS TO COMMUNICATE WITH MY **KUBELET🛞** SERVICE. LISTENS TO THIS PORT. 

> ==**PORT HAS TO BE OPENED ON BOTH OF THE NODES==** = CONTROL PLANE + WORKER NODE too.

---
#### CRI⚙️
> [!NOTE] **2) CRI ⚙️**
> CONTAINER RUNTIME INTERFACES? -> What are all that?
> 1) Containerd
> 2) docker
> 3) cri-o and more.
> > will be using **containerd** for the same to stay relevant
>********
> CRI - Container Runtime Interface. K8s doesn't create containers in the backend. its is a platform. Container Runtime does! 
These Container runtime interfaces talks to the CONTAINER ENGINE WHICHEVER THAT WORKS ON THE SAME. such as Containerd, Docker-Engine, CRI-O
> > -- + IT EXISTS ALL THE NODES BOTH CONTROL PLANE AND COMPUTE PLANE NODES INSIDE THE CLUSTER.

**CRI - CONTAINER RUNTIME INTERFACE:**
is **plugin** who will takes care of the responsibility of  Kubernetes to create containers/PODS in the backend by using another Container runtime (containerization) tools. such as Docker, Containerd, CRI, Merantis CR and more,
the tools that you can integrate. 
> Kubernetes alone cannot able to create PODS/ Containers in the backend. Thats why the plugin created to integrate compatible container tools to create all these PODS and Containers.

	SUPPORTS MULTIPLE CONTAINERIZATION TOOLS THAT CAN BE INTEGRATED TO K8s CLUSTER GIVING US THE FLEXIBILTY TO SPIN UP WHATEVER CONTAINERS IN EASE.

some of the CRI tools:
1) Docker Engine (deprecated)
2) Containerd (default, standardized and recommended) -> CNCF project
3) CRI-O 

*why docker engine got deprecated + will analyze the* **workflow of CRI:**
#### containerd v1.0
![[dockerCRIvsContainerd.excalidraw]]

  
###### Diagram 1:
previously when Docker Engine was the one being the default one. as you can see, 
  ***Docker version of CRI***
  =>  **Kubelet🛞** < - *CRI* - >  **dockershim** < - - - > **Docker Engine** < - - - >  **containerd** < - > >> PODS/CONTAINERS

why docker got deprecated and the value of efficiency and latency,  -> => *lets take a scenario of launching number of containers a pod*
1) earlier, **Kubelet🛞**(*takes the information from **apiserver** of what to create*)  
2) --> talks to **CRI**⚙️'s (*takes* help from it to create the Pods)
3) ==Inner workings of the previous versions  CRI==  -> **CRI** ⚙️takes **dockershim** (previous docker's runtime which was default at that *time*) ---> this **dockershim** talks to **Docker Engine** (*basically **docker daemon** + its **CLI***) ---> (***daemon** the talks* to) **containerd** => that creates all the container. 
![[dockerCRI.excalidraw]]

> THE PROBLEM HERE IS:
> 1) Too many gates, Too many obstacles, ==LOTS OF LATENCY== 

###### Diagram 2:
SEE HOW THE CURRENT VERSION OF IT **RESOLVES THE PROBLEM**:
  ***containerd v1.0*** 
  =>  **Kubelet🛞** < - *CRI* - >  **cri-containerd** < - - - > **containerd** < - > >> PODS/CONTAINERS

same here, 
1) **Kubelet🛞** -> talks to **CRI**⚙️(*passes the input of what to create*)
2) **CRI**⚙️-> **CRI-containerd** the client side of the tool (*the CRI itself*) (*having the information*)
3)  **CRI-containerd** -> (*communicates directly to*) **containerd** (*creating the containers on the POD right away*)
4) **containerd** ->>> CREATES ALL THE CONTAINERS
![[Containerdv1.excalidraw]]

> Here, THE SOLUTION is that:
> 1) We have avoided the latency of 
> => SKIPPING the **CRI⚙️**  -> taking the dockershim (the docker engine client) --> to **DockerEngine** (the daemon + CLI) --> and then to **containerd**  -- > to create the CONTAINERS on a POD
==AVOIDING THE LATENCY WORKING *WITH THE WORKFLOW OF DOCKER*, Instead the **Kubelet🛞** directly making communication with **CRI**⚙️PLUGIN TO TAKE **cri-containerd** to spin up containers on the PODS== .
	they created containerd integrated with kubelet -> creates the containers on the PODS. 
**VIOLA!**
*****
> WHY **containerd** when we have **DockerEngine**?
>  > There has to be something that got to be under K8's Worker node's **Kubelet🛞**'s Control -> To define such container's as PODS to working with it in ease. 
>  > 
>  > If isn't the case with this practice, the CONTAINER WILL END UP HOUSED UNDER ***DOCKER ENGINE*** which is such a pain to administrate.
 => so that daemon talks to containerd giving all the containers and Making it work as a POD

>THESE THINGS DOESN'T MATTER WHEN WE WORK WITH IT. 
>BUT TO HAVE AN INSIGHT OF WHAT IS WORKING UNDER THE HOOD/BACKEND
>
>THE VALUE OF LATENCY  AND EFFICIENCY WILL MEAN NOTHING FOR A SMALLER WORKLOADS. 
>BUT IF THE CLUSTERS ARE LARGER IN SIZE AND WIDER TO SCALE ->  PLAYS A MAJOR ROLE -> GETS SHIT DONE QUCIKER AND SHIP THINGS FASTER WITHOUT ANY HASSLE.
>> due to latency and for efficiency, **dcokerengine** got DEPRECATED. STOPPED SUPPORT from v1.24 and the **containerd** got standardized, default and recommended from the same v1.24.

***
--> LEAVE THIS ALL <--
we got **containerd v2.0**, we see what is it about in brief.
#### containerd v2.0
![[Containerdv2.excalidraw]]

WHAT THIS HAS TO OFFER/ WHAT SPECIAL ABOUT IT?
> NOTHING NEW. AS SAME AS BEFORE BUT EVERYTHING GOT STREAMLINED!

here in the diagram in brief, you can see the 
1) the highlighted crossed out section is **containerd v1.0**, where
-> the **Kubelet🛞** -> **DockerEngine** -> **dockerd**(daemon + CLI) -> **containerd** -> containers on the POD
(instead of kubelet talks to docker -> talks to dockerd -. talsk to containerd)

2) the pointed out green on is **containerd v2.0**, where
-> the **Kubelet🛞** -> (previously the **CRI** ⚙️plugin was standalone to work with whatever containers)
here the plugin coupled with **containerd** itself. -> (now **Kubelet🛞** directly talks to **containerd**) --> creates all the containers as Pods. ( ==**v2.0** (*which reduces more latency and gains efficiency*)== > *v1.0*)

> THIS DOCUMENTATION IS RECORDED IN **v2.4**
---
#### **kube-proxy**🔀

> [!NOTE] **3) kube-proxy 🔀**
> network proxy. If you'd like to access the application that is sitting inside the K8s Cluster. The request will comes here in this **kube-proxy**
> 
> In the backend, it'll check all the entries in all the **Iptables** or IPV's and takes decision to send the traffic to the respective application in the backend. 
> > User 👤 -->  Node  --> **kube-proxy**🔀 (redirects) -- PODS (in theory)
> > 
> > technically in overview:
> > frontend CLIENT  ---> Service (LB with RR Algo) ---> **kube-proxy**🔀(with IPTables) ---> PODS x3🪣
> > 
> > technically in brief:
> >   ---> (*takes input from **kube-apiserver***☸️) 
> >   ---> **kubelet**🛞  --> **CRI** ⚙️ --> containerd  ->  creates -->   Service (LB) ---> **kube-proxy**🔀 ---> PODs 🪣
> > 
>********
> a proxy server which takes care handling the network managing things such as communication between the Pods and traffic. Kube-proxy knows how to send/redirect the traffic where it has to point to.

1.
> ==NOTE: THAT IT IS RUNNING ON BOTH THE NODES, **CONTROL PLANE** NODE + WORKER **COMPUTE PLANE** NODE==
> EVEN THOUGH, THIS IS A COMPUTE PLANE COMPONENT

**kube-proxy**🔀runs on each node of the Kubernetes cluster.

2.
Handles <b><u>network-proxy</u></b> on each node, where it watches control plane for the **Addition or Removal of Service and EndpointSlice Objects**. (if it didn't make sense),
eg: ***TRYING TO CREATE A POD:***

**CLIENT** [FRONTEND APPLICATION for eg] ---> (the requests will be sent only to the service) **Load Balancer** (=is a **Service**)  (with Round Robin algorithm) - - - > Backend Apps (endpoints = )
|endpoint1  |  endpoint2 | endpoint3|  

| POD1       | POD2       | POD3       |
| ---------- | ---------- | ---------- |
| containers | containers | containers |
![[CRIoutOfTheBox.excalidraw]]

Here, if the Client as frontend getting all the traffic, requests and such,
To whom does it connect to? Since you have 3PODS as Backend which each of has IP address as its own .
	Will we be changing each of the PODS one on one in the backend? NO! 
We should not and not supposed to do any manual task. To tackle and distribute the traffic/requests to each of the POD, should integrate **LOAD BALANCER**. 
> The implementation of a LOAD BALANCER  in any of the K8s Environment => **==Service==**  
 
These **Services** will have **EndpointSlices**. which means that each destinations of the requests/ traffic that ends up name **Endpoint**. So the LB is integrated with the **backend endpoints**.
>NO NEED TO WORRY ABOUT THE PODS AT ALL, THE REQUEST WILL NOT BE GOING TO THE BACKEND PODS but to the **Services** which is **LB** here. 
>SO THE SERVICE WILL RUNNING INTERNALLY TO LOAD BALANCE THE TRAFFIC.

*WHO WILL BE THE ONE SENDING TRAFFIC FROM THE ==CLIENT==  --> TO THE ==SERVICE== (which is a Load Balancer in this case)  --> to the ==Endpoints== (which are Backend PODS)?* => **kube-proxy**🔀. How?
> **kube-proxy**🔀 have **Iptables** in the backend of it. (a linux kernel feature). With that Iptables concept, it will understand that IF THIS REQUEST COMING TO THIS  IP of this **Service** (which is either a LB, Proxy or DNS).
> ***Determines (how it has to be sent)***, that request has to be redirected to the backend PODS. 
> This will be controlled by the ==**kube-proxy's**🔀Iptables== 

3. 
**kube-proxy**🔀: Supports different proxy-modes like:
1) user space (old and deprecated)
2) iptables or IPVS (linux-kernel feat uses netlink) //if you fail to mention anything, it will proceed with the default.
-> used to stored the network details. IF THIS PACKET SENT TO THIS PARTICULAR IP, TO WHOM I NEED TO SEND IN THE BACKEND. 
		These type of information will be stored in all the **kube-proxy**🔀iptables. 
4.
Handles all the traffic inside and outside of the cluster to the respective PODS. 
> Ultimately, the responsibility of the **kube-proxy**🔀is nothing but IF YOUR REQUEST COMING FROM INSIDE or OUTSIDE the cluster? According to the request, redirects the traffic to the relevant POD. in the backend behind the service.

5.
When the traffic reaches the service (ClusterIP or DNS), Kube-proxy redirects the traffic based on the rules (round robin algo) to the Backend PODS. [the picture for reference we saw previously]. Request -> Service => by either a ClusterIP or DNS name. After reaching there, the traffic has to go the backend PODs. (who does that job?) -> **kube-proxy**🔀.

![[trafficDemystified]]

For better understanding:
can see the **kube-proxy**🔀 here, 
--> **apiserver** --> WHATEVER --> **kube-proxy**🔀or **Client** -> vIP OR dns -> Iptables --> redirects --> POD

---
## 6.  Kubernetes Components ports and protocols
we saw K8s, components that exists on both and individual in each of the nodes.

> If your K8s Service have some firewall enabled in it. ON BOTH OR IN THE NETWORK LEVEL. Have to take it all into consideration that:
> What are all the ports and protocols that has to be open ensure to enable communication between everything inside WITH SECURE PRACTICES within or across the nodes.

here, gave a brief of each of it.

**Control Plane:**

| Protocol | Direction | Port Range | Purpose                 | Used By              |
| -------- | --------- | ---------- | ----------------------- | -------------------- |
| TCP      | Inbound   | 6443       | Kubernetes API server   | All                  |
| TCP      | Inbound   | 2379, 2380 | etcd server client API  | kube-apiserver, etcd |
| TCP      | Inbound   | 10250      | Kubelet API             | Self, Control Plane  |
| TCP      | Inbound   | 10259      | kube-scheduler          | Self                 |
| TCP      | Inbound   | 10257      | kube-controller manager | Self                 |

**Worker Node(s):**

| Protocol | Direction | Port Range  | Purpose           | Used By             |
| -------- | --------- | ----------- | ----------------- | ------------------- |
| TCP      | Inbound   | 10250       | Kubelet API       | Self, Control Plane |
| TCP      | Inbound   | 30000-32767 | NodePort Services | All                 |


SAYS Components on which node, what protocol has to be enabled, the direction (whether it is inbound or outbound), Port Number (one or a series of numbers), what component, who or what using it and all(whwther inside or outside the node).

 --- FIN ---
 ****
 

 




