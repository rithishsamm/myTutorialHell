- k8s workload resource Introduction to Daemonset (ds) - Doc


# 1. Â Introduction to DaemonSet(ds)
k8s workload resource Introduction to DaemonSet (ds) - [Doc]

> K8s Workloads - ==**DaemonSet `ds`**==

Will see our next as part the of the list of **Kubernetes Workload Resources:** **`DaemonSet`**`ds`

Will cover in brief about,
- what is `DaemonSet`
- the purpose, usecases and applications
- and all the interrogatives

> The purpose of a DaemonSet `ds` is simple:
**==-If you want to run an app in the PODs on each and every available node in your K8s cluster, go for DaemonSet `ds`==**

##### K8s Workload Resources: **DaemonSet** `ds`
- `DaemonSet` object makes sures that one **POD will running on each node** (control plane and worker node).
- It doesn't support `replica` concept, for which `scaleIn` and `scaleOut` feature is not available
- **Workflow**: `DaemonSet` -> POD
- `apiVersion` for `DaemonSet` Object is `apps/v1`
- Since by default control plane node doesn't launch workloads, we need to define tolerations under template
- DaemonSet object supports two selector parameters (`matchLabels` and `matchExpressions`)
- In order to manage PODs controller by `ds`, `.spec.selector` should match with `.spec.template.metadata.labels`
- We can still control the `ds` to launch POD on specific node with help of `nodeSelector` and `nodeAffinity`
- **DaemonSet** supports `rollout` and `rollback` of PODs
- `DaemonSet` has two **update strategies**:
1) **`RollingUpdate`** (default) -> max unavailable, max surge
2) OnDelete
- **DaemonSet** revision history limit can be set under `.spec.revisionHistoryLimit` (default:10)
```sh
kubectl rollout history ds/daemonsetName
```
- For DaemonSet, each revision is stored in a resource names ControllerRevision
```sh
kubectl get controllerrevision -l ds-key=ds-value
```


##### `DaemonSet` object makes sures that one **POD will running on each node** (control plane and worker node).
If you want to run an app in the PODs on each and every available node in your K8s cluster, `ds` DaemonSet is the choice. Regardless of how many nodes, how many clusters and such. 

can't use the rest of the K8s workload resources that we gone through so far. all the `standalone pods,rc,rs,deploy` and such. Because, **we will not know how many NODES and PODs in all those nodes we will be having in our clusters.**  

If you want to run all your applications on each and every available node in your cluster. Control plane or number of worker nodes  - makes sure that atleast one POD will be running on each and every available node. For that, `ds` is the way. 


##### It doesn't support `replica` concept, for which `scaleIn` and `scaleOut` feature is not available
No `replication` concept here. Since we saw that it makes sure that it runs at least one POD on the nodes, Purpose of it doesn't fit here.  

The concept of `Replication` that we have seen in the following K8s workload resources `rc, rs, deploy`. NOTHING HERE IN THE `ds` DaemonSet. 

No features of replication will be available for us here,
- replica
- scaleIn and scaleOut
- HA practices 
and such

**TO PUT THINGS SIMPLY:**
You want some specific PODs (systemic service oriented PODs like logs, jobs and services) to be running on each and every available nodes at are in the present or in future. You will not know how many nodes and PODs that you are going to need in near future. you'll never know. 
I should be making sure that i will be running these PODs on all the available nodes.
> FOR THESE: DaemonSet `ds`

##### **Workflow**: `DaemonSet` -> POD
that simple! right the `ds` manifest -> creates PODs in all the available nodes. (no strings attached such as replication or other workload resources)

##### `apiVersion` for `DaemonSet` Object is `apps/v1`
as same as `deploy` and such. 
```yaml
apiVersion: apps/v1
kind: DaemonSet
```

##### Since by default control plane node doesn't launch workloads, we need to define tolerations under template
As we saw that `ds` **DaemonSet** will be making sure that the PODs that must be running on all the nodes and such, That nodes includes the **Control Plane** too.


**ALL ABOUT TAINTS AND TOLERATIONS:**
**Check Existing Taints**: First, check if the taints are applied to the `controlplane123` node.
```sh
kubectl describe node controlplane123 | grep Taints
```
**Add Taints**: To add or restore taints to your control plane node, use the following command: eg: `kubectl taint nodes controlplane123 key=value:effect`
```sh
kubectl taint nodes controlplane123 node-role.kubernetes.io/master:NoSchedule
```
**Verify the Taints**: After applying the taint, you can verify it by running:
```sh
kubectl describe node controlplane123 | grep Taints
```

IN BRIEF:
**Step 1: Add Default Control Plane Taint**
The default taint for control plane nodes in a `kubeadm`-initialized cluster is:
```sh
kubectl taint nodes controlplane123 node-role.kubernetes.io/control-plane=:NoSchedule
```
**Explanation:**
- `node-role.kubernetes.io/control-plane` is the key.
- `=` (empty value) means the key exists without a specific value.
- `NoSchedule` ensures that only pods with the correct toleration can be scheduled on the control plane.

**Step 2: Verify the Taint**
After adding the taint, confirm it was applied:
```sh
`kubectl describe node controlplane123 | grep Taints`
```
```sh
`Taints: node-role.kubernetes.io/control-plane:NoSchedule`
```

**Step 3: Restart `kubelet` (if necessary)**
If the taint does not take effect, restart the `kubelet` service on the control plane:
```sh
sudo systemctl restart kubelet
```

If you still see the PODs in the node, take time or just,
**1. Verify Taints**
```sh
kubectl describe node controlplane123 | grep Taints
```
```sh
Taints: node-role.kubernetes.io/control-plane:NoSchedule
```
If this is missing, reapply the taint:
kubectl taint nodes controlplane123 node-role.kubernetes.io/control-plane=:NoSchedule --overwrite
```sh
kubectl taint nodes controlplane123 node-role.kubernetes.io/control-plane=:NoSchedule --overwrite
```
Since by the default, we will be not running PODs in the Control Plane node, he we should. To make `ds` its way to spin up the PODs in the **control plane** too, need to enable and define **`Taints`** and **`tolerations`** under the same.  

 **2. Check Why Pods Are Still Running on the Control Plane**
list Running  -pods
```sh
kubectl get pods -A -o wide | grep controlplane123
```
Check Pod Tolerations
```sh
kubectl describe pod <pod-name> -n <namespace>
Tolerations: node-role.kubernetes.io/control-plane:NoSchedule
```

**3. Evict Running ReplicaSet Pods from the Control Plane**
**Step 1: Cordon the Node (Prevent New Pods)**
```sh
kubectl cordon controlplane123
```
**Step 2: Drain the Node (Evict Running Pods)**
```sh
kubectl drain controlplane123 --ignore-daemonsets --delete-emptydir-data --force
```
- `--ignore-daemonsets`: Ensures system DaemonSet pods (like CoreDNS, Calico, etc.) are not deleted.
- `--delete-emptydir-data`: Removes pods using `emptyDir` volumes.
- `--force`: Forces eviction if necessary.
**3. Uncordon the Node (If Needed)**
If you want to re-enable scheduling on the control plane later:
```sh
kubectl uncordon controlplane123
```

##### DaemonSet object supports two selector parameters (`matchLabels` and `matchExpressions`)
For the **selectors** and such for the identification of PODs, `matchLabels` and `matchExpressions` works just fine! 
same as we saw with the `rs` and `deploy`. All the **selectors** but not the *replication. 

 
##### In order to manage PODs controller by `ds`, `.spec.selector` should match with `.spec.template.metadata.labels`
Selectors in the sense, all that we saw previously with the PODs identification, labels, labels being identical with the PodTemplate and such. 

```sh
.spec.selector = .spec.template.metadata.labels
```
this way `ds` identify the POD that handles. 

Where `kube-controller` creates the PODs, we have to/will be needed to identify those PODs. How? with the help of the labels concept and these components of `matchLabels` and `matchExpressions` .

Labels are simply just the metadata. But that helpful  with the workload resources as well. 


##### We can still control the `ds` to launch POD on specific node with help of `nodeSelector` and `nodeAffinity`
 We have a concept named 
 - **`nodeSelector` and**
 - **`nodeAffinity`,** 

We've been seeing that the `ds` just spins up the PODs on all the available nodes and stuff, even in the control plane. On an overview, it seems like that the `ds` that we have no control over it but to create and remove one. BUT
> We can control the `ds` from going out of hand with some exceptions. 
==We control the scheduling of the POD created by `ds` to launch POD on specific node
**with help of `nodeSelector` and `nodeAffinity`**  decides in which node you can launch and control the PODs== 

##### **DaemonSet** supports `rollout` and `rollback` of PODs
Supports rollout and rollback -> maintaining `ds` PODs version with the same. In the same, we have two types of StrategyUpdate:
- RollingUpdate - rolls one and kills one for update one at a time
- OnDelete - !new - Only if the POD is deleted, the new on gets rolled out.

##### `DaemonSet` has two **update strategies**:
In the same, we have two types of StrategyUpdate:
- **`RollingUpdate`** (default) ->  rolls one and kills one for update one at a time (same maxes)
- OnDelete - !new - Only if the POD is deleted, the new on gets rolled out.

##### **DaemonSet** revision history limit can be set under `.spec.revisionHistoryLimit` (default:10)
Since it supports Rollout and rollback upgrading and downgrading the application, the revision history can be seen for `ds` DaemonSet too.
```sh
kubectl rollout history ds/daemonsetName
```

##### For DaemonSet, each revision is stored in a resource names ControllerRevision
Shows the revision numbers of `ds` as similar as for the `deploy` objects. Where all those revisions will be stored in the Resource Name - Controller Revision.
```sh
kubectl get controllerrevision -l ds-key=ds-value
```
To fetch that, passing this command will be handy where it is called the same from `controllerrevision` -list key and its value of the **DaemonSet**

Will see all the same in practice! 

---