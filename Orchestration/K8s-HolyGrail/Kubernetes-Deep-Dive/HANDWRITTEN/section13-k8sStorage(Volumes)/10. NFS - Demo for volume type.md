
**GETTING DEEP INTO `NFS` MOUNTING STUFF WITH `NFS` SHARING DATA ACROSS NODES.** 
After setting up the `nfs` server, lets write a manifest implementing a `deployment` object to utilize the `nfs` exposed directory from that storage server to share data across replicas of our PODs out of the deployment object.

Same `deploy.yaml` to `nfs-deploy.yaml` manifest template, with the `nfs` VolumeType to it, 
`nfs-deploy.yaml`,
```yaml
apiVersion: apps/v1
kind: Deployment
metadata:
	name: nfx-nginx-deploy
	labels:
		app: nginx-app
		env: prod
		release: v1.0

spec:
	replicas: 2
	selector:
	matchLabels:
		app: nginx-app
		env: prod
		release: v1.0

template:
	metadata:
	name: nfx-nginx-deploy
	labels:
		app: nginx-app
		env: prod
		release: v1.0

spec:
	containers:
		- name: write-app
		image: alpine
		command: ["/bin/sh"]
		args: ["-c", "while true; do date >> /var/log/index.html; sleep 10; done"]
		resources:
		limits:
			cpu: 100m
			memory: 100Mi
		volumeMounts:
			- name: nfs-volume
			mountPath: /var/log
  
		- name: serve-app
		image: nginx:1.27.4
		ports:
			- containerPort: 80
		resources:
		limits:
			cpu: 100m
			memory: 100Mi
		volumeMounts:
			- name: nfs-volume
			mountPath: /usr/share/nginx/html

volumes:
	- name: nfs-volume
	nfs:
		# server: nfs-server.default.svc.cluster.local
		server: 192.168.0.126
		path: /var/nfs
# readOnly: true #commented out since we have a write container writing to the volume
```

Just the volume and,
```sh
volumes: #type
	- name: deploy-shared-volume #name of the volume 
	nfs: #VolumeType
	server: 192.168.0.125 #ip of the nfs server 
	path: /var/nfs #the directory in that nfs server. 
```

Create and verify,
```
kubectl create -f /path/to/nfs-deploy.yaml
```

WE HAVE INTENTIONALLY MADE AN ERROR HERE TO SIMPLY OBSERVE IT AND TO SEE HOW TO RESOLVE IT,

```
kubectl get deploy,rs,po -o wide
```

```
kubectl get deploy,rs,po -o yaml
```
To verify mounts. 

Will be telling that the container is still creating, so THERE IS A PROBLEM. HOW TO DEBUG IT.
```
NAME      nginx-deploy-97b9d97bd-gcdhd 
READY     0/2 
STATUS    ContainerCreating
```

To check pod under the hood,
```
kubectl describe po podName
```

```
Warning:  
FailedMount  75s (x10 over 5m25s)  kubelet            MountVolume.SetUp failed for volume "deploy-shared-volume" : mount failed: exit status 32

Mounting command: mount
Mounting arguments: -t nfs 192.168.0.125:/var/nfs /var/lib/kubelet/pods/e57628f7-0fab-4f72-81c3-78a0057ca3e3/volumes/kubernetes.io~nfs/deploy-shared-volume

Output: 
mount: /var/lib/kubelet/pods/e57628f7-0fab-4f72-81c3-78a0057ca3e3/volumes/kubernetes.io~nfs/deploy-shared-volume: bad option; for several filesystems (e.g. nfs, cifs) you might need a /sbin/mount.<type> helper program.
       dmesg(1) may have more information after failed mount system call.
```

Here in the events, you can see that 
- it performs the volume mounting with the `nfs` VolumeType. 
- Saying **FailedMount** 
- Executing `mount` command,
- Passing `mount` arguments to perform volume mount over the network to the storage server using `nfs` right to the directory that we exposed. 
- Output that this is mount is not executed well due to failed mount. 
==**The reason for the failed mount is that it is telling that there are more FS packages missing inorder to support the NFS to perform mounting, (Which can be seen in the Output - for several filesystems (e.g. nfs, cifs) you might need a /sbin/mount. Helper program. )**==

To further troubleshooting, let see in which node these are the PODs that are getting deployed to. So, lets install the rest of the dependencies into all those nodes and to the storage server. 

INSTALL `nfs` and the rest of its dependencies,
```
sudo apt install nfs-common nfs-kernel-server
```

And now, this got mounted over the network. `NFS` has been performed successfully.

IF THE CLUSTER GETS TO BE DEAD, THE DATA WILL NOT GET TO DUST, SINCE WE ARE STORING ALL IN A REMOTE LOCATION. 

Here for now, i am facing a problem even after installing dependencies, after describing the pod, in events we can see that,
```
Connection refused for 192.168.0.125:/var/nfs on /var/lib/kubelet/pods/e4dc1598-99f5-49bd-9282-502404772651/volumes/kubernetes.io~nfs/deploy-shared-volume

Output: mount.nfs: access denied by server while mounting 192.168.0.125:/var/nfs
```

**Troubleshoot! By passing `describe`, `events` and `logs`.** 

TO verify in deep by listing the PODs and get to know where it is running around and ,
```
kubectl get po -o wide
```

And getting into the node and verify whether the POD has been mounted to which volume, by passing `df -h` command,
```
df -h | grep nfs 
```

Output as:
```
192.168.0.126:/var/nfs              15G  4.5G  9.5G  33% /var/lib/kubelet/pods/ee93d8d9-ef06-4ae6-89f5-8aa1c56da078/volumes/kubernetes.io~nfs/nfs-volume
```
Telling the directory mounted to which, and there it goes. 192.168.0.126 via `nfs`.

And also pass `mount` command to verify mount,
```
mount | grep nfs 
```

And output as,
```
nfsd on /proc/fs/nfsd type nfsd (rw,relatime)
192.168.0.126:/var/nfs on /var/lib/kubelet/pods/ee93d8d9-ef06-4ae6-89f5-8aa1c56da078/volumes/kubernetes.io~nfs/nfs-volume type nfs4 (rw,relatime,vers=4.2,rsize=262144,wsize=262144,namlen=255,hard,proto=tcp,timeo=600,retrans=2,sec=sys,clientaddr=192.168.0.124,local_lock=none,addr=192.168.0.126)
```

Telling,
- Nfs (nfsd on /proc/fs/nfsd type nfsd (rw, relatime) -> mounted to host (192.168.0.126:/var/nfs) -> 
- PodVolume Directory (/var/lib/kubelet/pods/PodVolumeUID/volumes/kubernetes. Io~nfs/nfs-volume) -> 
- Type (type nfs 4 ) -> 
- Rules ((rw, relatime, vers=4.2, rsize=262144, wsize=262144, namlen=255, hard, proto=tcp, timeo=600, retrans=2, sec=sys, clientaddr=192.168.0.124, local_lock=none, addr=192.168.0.126))

Where this `controlplane` directory data got attached to `nfs`  storage server. 

For these types of usecases/scenarios, use `nfs`.

And ping each pod to get the response,
```
curl PodIP1
curl PodIP2
curl PodIP3
```

Use `nfs`, where you can still be able to use the common data in a shared manner. 

Cleanup:
```
kubectl delete deploy deployObjName
```

And check in the node `mount` status, which gets *unmounted*,
```
mount | grep nfs 
```

And ssh into nfs and clear data,
```
ssh username@NodeVMIp(nfs)
```

Delete content, 
```
ls -la /var/nfs/
rm /var/nfs/`files`
```

EVEN THE CLUSTER CRASHES DOWN, THE DATA WILL BE PERSISTENT IN THE `NFS` STORAGE SERVER, where we store data in a remote location and done!


---

