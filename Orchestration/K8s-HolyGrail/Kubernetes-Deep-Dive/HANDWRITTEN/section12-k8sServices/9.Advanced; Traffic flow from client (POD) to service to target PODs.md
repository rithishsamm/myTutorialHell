- Advanced Traffic flow from client (POD) to service to target PODs - Doc


---
# 9. Advanced: Traffic flow from client (POD) to service to target PODs
Advanced Traffic flow from client (POD) to service to target PODs - [Doc]

JUST A CONCEPTUAL DIAGRAM OF **THE WORKFLOW OF AN `svc`**:

![[ClusterIP-workflow.excalidraw]]
AFTER UNDERSTANDING IN BRIEF ABOUT `svc` and `ClusterIP`svc type, will see in detail about the workflow of an `svc` that how the traffic flows from client vs `svc` to target PODs. 

HOW IT ALL JUST WORKS? workflow of an `svc` mapping the flow of the traffic from
***ClientPOD -> `svc` -> TargetPODs***

As we have interpreted before, say for example based on this above given diagram, 
IF WE HAVE A Frontend Client POD, user, CDN, container or such for example, taking on all the ingress -> WE SAW ALL THE DRAMA OF BACKEND. 

(tldr: goes to `svc` which distributed to multiple pods from multiple nodes equally and does the same.)

Firstly, (workflow overview)
![[FE2svc2backendPODs.excalidraw]]
what do we have here is,
- a client Frontend POD receiving GETs/Ingress/Traffic address registered to an `svc` in a K8s environment.  
- sends to that K8s Cluster environment, there we have an `svc`
- that `svc` is sending that traffic to PODs having identical labels of each other. 
- this `svc` distributes traffic to the relevant PODs based on the labels.
- where those PODs are managed under a workload resource (here, it is a `deployment` object for managing versions and rolling it out and back which also creates `rs`for the `replica`s )
we have already seen all these in practice previously. 

Here, the context is that - the frontend POD wants to talk to an another POD, which is its supposed backend. ( a POD to POD Conversation)
1) We will be giving the frontend POD, the `svc`'s IP address which is `ClusterIP` or `dns name` 
2) **Frontend `POD` -- req will be sent to --> `svc`**
3) `svc` as `LB` -- distributing traffic evenly (round robin) --> to backend `PODs`.

THIS IS THE MODEL, HOW DOES THIS WORKFLOW WORKS?
**Understanding in brief from the Networking Perspective:** (how does the traffic flows travelling from Client POD via `svc` to those relevant Backend PODs.)

HOW THE TRAFFIC WILL FLOW IN THIS?

Secondly, brief UNDER THE HOOD of the workflow:
![[ClusterIP-FE2BEUnderTheHood.excalidraw]]
Lets say that 
- the **Frontend POD** is running on **node1** (which has its client IP address,
- here, lets take the same as **Client1**, wants to talk to **Backend** service pod.
- Under the hood, the **Frontend** POD first talks to --> `kube-proxy` on its node(**node1**)
>`kube-proxy` plays a key role when its coming to K8s `svc`'s
So, that's why, `kube-proxy` is obsolete on every available node (both control plane or worker node)

**GET:**  FE ---> BE
- That way, the ingress/traffic out of the **Client1** -- sent to --> `kube-proxy` first. (why?)
- coz, `kube-proxy` knows where the relevant `svc` is available (where it has the **src** and **dest**-`ClusterIP` in its packet)
- that uses **DNAT** - Destination Network Address Translation
- in the DNAT, it acknowledges and resolves the **src** and redirects **dest** to the **Backend Service POD** of the request using the packet. (this what DNAT does)
- after resolving out of `kube-proxy`, packet travels from Node1 -> Node2's Pod2
POST: BE ---> FE
- packet will be travel back as src from PodB and the destination is simply PodA straight away.
- Here, no DNAT but Reverse DNAT.  
- simply goes to `kube-proxy` to get resolved and 
- There, by **Reverse DNAT** ting , the **src** changes back to the appropriate Client PodA which is the frontend
- POSTs to Frontend client PodA.
This how the packets will be travelling back and forth between two PODs across **Nodes** - using **ClusterIP** Address.

Here, we understand `kube-proxy` plays a key role on resolving traffic for GET and POST between nodes using DNAT and Reverse DNAT ting. 

---
