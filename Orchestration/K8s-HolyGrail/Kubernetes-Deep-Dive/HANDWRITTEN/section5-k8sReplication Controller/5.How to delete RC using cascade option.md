 5) Demo: Deleting RC  using Cascade Option

Deleting `rc` without deleting the PODs which are created by `rc`itsef.

referring the same .yaml what we've been using earlier,

```yaml
apiVersion: v1
kind: ReplicationController
metadata:
  name: nginx-rc
  
spec:
  replicas: 3
  selector:
    app: nginx-multicontainer-rc
  template:
    metadata:
      labels:
        app: nginx-multicontainer-rc
        env: prod
        release: v1.0
        
    spec:
      containers:
      - name: write-app
        image: alpine
        command: ["/bin/sh"]
        args: ["-c", "while true; do date >> /var/log/index.html; sleep 10; done"]
        volumeMounts:
        - name: rc-shared-volume
          mountPath: /var/log

      - name: server-app
        image: nginx:latest
        ports:
        - containerPort: 80
        volumeMounts:
        - name: rc-shared-volume
          mountPath: /usr/share/nginx/html
  
      volumes:
      - name: rc-shared-volume
        emptyDir: {}

# apiVersion: v1
# kind: ReplicationController
# metadata:
#   name: nginx-rc
# spec:
#   replicas: 3
#   selector:
#     app: nginx-multicontainer-rc
#   template:
#     metadata:
#       labels:
#         app: nginx-multicontainer-rc
#         env: prod
#         release: v1.0
#     spec:
#       containers:
#       - name: write-app
#         image: alpine
#         command: ["/bin/sh"]
#         args: ["-c", "while true; do date >> /var/log/index.html; sleep 10; done"]
#         volumeMounts:
#         - name: rc-shared-volume
#           mountPath: /var/log
#       - name: server-app
#         image: nginx:latest
#         ports:
#         - containerPort: 80
#         volumeMounts:
#         - name: rc-shared-volume
#           mountPath: /usr/share/nginx/html
#       volumes:
#       - name: rc-shared-volume
#         emptyDir: {}
```
so, simply and create and verify resources,
```sh
kubectl create -f /path/to/rcmulticontainer.yaml
kubectl get rc -o wide
kubectl get po -o wide 
```
Voila! We have all the PODs and there is an `rc` which are managing the PODs, replica of three PODs in total. Done! Now,

So , the main objective for the current context is to remove the `rc` but not the PODs. We have seen the reverse of it where we removing the PODs but not the `rc` by simply reducing the number of replicas to `0` where there will be no pods, 
```sh
kubectl scale --replicas=0 rc/rcName 
```

What is cascade? it is the straight opposite of the previous one where you keep the PODs but delete `rc` which are even created by `rc`. to perform the same (deleting `rc` but keeping the PODs)
```sh
kubectl delete --cascade=orphan rc rcName
```
> **`rc` gets deleted but all those PODs remains.**  

What happens if I try to recreate the same `rc`?  
```sh
kubectl create -f /path/to/rcManifest.yaml
```
adapts and takes to be reused to manage the PODs created by `rc` previously, instead of creating own. if any changes has been applied in the `replica=n` either creates new or terminates existing ones. 

This is what `cascade` used for. Why? 
- for any troubleshooting
- any revision or changes has to be made in the manifest to reapply.
cascade back -> edit manifest -> apply manifest yaml file.

==RARE SCENARIO== BUT TINKERING `RC`
Example: 
- `replica=3`
- `image: nginx:latest` -> `nginx:20.2`
-  any tother changes can be made in manifest as per convenience
> THE PODs are still reused,
```sh
curl -I IP
```
with the previous version. 

and if we change manifest, the older versions of the image will still get persistent and eventually mismatch with the one in the manifest. 

> IT HAS TO MATCH! so pods must be relaunched, to do that 
> 1) you can simply stop all the PODs one by one
> 2) or simply scale down and up to re-spin with the updated version of the image
```sh
kubectl scale --replicas=0 rc rcName
kubectl scale --replicas=n rc rcName
```
Versions gets created and updated. Verify the same with
```sh
curl -I IP
```

this has shown How to manage `cascade` under  `rc` keeping the PODs without a workload resource. 

--- 

