- RC using declarative approach - Doc

2) **Demo! Implement `rc` using declarative approach?**
PRACTICAL DEMO OF DEPLOYNG THIS K8s WORKLOAD RESOUrcES IN REALTIME - `rc` ON MULTIPLE NODES CREATING **MULTI-NODE CLUSTER**:

Now, we will reflect back our setup and the systems environment that we are working with. 

Systems: 
- On-prem
- Virtual Machine environment
- Ubuntu 24.04 Platform 
1) K8s Control Plane - controlplane123 
- ipa-192.168.0.123
- 4096mb(4GB) RAM
- 4vCPU
- 25GB Storage
2)  K8s Worker Node - workernode124
- ipa-192.168.0.124
- 4096mb(4GB) RAM
- 4vCPU
- 25GB Storage

A multi-node (1control/1worker) K8s environment. 

From hereafter, we are going to cover all the same with extra more cluster. (nothing much, just 1node).

WHY? hereafter, we will be diving deep into these and to grasp the true nature of the `replication` concept. 

Nothing to worry here, we simply gonna add one more VM to spin up for - **workernode125** having one more worker node under this control plane. same setup to configure as above.  (this can be seen here in the K8s Setup Guide - [[00. section2-full-k8sClusterSetup#Kubernetes multi-node setup using kubeadm tool(CRI containerd)]]

In order to practice, upcoming K8s workload resources  such as
- replication controller `rc`
- replica set `rs`
- deployment `deploy`
- DaemonSet `ds`
- Statefulset `sts`
- jobs `jobs`
- cronjobs `cj`

So, gonna use this multi-node setup of 
- controlplane123 as Control plane
- workernode124 as worker node1
- workernode125 as worker node2

Task: 
- spin up a VM 
- get back to control plane and pass
```sh
kubeadm token create --print-join-command
```

if any queries with connecting a new node with the control plane, refer this [thread](https://stackoverflow.com/questions/51126164/how-do-i-find-the-join-command-for-kubeadm-on-the-master).

So far with the previous setup, all the PODs that we spin up will be end up launching at `workernode124`. 

or launching PODs on `controlplane123` itself by passing 
```sh
kubectl edit node <controlplanenodename> 
```
edit to remove the **taints**. and save the changes.
>(a key value pair that says not to schedule any PODs here in this node which launched no POD here except the K8s Control Plane components.)

now, we can launch applications on the Control Plane itself. (If you don't want to, don't remove the taints and add another worker node instead.)

**BACK TO `rc`,** 
hereafter, with the concept of `replication` where we having two worker nodes and will be covering how we are launching the PODs with multiple-nodes. 

###### NOW, WILL SEE IN BRIEF ABOUT LAUNCHING `rc` using Declarative Approach?

`rc.yaml`: mostly as same as POD manifest but extra parameters for `replication` things, from `apiVersion` to `metadata` but for the `spec` of the `rc`defining 
- `replicas`,
- `selectors` to identify my pods using labels and 
- Pod `templates` - id template for the PODs are yet to get created
- `lables` !imp - the selector that we might use, has the labels to identify, first
then the `spec` with `containers` section: same as we declare for the rest:
```yaml
apiVersion: v1 #same as pod
kind: ReplicationController #object kind
metadata: 
#nothing here is gonna affect but to identify. 
  name: nginx-rc #as per convenience
  labels:
    app: nginx-rc
    type: rc-prod
spec:
  replicas: 1
  selector:
    app: nginx-rc
  template:
    metadata:
      labels:
        app: nginx-rc
    spec:
      containers:
      - name: nginx-app
        image: nginx:latest
        ports:
        - containerPort: 80
        resources:
          limits:
            memory: "300Mi"
            cpu: "0.2"
          requests:
            memory: "200Mi"
            cpu: "0.1"
```

Lets create the `rc` and verify:
```sh
kubectl create -f path/to/rc.yaml 
kubectl get rc -o wide
```
output:
```
NAME              nginx-rc 
DESIRED           1
CURRENT           1
READY             1
AGE               n mins
CONTAINERS        nginx-app
IMAGES            nginx:latest   
SELECTOR          app=nginx-rc
```
here, will understand the columns. 
we get the `replication` natures such as 
- desired state - how many you want to get created in the backend, 
- current state - how many are created, 
- ready - how many are up and running, 
- age, containers and its image
- selector - that we are using right now to identify the PODs.
 `rc` created all these. 

check the PODs,
```sh
kubectl get po -o wide
```
more or less the same. 
name, readiness, status, restarts, age, IP, node and more. 
here, in the name. -> see the name of the pod + a random suffix
> it is a standard naming convention that it follow. so, for that identification - we use selectors. 
```sh
curl IP
```
to verify the launch. 

> **`RC` is just a logical layer as same as the pod to have one or more containers under the same but for pod is just a static POD, for `rc` it is a replication functionalities to create replicas running multiple PODs in the backend.**

check the `replication` functionalities:

to clean up the resources:
```sh
kubectl get po -o wide
kubectl delete po podname
kubectl get po -o wide
```
no change, the desired state will be `1`. to delete, remove the `rc` itself no the POD.
```sh
kubectl delete rc rcName
kubectl delete -f path/to/rc.yaml
```

> [!NOTE]  ##### FINAL file - demo manifest of a simple `replicationcontroller`.  RC - MANIFEST
> ###### rc.yaml
> ```yaml
apiVersion: v1 #same as pod
kind: ReplicationController #object kind
metadata: 
#nothing here is gonna affect but to identify. 
  name: nginx-rc #as per convenience
  labels:
    app: nginx-rc
    type: rc-prod
spec:
  replicas: 1
  selector:
    app: nginx-rc
  template:
    metadata:
      labels:
        app: nginx-rc
    spec:
      containers:
      - name: nginx-app
        image: nginx:latest
        ports:
        - containerPort: 80
        resources:
          limits:
            memory: "300Mi"
            cpu: "0.2"
          requests:
            memory: "200Mi"
            cpu: "0.1"

