##### Section 15: K8S StatefulSet
1.  Introduction to StatefulSet Object
		Introduction to StatefulSet Object - [Doc]
2. Understanding `STS` workflow
		Understanding `STS` workflow - [Doc]
3.  `STS` scaleIn and scaleOut strategies
		`STS` scaleIn and scaleOut strategies - [Doc]
4. `STS` with Headless service
		`STS` with Headless service - [Doc]
5. Update strategies supported by `STS`
		Update strategies supported by `STS` - [Doc]

---
# Section 15: K8s StatefulSet

# 1.  Introduction to StatefulSet Object
Introduction to StatefulSet Object - [Doc]

Before diving all in `StatefulSet - STS` workload resource object, there are prerequisites to understand StatefulSet in brief,

- Workload Resource (Except STS since we had to cover the prerequisites expecially service to understand STS),
- Service (Except Headless, same excuse applies to understand STS),
- Persistent Volume (PVs and its workings, plugins that supports, reclaim policy, phases and access modes are essential to understand STS in brief) 
- Persistent Volume Claim (Seen static and but dynamic PVC is so important for STS) yet to cover,

Will see in brief about StatefulSet workload objects, why it is used, what is the purpose, core functionalities, when to use **StatefulSet object** for our application deployment on K8s Cluster, 

Again before this, lets understand what is Stateless objects and applications before understanding what is Stateful objects, 


##### StatefulSet vs Stateless:
Lets say, we deploy applications using Deployment object with 3 replicas. (which is a stateless workload object), 

**Can you able to predict the PODs name??? Little to no chance!** - PODs created by RC, RS, Deploy workload objects - the names will be dynamic, it keeps on changing whenever it goes down and re-spawns. Can't predict before getting deployed, 

In terms of `STS` StatefulSet, it matters.

Challenges with Stateless Applications and how StatefulSet satisfies all that:
Challenge 1 - 
Challenge 2 - 
Challenge 3 - 

---
### StatefulSet (STS) 
- In order to run **StatefulSet applications** on K8s cluster, we need to use StatefulSet workload objects,
- We can **scaleIn, scaleOut, rollout and rollback** StatefulSet Object,
- In order to create/delete PODs in proper order and to main uniqueness, we use StatefulSet,
- **Workflow**: StatefulSet -> POD
- For **Stateless applications**, it is recommended to use `Deployment, ReplicaSet, Replication Controller, DaemonSet and such`
- For **`StatefulSet` applications**, we need to use `StatefulSet` object,
- Every POD that is created under `StatefulSet` will have stranded naming convention like `<sts-name>-<index>`
- One of the main reason for using StatefulSet is to provide **persistent storage volumes** for workloads. 
- Use cases for application deployment using `StatefulSet`:
	- Stable, unique network identifiers
	- Stable, persistent storage
	- Ordered, graceful deployment and scaling
	- Ordered, automated rolling updates
- Limitations:
	- PV should be provisioned by **`StorageClass`** or by **Administrator**
	- Deleting `StatefulSet`, will not delete volumes associated 
	- StatefulSet required/recommended to use only Service type as Headless (`ClusterIP: None`)
- `STS` update strategies: **`RollingUpdate`** (default) and **`OnDelete`**
- `StatefulSet` uses **`volumeClaimTemplates`** to create dynamic PVC Object. 


##### In order to run **StatefulSet applications** on K8s cluster, we need to use StatefulSet workload objects,

In order to run **StatefulSet applications** on `K8s` cluster, we need to use StatefulSet workload objects. In the sense, we have saw the difference between 
- Stateless Objects
- Stateful Objects

Stateless:
say, An App rolled out using deployment object with three replica. 
BTS: It creates pods (p1, p2, p3) across nodes as per scheduler. 

THE CATCH and all the CHALLENGES is that: here with **Stateless objects**,  is that, 
we couldn't be able to predict:
1) will the POD Name be persistent? Can it be static?
2) for this case, for an HA infra, Primary  Pods to be in active mode, One in Passive mode. - NOT POSSIBLE WITH STATELESS OBJECTS.
3) Can it be scheduled One by one at a sequential order (for creation, updation, deletion, scaleIn, scaleOut), instead of getting created all at a time.  (if a mistake happens, all will go down together) - STRUGGLES WITH STATELESS OBJECTS.
4) With STATELESS, all PODs have it's own `PVC` which it gets bounded to the its or some `PV`. If PODs get recreated, all those Persistent Volumes and Claim will be same which was created with the last name but the Name will be different after recreation, Which is NOT IDEAL FOR STATEFULSET OBJECTS. / 
If the admin has to scale, he has to create seperate PV and PVC for those workloads (as a prerequisite) and then scale as needed. Without doing that and deploy workloads, ALL THOSE PODs will remain in pending status. - WILL MEET THESE CHALLENGES FOR STATELESS APPLICATION DEPLOYMENT - eg: nginx, apache, tomcat - the state of the application will not be saved on the server side. 
5) shall we able to predict of be able to declare the name of the PODs to have more control over it - important for a Stateful object to run Stateful application - all that gives it is a dynamic but standardized hash.
6) Which specific node it has to get assigned to. 
THESE ARE THE CHALLENGES WITH STATELESS OBJECTS, SEE HOW STATEFULSET OBJECTS SATISFIES THE SAME. 


##### We can **scaleIn, scaleOut, rollout and rollback** StatefulSet Object,

We can scaleIn, scaleOut, rollout and rollback - StatefulSet Objects - previously, we simply do

```
kubectl scale --replica=n
```
 Here, we can do the same too with rollout and rollback option. 
 
##### In order to create/delete PODs in proper order and to main uniqueness, we use StatefulSet,

  In order to upgrade or downgrade applications or delete PODs in proper Sequential order and to maintain uniqueness, use STATEFULSET. 

>  Sequential order: eg: first creates POD, then creates PVC then creates PV. After finishing, goes to next POD.  - FOR STATEFULSET OBJECTS. 

If we scaleDown in future, turning replica from 3 to `--replica=2`. The sequencial order will be ascending --> , when deletion goes descending. 

And the naming convention? - STATEFULSET + Index Value -> will see this in brief. 


##### **Workflow**: StatefulSet -> POD
**Workflow**: StatefulSet -> POD. 

**StatefulSet creates POD** -> POD. (no dependent objects needed like for rc, rs and deploy objects - not creating out of rs or deployment object)

##### For **Stateless applications**, it is recommended to use `Deployment, ReplicaSet, Replication Controller, DaemonSet and such`
For **Stateless applications**, it is recommended to use `Deployment, ReplicaSet, Replication Controller, DaemonSet and such`. Statement speaks for itself. 


##### For **`StatefulSet` applications**, we need to use `StatefulSet` object,
For **`StatefulSet` applications**, we need to use `StatefulSet` object. **OBVIOUS!**

>  ***Every POD that is created under `StatefulSet` will have standard namingconvention like `<sts-name>-<index>` value.***

Eg: StatefulSet -> (replica=3) -> POD 1, POD 2, POD 3. Naming convention of the PODs would be, example if say, we create Jenkins App:,  name will be like:
- `StatefulSetName+indexValue`
- Pod1 - jenkins-0
- Pod2 - jenkins-1
- Pod3 - jenkins-2
Here, StatefulSet -- creates --> PODs. Once, one get bootstrapped and created successfully, the other will be created. We can able to predict and bit and work specifically towards those PODs.

In future, if i scale the replica from 3 to 5, the PODs will be predictably, 
>  Index = (n-1)
- Pod 4 - jenkins-3
- Pod 5 - jenkins-4


##### One of the main reason for using `StatefulSet` is to provide PVs **persistent storage volumes** for workloads. 

***One of the main primary reason for using `StatefulSet` is to provide `PVs` `persistent storage volumes` for workloads.*** 

If, we would like to save the state of my application, STATEFULSET is the choice with the help of PVs concept. 



##### Use cases for application deployment using `StatefulSet`:
- Stable, unique network identifiers
- Stable, persistent storage
- Ordered, graceful deployment and scaling
- Ordered, automated rolling updates

###### **Limitations:**
- PV should be **provisioned and controlled** by **`StorageClass` or by Administrator.** (to create Dynamic PV instead of creating statically/manually for each deployment, can do but why : )
- Deleting `StatefulSet`, will not delete volumes associated - manual deleting required. - manual deletion here too. 
- StatefulSet required/recommended to use only Service type as **Headless** (`ClusterIP: None`) - !IMPORTANT 


##### `STS` update strategies: **`RollingUpdate`** (default) and **`OnDelete `**
`STS` update strategies: **`RollingUpdate`** (default) and **`OnDelete`**


##### `StatefulSet` uses **`volumeClaimTemplates`** to create dynamic PVC Object. 

Tldr: when creating STATEFULSET objects, it is always good to create Dynamic PVC instead of static creating all by our own for each one manually. - Dynamic PV only complies and available for STS. Here, 
`StatefulSet` uses **`volumeClaimTemplates`** to create dynamic PVC Object. 

These are all the critical points and prerequisites in order to move forward with STATEFULSETs and PVs. Managing a predictable and persistent infra. 


---
# 2 . Understanding `STS` workflow
Understanding `STS` workflow - [Doc]

Will get started with `STS` in practice. 

To find `api-resource` of an object, 

```
kubectl api-resources 
kubectl api-resources | grep statefulsets/sts
```

>  `sts: apps/v1`

`nginxSts-hostPath.yaml`
```yaml
# this one is to get familiar with the concepts, working principles, functionalities, behaviour, handling things first then proceed with the rest. 
# then will go deep into high level deployment on K8s Cluster. 
apiVersion: apps/v1
kind: StatefulSet
metadata:
  name: nginx-sts-hostpath
  labels: #optional
    app: nginx 
    env: prod
    release: "1.0"

spec:
  selector:
    matchLabels:
      app: nginx
      env: prod
      release: "1.0"

  replicas: 3

  template:
    metadata:
      labels:
        app: nginx
        env: prod
        release: "1.0"
    spec:
      containers:
      - name: nginx
        image: nginx:latest
        ports:
        - name: http
          containerPort: 80
          protocol: TCP
        volumeMounts:
        - name: shared-nginx-sts-data
          mountPath: /usr/share/nginx/html

  volumeClaimTemplates:
  - metadata:
      name: shared-nginx-sts-data
    spec:
      storageClassName: hostpath-storage
      accessModes: [ "ReadWriteOnce"]
      resources:
        requests:
          storage: 1Gi

#this is a volumeClaimTemplates for this StatefulSet
#before deploying this, we should have a storageclass/PV available first
#we have declared 3 replicas for this StatefulSet, so should have 3 PVs available, 
#else, it will be in pending state. 
```
Keep this aside for now, because. 

>  this is a `volumeClaimTemplates` for this `StatefulSet`

Before applying this, should have a `PV` ready in the backend in order to mount with the same.  Since, here we have `replica=3`, should create and have 3 PVs in hand. Else, it will be in pending status. NOTE CAREFULLY.

`nginxSts-PV.yaml`
```yaml
apiVersion: v1
kind: PersistentVolume
metadata:
  name: nginx-sts-hostpath-pv0 #01 to 0n is for unique identification for each of the replicas
  labels:
    app: nginx
    env: prod
    release: "1.0"
spec:
  storageClassName: hostpath-storage
  capacity:
    storage: 1Gi
  accessModes:
    - ReadWriteOnce
  hostPath:
    path: "/var/tmp/shared-nginx-sts-data0" #01 to 0n is for unique identification
    type: DirectoryOrCreate
  # seperate directory for each of the replicas, 01 is for first one, 02 is for second and so on

  # persistentVolumeReclaimPolicy: Retain

  #this is a PersistentVolume for this StatefulSet, this is the base template,
  #should have 3 of the same for each of the replicas. 


  # inorder to write multiple section in a single yaml file, we use `---` to separate each section

---


apiVersion: v1
kind: PersistentVolume
metadata:
  name: nginx-sts-hostpath-pv1 #identification
  labels:
    app: nginx
    env: prod
    release: "1.0"
spec:
  storageClassName: hostpath-storage
  capacity:
    storage: 1Gi
  accessModes:
    - ReadWriteOnce
  hostPath:
    path: "/var/tmp/shared-nginx-sts-data1" #identification, seperate directory for each of the same
    type: DirectoryOrCreate
  # persistentVolumeReclaimPolicy: Retain

---

apiVersion: v1
kind: PersistentVolume
metadata:
  name: nginx-sts-hostpath-pv2 #same as above
  labels:
    app: nginx
    env: prod
    release: "1.0"
spec:
  storageClassName: hostpath-storage
  capacity:
    storage: 1Gi
  accessModes:
    - ReadWriteOnce
  hostPath:
    path: "/var/tmp/shared-nginx-sts-data2" #same applies here. 
    type: DirectoryOrCreate
  # persistentVolumeReclaimPolicy: Retain
```


Here,
```
spec:
	storageClassName: hostpath-storage
```
Is the base identifier. 

Now: 
PV first and verify, 
```
kubectl apply -f /path/to/nginxSts-PV.yaml
kubectl get pv -o wide
```
All will be available and all got created one at a time. - With the storageClassName - `hostPath` where the `STS` looking for it. 

Will be `bounded` once the STATEFULSET gets created -> which creates the PV -> which gets **bounded** to the available PVCs.

Then `sts`:

	This STATEFULSET will be creating a PVC. 
	That PVC will be looking out for their determined PVs to get attached with which having its storageClassName - hostPath. 

```
kubectl apply -f /path/to/sts.yaml
```

Result: the STATEFULSET created. 

Verify: 
```
kubectl get sts,po,pv,pvc -o wide
kubectl get all -o wide
```
Should see,
- This `STS` should've been created, 
- The PODs should've been created in the `n-1` manner and run,
- The PVC should've been created based on the name of the workloads in order, 
- The PVC should've been bounded to the PV created, 
- The Each of the PV should  be bounded with the PVC out of the `STS`.  

BUT! (WE ARE ABOUT TO SEE THE DOWNSIDE OF STATIC PV CREATION):

If he have to scale this?
```
kubectl scale -- replica=5 sts sts-name
```
>  sts scaled

```
kubectl get po -o wide
```
One of the POD will be in pending status, Since there are no PV readily available for the POD's PVC. 

The administrator has to be manually come in and provision the same for the resource to be utilized as a prerequisites for the STS to run in ease. Until then, it will remain in pending status. 

To resolve the same, just add an another `PV` in the manifest for the newly created PODs. 
 
> ==YOU SHOULD NOT CREATE PVs STATICALLY==

It is such a overhead to the setup and not well recommended. 

**BEST APPROACH IS TO CREATE PVs - DYNAMICALLY - with the help of StorageClass**. Meanwhile with this one and the `VolumeClaimTemplate`, new PVC will be created.  

Cleanup:
```
kubectl delete -f /path/to/sts.yaml
kubectl get all -o wide
```

And the PVC's and PVs won't be deleted, need manual cleanup intervention:
That is so obvious. That's the whole point of STS workload resource object. 
Headless applications will be recreated from scratch, where data will persist and can be re-utilized. 

PVCs will be bounded to the same pod. Coz, the POD will be created with the same name. With the naming reference, we can re-utilize the same with the newer one maintaining the state of the applications. 

With stateless applications, not so POSSIBLE. 

To prove the same point, recreate the same STS manifest,

```
kubectl apply -f /path/to/sts.yaml
```
 >  It is not created any new PVC out of the STS. Using existing PVC. 
 > Meaning that, PODs are using the previous state of the same. As simple as that. 
 > 


Will see the rest of the functionalities as we go through. 

---
# 3.  `STS` `scaleIn` and `scaleOut` strategies
`STS` `scaleIn` and `scaleOut` strategies - [Doc]

**`STS` `scaleIn` and `scaleOut` strategies and demonstration:**

Will understand in brief of the scaling behaviour of STATEFULSET,  in order to get the scaling right when we work with it, 

Let's see how to overcome that. Two obvious approach's - 
- Imperative approach - using cli commands
- Declarative approach - using manifests. 

###### Imperative approach - using cli commands
Lets see Imperative approach. 

PVC get created out of STS, but what about PV. Has to be created DYNAMICALLY based on the STS and it's PVC right. 

We know the result when, 
- If we **scale up** -> goes error since there are no PV readily available for the PVC's to get attached with. 
```
kubectl scale --replica=5  sts sts-name #from 3 to 5
```
Error if there are no PVs, 
if exists, gets bounded and running. 

- If **scaled down** -> scales down but the PVC's and its PV's persists still. 
```
kubectl scale --replica=2  sts sts-name #from 5 to 2
```
Pods will be removed, PVC and it's PV will persist still. 

And gets deleted by index, higher the value is - gets deleted first, Lower the value, gets delete at the least. 

>  ==**Scale out - incremental,** 
>  **Scale in - decremental**==

This is the understanding of scaleIn, scaleOut of the **STATEFULSET**. 

Will see in brief about service **clusterIP=none -> Headless** service and its advantages in short.  Where this service specifically has to be or STS. 


---
# 4. `STS` with Headless service
`STS` with Headless service - [Doc]

Lets understand - Headless Service,

By, 
- Getting the core of the headless service,
- Creating a headless service for STS and 
- Understanding the workings of the same and why this is specific to STS. 

The type is,
`ClusterIP= none`, means that it should not be assigned to my or any service. Making it **`headless`**

```yaml
apiVersion: v1
kind: Service
metadata:
  name: nginx-sts
  labels: 
    name: nginx
    env: dev
    release: v1.0

spec: 
  selector:
    app: nginx
  ports: 
    - port: 80
      targetPort: 80
      protocol: TCP
      name: http   
  type: ClusterIP
  clusterIP: None
```

Create and verify:
```
kubectl create -f /path/to/sts-svc.yaml
kubectl get svc -o wide
```

Says:
```
Warning: spec.SessionAffinity is ignored for headless services
service/nginx-sts created
```

```
NAME         TYPE        CLUSTER-IP   EXTERNAL-IP   PORT(S)   AGE     SELECTOR
nginx-sts    ClusterIP   None         <none>        80/TCP    4m38s   app=nginx
```
Here, the **`SVC`** gets created but no IP got assigned - **Headless**, 

User or Service will be trying to connect with a DNS of the SVC. That DNS will point it to the POD's IP Address. Here, the barrier got removed between the DNS and the POD but in-between a `ClusterIP` without an IP which is a Headless service.

Now, will test the same by creating a dummy workload with some program in it. To do that, 

Before this, Execute  [[00.Section 15 - K8S StatefulSet#2 . Understanding `STS` workflow]] 's PV and PVC. 

And now run, 
```
kubectl run test-pod --image=dubareddy/utils --rm -it -- bash 
```

You'll be prompted to:
```
root@test-pod:/# 
```

```
nslookup sts-name
```
And verify the same. **==It'll try to match with the A records of the SVC and that directs to the PODs and shows details of each==**.

Even if I scaleOut with the appropriate PVs , works the same by showing all the PODs details and simply works in ease. 
```
kubectl scale --replica=5 sts sts-name #3to5
```

Pods will be created and, Now, the DNS will be pointed towards all these news PODs too. 

```
kubectl run test-pod --image=dubareddy/utils --rm -it -- bash 
nslookup sts-name
```

Shows all the five pod's DNS details and pointed directly towards POD IP address. 

**==Now the DNS of the SVC is point towards the A record of the POD, not the A record of the ClusterIP.==** THIS IS THE WHOLE POINT FOR USING **==Headless==** SVC

Quick Cleanup Hack: 
```
kubectl delete -f /path/to/listOfManifests
```

Deletes all the objects at a time all in one go!. 
> **==Note: without deleting PVC, you can't delete PV. Since its bounded.==** 



---
# 5 . Update strategies supported by `STS`
Update strategies supported by `STS` - [Doc]

Will understand in brief about the Update strategies available and supported by `STS`.

We have seen some in the DEPLOYMENT object in the name 
- RollingUpdate - deploy
- Recreate 
- On-delete - sts and DaemonSet
For the mix of workload objects. 


###### RollingUpdate:
Create PVs and verify:
```
kubectl create -f /path/to/sts-pv.yaml
kubectl create -f /path/to/sts.yaml
kubectl get sts,po,pvc,pv -o wide
```

SVC is optional but still a free will. Here, we are trying to understand the Update Strategies so all good, 


Let' s find out what is the default `updateStrategy` for STS:
```
kubectl get sts sts-name -o wide | grep updateStrategy
```

>  The default `updateStrategy` = `RollingUpdate`

To change to other `updateStrategy` 's and i want to update the application, what and how to do,

```
kubectl edit sts sts-name
```
And edit the `image` 's version. 

We have performed an update here, observe the behavior of the `STS`. 
>  ==**delete's and recreates the update for each of the POD in order - descending manner.==** 

```
kubectl get po -o wide
```

To see the difference in the AGE of the PODs and clearly see the pattern. -> **`RollingUpdate`**

Also you can observe the same by,

```
kubectl rollout history sts sts-name
kubectl rollout history sts sts-name --revision=n
```
Check the **Revision**. 


###### OnDelete:
```
kubectl edit sts sts-name
```
And edit the **`updateStrategy`**,

```
updateStrategy:
	type: OnDelete #from RollingUpdate
```
Save and exit.

The behaviour of OnDelete **`updateStrategy`** is - Updates the application gets any update. *You wont see anything getting rolled-out right away.* 

To update app,
```
spec:
	containers:
	- image: nginx:1.22.0 #from nginx:1.21.0 version +1 as update
```

Now check the status of the PODs,
```
kubectl get po -o wide
```
And check the `Age` of PODs - same as old. 

Why it is the same?
>  the `updateStrategy` is `OnDelete`, the name speaks for itself. Updates only when you delete/once the PODs gets deleted or re-created. 

Has to be `recreated`, **The recreation will happen with the same name as before.** !IMP - storing the state of the POD. 

Say that this is a prod server environment, the state of the POD and the name will be the same (which is stored in `etcd` and recreates with the same name), Just IP differs. 

To verify this behaviour, lets prove the same in practice:

Verify what image currently the PODs are using;  
```
curl -I PodIP
```
Shows the current version of the image that the POD is using. 

And the target version that we have looking forward to use is  + 1 of the current version. 

To apply the change: 
```
kubectl delete po pod-name1
```
Once the pod gets deleted, the POD will be recreated. Same STS behaviour. Its all the same only the IP differs, 

 Verify: 
 ```
 curl -I PodIP1
 ```
Update got rolled out after deletion. Which can be verified with the version of the image. 

For the rest of the PODs, it still in the older version. 
```
curl -I PodIP2
curl -I PodIP3
```
Still stuck with the older one. That will get updated once these PODs faces it's recreation. 

**But if you look at other workload objects, the case is totally different - gets recreated with the bunch of hashes and random values - Dynamic name allocation, which is not so viable for persistent cases.**

This is the core difference between the STS and other workload resources. 



**==And This is  `On-delete` `UpdateStrategy` for the STS.==** 

Perform cleanup and done!!


---

